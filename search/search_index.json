{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Open Chat Studio","text":"<p>This is the home page for all documentation related to Open Chat Studio. Developed by Dimagi, Open Chat Studio is an easy-to-use, open source platform for rapidly prototyping, testing and deploying chatbots created using Large Language Models (LLMs).</p>"},{"location":"#what-can-i-do-on-open-chat-studio","title":"What can I do on Open Chat Studio?","text":"<ul> <li> <p>Make Your Own Chatbots: With Open Chat Studio (OCS), you can easily create your own chatbots using advanced   language technology. OCS is built for use by program staff and other teams - you don't need to be an engineer to get   started.</p> </li> <li> <p>Deploy: Use Open Chat Studio to launch your chatbots on the web and mobile apps such as Telegram and WhatsApp,   with more   options coming soon.</p> </li> <li> <p>Enable Access for All: Anyone you share a chatbot with will be able to access it, either through a web link or   directly   on platforms such as WhatsApp or Telegram. Chatbot users do not need to have an account on Open Chat Studio to use   your   chatbots.</p> </li> <li> <p>View and Download Data: View and export the data from interactions with your chatbots, formatted In CSV.</p> </li> </ul> <ul> <li> Quickstart Guide</li> <li> What is a Chatbot?</li> <li> Configuring your Team</li> <li> What is an LLM?</li> <li> Deploying your bot</li> </ul>"},{"location":"#how-do-i-use-open-chat-studio","title":"How do I use Open Chat Studio?","text":"<p>You can host your own instance of Open Chat Studio, or use the hosted version at chatbots.dimagi.com.</p> <p>If you would like an account on Dimagi's hosted version of Open Chat Studio send an email to ocs-info@dimagi.com. </p>"},{"location":"about/","title":"About Open Chat Studio","text":"<p>Dimagi is developing Open Chat Studio (OCS) as an easy-to-use, open source platform for rapidly prototyping and testing chatbots created using Large Language Models (LLMs). Open Chat Studio makes it easy to develop and test LLM-based chatbots, and to instill a variety of guardrails to improve the safety and accuracy of these bots.</p> <p>Open Chat Studio can work with any LLM with an API such as the OpenAI Chat Completions API.</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#oct-24-2025","title":"Oct 24, 2025","text":"<ul> <li>BUG Fixed dashboard query issues with unique counts, date range filtering for active participants and sessions, and improved cache keys.</li> <li>NEW Added support for bulk updating participant_data and session_state keys via CSV upload in evaluation message views.</li> </ul>"},{"location":"changelog/#oct-23-2025","title":"Oct 23, 2025","text":"<ul> <li>BUG Fixed an issue where timeout triggers would fire repeatedly instead of respecting the configured delay between triggers.</li> </ul>"},{"location":"changelog/#oct-9-2025","title":"Oct 9, 2025","text":"<ul> <li>BUG The dynamic filter widget will now update the available versions to match those of the selected chatbot(s).</li> </ul>"},{"location":"changelog/#oct-17-2025","title":"Oct 17, 2025","text":"<ul> <li>CHANGE Fix edge case errors for pipelines with parallel workflows. </li> <li>CHANGE Make all node inputs available to Python Nodes and Render Template nodes via the <code>node_inputs</code> argument.</li> <li>NEW Introduce wait_for_next_input Python Node function for advanced parallel flow control. </li> </ul>"},{"location":"changelog/#oct-15-2025","title":"Oct 15, 2025","text":"<ul> <li>NEW Participants can now be removed.</li> </ul>"},{"location":"changelog/#oct-7-2025","title":"Oct 7, 2025","text":"<ul> <li>NEW Users can now create datasets from individual message pairs that match the filter parameters, rather than being limited to importing entire sessions.</li> <li>BUG Fixed an issue where the evaluation run page would sometimes get stuck.</li> <li>CHANGE Removed the <code>Experiments</code> and <code>Safety Layers</code> sidebar entries. This is part of the migration from Experiments to Chatbots.</li> </ul>"},{"location":"changelog/#sept-26-2025","title":"Sept 26, 2025","text":"<ul> <li>NEW Participant import and export functionality has been added to the Participant list page.</li> <li>CHANGE Update the 'attach-media' tool used by Media Collections to allow attaching multiple files at a time (up to a maximum of 5).</li> </ul>"},{"location":"changelog/#sept-25-2025","title":"Sept 25, 2025","text":"<ul> <li>BUG Fixed an issue where participant identifiers for the connect channel were not treated as case insensitive.</li> <li>BUG Fixed an issue with the dashboard filters not updating date filters correctly.</li> </ul>"},{"location":"changelog/#sept-24-2025","title":"Sept 24, 2025","text":"<ul> <li>NEW Added a view where all sessions across all chatbots are shown and can be filtered.</li> </ul>"},{"location":"changelog/#sept-22-2025","title":"Sept 22, 2025","text":"<ul> <li>NEW A 'calculator' tool has been added to the LLM node to allow bots to perform mathematical calculations consistently and reliably.</li> </ul>"},{"location":"changelog/#sept-19-2025","title":"Sept 19, 2025","text":"<ul> <li>MIGRATION All Experiments have been migrated to chatbots.</li> <li>NEW Add filtering capabilities to the participant table.</li> </ul>"},{"location":"changelog/#sept-12-2025","title":"Sept 12, 2025","text":"<ul> <li>CHANGE Deprecate Assistant Nodes. See the migration documentation for more details.</li> </ul>"},{"location":"changelog/#sept-4-2025","title":"Sept 4, 2025","text":"<ul> <li>BUG Fix user invitation flow where email case doesn't match the registered email.</li> </ul>"},{"location":"changelog/#aug-29-2025","title":"Aug 29, 2025","text":"<ul> <li>CHANGE Remove the default \"API\" filter applied to the chatbot session tables.</li> </ul>"},{"location":"changelog/#aug-21-2025","title":"Aug 21, 2025","text":"<ul> <li>NEW Added a voice selection field to the LLM node to override the global STT setting. This field is only visible when a voice provider is configured for the bot.</li> </ul>"},{"location":"changelog/#aug-20-2025","title":"Aug 20, 2025","text":"<ul> <li>NEW Added read-only API key permissions. All API keys (new and existing) default to read-only. Enable write permissions by checking \"Allow Write\" when creating a new API key.</li> </ul>"},{"location":"changelog/#aug-14-2025","title":"Aug 14, 2025","text":"<ul> <li>CHANGE Added <code>page_size</code> parameter to list APIs to allow overriding the default page size (100). The maximum page size is 1500.</li> <li>CHANGE Added <code>experiment</code> parameter to the session list APIs to allow filtering by experiment ID.</li> <li>CHANGE Include session tags in the response from the session list API.</li> </ul>"},{"location":"changelog/#aug-11-2025","title":"Aug 11, 2025","text":"<ul> <li>CHANGE Multi-select filters now have \"Select All\" and \"Clear All\" option buttons located at the top of the dropdowns.</li> </ul>"},{"location":"changelog/#aug-7-2025","title":"Aug 7, 2025","text":"<ul> <li>CHANGE Allow full customization of the chat widget appearance via additional CSS variables.</li> </ul>"},{"location":"changelog/#aug-4-2025","title":"Aug 4, 2025","text":"<ul> <li>BUG Fixed an issue with versioning when using a collection where it would always show new changes.</li> </ul>"},{"location":"changelog/#aug-1-2025","title":"Aug 1, 2025","text":"<ul> <li>NEW Display the list of all tags used in a session at the top of the session transcript. Clicking on a tag filters the transcript.</li> </ul>"},{"location":"changelog/#jul-31-2025","title":"Jul 31, 2025","text":"<ul> <li>NEW Added <code>set_participant_data_key</code>, <code>append_to_participant_data_key</code> and <code>increment_participant_data_key</code> utility methods to the code node.</li> <li>NEW Added LLM tools to append to and increment a counter in the participant data.</li> </ul>"},{"location":"changelog/#jul-30-2025","title":"Jul 30, 2025","text":"<ul> <li>NEW Chat Widget v1.4 released. This represents a major rewrite of the widget with significant improvements and new features. See the widget changelog for full details.<ul> <li>Enhanced Mobile Experience</li> <li>Draggable Chat Window</li> <li>Button Customization</li> <li>Fullscreen mode</li> <li>Welcome Messages &amp; Starter Questions</li> <li>Session persistence across page loads</li> </ul> </li> </ul>"},{"location":"changelog/#jul-29-2025","title":"Jul 29, 2025","text":"<ul> <li>BUG Fixed an issue where certain files could not be deleted from local indexes.</li> <li>NEW Added support for the <code>gemini-embedding-001</code> embedding model. Users can now use this embedding model with local indexes.</li> </ul>"},{"location":"changelog/#jul-17-2025","title":"Jul 17, 2025","text":"<ul> <li>NEW Added support for reading xls, xlsx, pptx, and Outlook files using the code node.</li> </ul>"},{"location":"changelog/#jul-17-2025_1","title":"Jul 17, 2025","text":"<ul> <li>NEW Files generated by OpenAI's code execution tool are now also downloadable.</li> </ul>"},{"location":"changelog/#jul-15-2025","title":"Jul 15, 2025","text":"<ul> <li>NEW File referencing can now be disabled for indexed collections.</li> <li>NEW Added a tool that allows users to explore their local indexed collections by querying the collection with natural language.</li> </ul>"},{"location":"changelog/#jul-14-2025","title":"Jul 14, 2025","text":"<ul> <li>CHANGE File index status now updates in real-time. Users no longer need to reload to see the latest status.</li> <li>BUG Fixed an issue where session and message tags were not displaying in the participant session details view.</li> </ul>"},{"location":"changelog/#jul-10-2025","title":"Jul 10, 2025","text":"<ul> <li>NEW Added the ability to translate chat transcripts to a new language from the chat transcript view page via a \"Translate\" button.</li> <li>CHANGE Chat widget can be dragged across the window rather than being limited to set positions.</li> </ul>"},{"location":"changelog/#jul-10-2025_1","title":"Jul 10, 2025","text":"<ul> <li>CHANGE Indexed collections will only show llm providers that has embedding models configured.</li> </ul>"},{"location":"changelog/#jul-5-2025","title":"Jul 5, 2025","text":"<ul> <li>BUG Fixed an issue with pipeline dragging and improved robustness for non-editable pipelines.</li> </ul>"},{"location":"changelog/#jul-4-2025","title":"Jul 4, 2025","text":"<ul> <li>NEW When using a local index, files that were used to generate responses are now attached to the response.</li> </ul>"},{"location":"changelog/#jul-3-2025","title":"Jul 3, 2025","text":"<ul> <li>NEW Allow team admins to manage specific feature flags. See Chatbots FAQ.</li> </ul>"},{"location":"changelog/#jun-30-2025","title":"Jun 30, 2025","text":"<ul> <li>NEW Pipelines now support parallel execution. Read more about the pipeline execution model.</li> <li>NEW New tools added to PythonNode to support parallel execution:<ul> <li>get_node_output</li> <li>abort_with_message</li> <li>require_node_outputs</li> </ul> </li> <li>BUG Fix speech to text feature so that transcribed text is returned from the STT service.</li> </ul>"},{"location":"changelog/#jun-27-2025","title":"Jun 27, 2025","text":"<ul> <li>BUG Fix attachment download link in chat transcript view.</li> <li>NEW Display image attachments in chat transcript view.</li> <li>BUG Fix banners so they stay dismissed between page navigations.</li> </ul>"},{"location":"changelog/#jun-26-2025","title":"Jun 26, 2025","text":"<ul> <li>BUG Fixed versioning not working properly with safety layers.</li> </ul>"},{"location":"changelog/#jun-24-2025","title":"Jun 24, 2025","text":"<ul> <li>NEW Message and session tags can now be added from Python Nodes using the <code>add_message_tag</code> and <code>add_session_tag</code> functions.</li> </ul>"},{"location":"changelog/#jun-23-2025","title":"Jun 23, 2025","text":"<ul> <li>BUG Fixed an issue with file citations where cited files are not always showing up as being cited.</li> </ul>"},{"location":"changelog/#jun-20-2025","title":"Jun 20, 2025","text":"<ul> <li>NEW Add a new tool that will end the current chat session when called.</li> <li>NEW Prompts can now reference pipeline temporary state and session state by using prompt variables.</li> <li>NEW Added support for OCS hosted indexes. See indexed collections for more details.</li> </ul>"},{"location":"changelog/#jun-19-2025","title":"Jun 19, 2025","text":"<ul> <li>BUG Fixed an issue where chunked messages sent via Twilio arrive out-of-order at the user.</li> </ul>"},{"location":"changelog/#jun-11-2025","title":"Jun 11, 2025","text":"<ul> <li>CHANGE Performance improvements to the versioning process.</li> <li>CHANGE Updated text diff widget for the \"Create new version\" page which preserves the original text formatting.</li> </ul>"},{"location":"changelog/#jun-4-2025","title":"Jun 4, 2025","text":"<ul> <li>NEW Added the ability to create an indexed collection from an OpenAI assistant that uses File Search.</li> </ul>"},{"location":"changelog/#jun-3-2025","title":"Jun 3, 2025","text":"<ul> <li>NEW Added support for filtering Chatbot sessions by relative date ranges. This adds the <code>range</code> operator as a filter option for date parameters with options such as <code>1 hour</code>, <code>1 day</code>. In addition, a top level filter is added which allows quick filtering of Chatbot sessions by 'Last Message' field.</li> <li>CHANGE For teams with Chatbots enabled, the default landing page will now be \"Chatbots\" replacing \"Experiments\"</li> </ul>"},{"location":"changelog/#may-22-2025","title":"May 22, 2025","text":"<ul> <li>CHANGE API update: The list experiments endpoint now returns only unreleased experiments in the top-level response. A new <code>versions</code> key has been added to each experiment, containing all version of that experiment.</li> <li>NEW Collections are now generally available for all users who are using pipelines</li> </ul>"},{"location":"changelog/#may-16-2025","title":"May 16, 2025","text":"<ul> <li>NEW Added support for indexed collections to support RAG use cases.</li> <li>CHANGE When using an OpenAI Assistant with a chatbot that has tracing enabled, a new event will be posted to the tracing provider with all the Assistant details. This makes it easier to see which assistant is being called.</li> </ul>"},{"location":"changelog/#may-15-2025","title":"May 15, 2025","text":"<ul> <li>NEW (UI) LLM provider names are now displayed in the dropdown menu on LLM nodes, making it easier to distinguish between providers of the same type.</li> <li>NEW The background color for pipeline nodes can be changed.</li> <li>CHANGE Trace spans for pipeline nodes are now named according to the node name and not the node ID. This makes it easier to understand traces.</li> <li>NEW Pipeline routing information is now exposed in pipeline state and can be easily accessed via 3 new helper functions in the PythonNode: get_selected_route, get_node_path, get_all_routes.</li> </ul>"},{"location":"changelog/#may-9-2025","title":"May 9, 2025","text":"<ul> <li>NEW Chatbot configurations can now be duplicated to create exact copies of the chatbot.</li> </ul>"},{"location":"changelog/#may-8-2025","title":"May 8, 2025","text":"<ul> <li>BUG Ensure that chatbot events and scheduled messages utilize parameters from the published version of the chatbot.</li> </ul>"},{"location":"changelog/#may-7-2025","title":"May 7, 2025","text":"<ul> <li>NEW Link from chatbot version details to a read only view of a pipeline</li> <li>NEW Tag user messages which are not processed due to an unsupported message type</li> <li>BUG Correctly fire the 'participant joined' event for participants that already have sessions with other chatbots</li> </ul>"},{"location":"changelog/#apr-29-2025","title":"Apr 29, 2025","text":"<ul> <li>BUG Fixed an issue where users are unable to view archived experiments / chatbots.</li> <li>NEW Dark mode support for the chat UI.</li> <li>NEW The session creation endpoint now supports passing state. Additionally, the state of an existing session can also be updated via the update state endpoint.</li> </ul>"},{"location":"changelog/#apr-25-2025","title":"Apr 25, 2025","text":"<ul> <li> <p>NEW Pipeline bots can now configure surveys and voice settings.</p> </li> <li> <p>NEW Default keyword can be configured by user on router nodes in pipelines.</p> </li> <li>BUG Fixed switching between different experiments in participants data view.</li> </ul>"},{"location":"changelog/#apr-22-2025","title":"Apr 22, 2025","text":"<ul> <li>NEW Router nodes in pipelines can be configured to tag output messages with the node name and the route that was selected. This is useful for understanding the flow of messages through the pipeline.</li> </ul>"},{"location":"changelog/#apr-21-2025","title":"Apr 21, 2025","text":"<ul> <li>NEW We now only support the participant_data prompt variable for routers.</li> <li>BUG Fixed an issue where session messages would sometimes not load.</li> </ul>"},{"location":"changelog/#apr-15-2025","title":"Apr 15, 2025","text":"<ul> <li>NEW Record and display the source of chats that originate from an embedded chat widget.</li> </ul>"},{"location":"changelog/#apr-11-2025","title":"Apr 11, 2025","text":"<ul> <li>BUG Fixed the <code>{participant_data}</code> prompt variable which was missing the participants scheduled messages.</li> </ul>"},{"location":"changelog/#apr-10-2025","title":"Apr 10, 2025","text":"<ul> <li>NEW Bots can now send multimedia files to users using the Collections feature.</li> <li>BUG Fixed reminder messages not appearing in web chats.</li> </ul>"},{"location":"changelog/#apr-7-2025","title":"Apr 7, 2025","text":"<ul> <li>BUG Fixed an issue where if custom actions are removed from a node, it resulted in an error when creating a new version</li> </ul>"},{"location":"changelog/#apr-3-2025","title":"Apr 3, 2025","text":"<ul> <li>NEW Added support for the static router to read from the session state</li> </ul>"},{"location":"changelog/#apr-1-2025","title":"Apr 1, 2025","text":"<ul> <li>BUG Deselecting source material in a pipeline node no longer reports an error.</li> </ul>"},{"location":"changelog/#mar-31-2025","title":"Mar 31 2025","text":"<ul> <li>NEW Page pagination added to chat transcript message list view for sessions with over 100 chats. 100 chats per page.</li> </ul>"},{"location":"changelog/#mar-27-2025","title":"Mar 27, 2025","text":"<ul> <li>NEW History modes added for chat history compression</li> <li>Summarize: Summarizes older messages when token count exceeds the limit.</li> <li>Truncate Tokens: Removes older messages to stay within token limits.</li> <li>Max History Length: Retains only the last N messages.</li> <li>NEW Allow chatbot builders to configure whether files referenced by assistants as citations can be downloaded.</li> <li>NEW Improve citation link rendering using footnotes.</li> </ul>"},{"location":"changelog/#mar-26-2025","title":"Mar 26, 2025","text":"<ul> <li> <p>BUG Resolved an issue preventing users from creating timeout events.</p> </li> <li> <p>BUG Can now download sessions with API key via 'Generate Export' button on Experiment Session page</p> </li> </ul> <p>Adds 'does not contain' to participant filter in sessions filter widget</p> <ul> <li>BUG API toggle updates automatically without modifying the filter widget</li> </ul>"},{"location":"changelog/#mar-25-2025","title":"Mar 25, 2025","text":"<ul> <li>NEW Added a new state that is scoped to the participant's session and can be read and written to using a code node. See the section on session state for more information.</li> </ul>"},{"location":"changelog/#mar-20-2025","title":"Mar 20, 2025","text":"<ul> <li>BUG Exclude 'thinking' messages from OpenAI assistant responses. Only the final message from the assistant will be shown.</li> </ul>"},{"location":"changelog/#mar-17-2025","title":"Mar 17, 2025","text":"<ul> <li> <p>NEW Access to participant's schedules from the python node added via the get_participant_schedules helper function.</p> </li> <li> <p>BUG Fix a bug where pipelines with tracing and assistant nodes is not working.</p> </li> <li> <p>BUG Allow using the <code>{participant_data}</code> and other prompt variables in Router Node prompts.</p> </li> </ul>"},{"location":"changelog/#mar-11-2025","title":"Mar 11, 2025","text":"<ul> <li>NEW Experiment Sessions table gains a new filter widget to replace the existing search bars which allows seaching on all and multiple columns.</li> </ul>"},{"location":"changelog/#mar-11-2025_1","title":"Mar 11, 2025","text":"<ul> <li>BUG Upgrade the Langfuse library to fix an issue where bots with Langfuse tracing is not responsive.</li> </ul>"},{"location":"changelog/#mar-7-2025","title":"Mar 7, 2025","text":"<ul> <li>NEW Scheduled messages for chat participants can now be cancelled via the participant's details page.</li> </ul>"},{"location":"changelog/#mar-6-2025","title":"Mar 6, 2025","text":"<ul> <li>NEW The Chat Completions API and New Message API endpoints now support a version parameter (<code>vN</code>) to specify the working version of the experiment.</li> <li>NEW Added a button in the versions table to copy the API URL for each version of the experiment.</li> <li>BUG Fix an error when reading <code>docx</code> files: \"There is no item named '...' in the archive\".</li> <li>BUG Fix a bug with pipelines which resulted in the bot generating a new scheduled message every time a scheduled message is triggered.</li> </ul>"},{"location":"changelog/#mar-5-2025","title":"Mar 5, 2025","text":"<ul> <li>NEW Participant global data can be get &amp; set in the Python Node.</li> <li>CHANGE UI update: Add \"(upload file)\" text behind title of Code Interpreter Files and File Search Files for clairty to upload files in both locations if desired.</li> </ul>"},{"location":"changelog/#feb-27-2025","title":"Feb 27, 2025","text":"<ul> <li>NEW The <code>random</code> Python module is now available in the Python Node.</li> </ul>"},{"location":"changelog/#feb-26-2025","title":"Feb 26, 2025","text":"<ul> <li>BUG Fixed an issue where the prompt used to generate ad-hoc bot messages were being saved in the chat history as a user message.</li> <li>BUG Fixed an issue with the code node causing the pipeline to not load.</li> <li>BUG Restores ability to create event for the bot to summarize the conversation.</li> </ul>"},{"location":"changelog/#feb-25-2025","title":"Feb 25, 2025","text":"<ul> <li>BUG Fixed an issue where OpenAI's API message limit was reached before summarizing the conversation.</li> <li>NEW The Assistants UI now displays the number of uploaded files for code interpreter and file search</li> <li>NEW Bots will now inform users when something goes wrong while generating a message</li> <li>BUG Fixed an issue where empty responses caused the bot to malfunction. These responses are now saved as an AI message</li> </ul>"},{"location":"changelog/#feb-24-2025","title":"Feb 24, 2025","text":"<ul> <li>NEW <code>.docx</code> attachments can now be read in the code node.</li> <li>BUG Prevent event messaging loops by restricting the event options when \"a new bot message is received\" is selected. This is a followup to the celery issues on Feb 7, 2025.</li> </ul>"},{"location":"changelog/#feb-21-2025","title":"Feb 21, 2025","text":"<ul> <li>NEW: Add an AI helper for writing code in Python Nodes. Demo</li> </ul>"},{"location":"changelog/#feb-19-2025","title":"Feb 19, 2025","text":"<ul> <li>NEW: Add support Deepseek models.</li> <li>NEW: Added theme toggle in the navbar which allows switching between dark and light theme.</li> <li>CHANGE The experiment description can now include markdown formatting which will be rendered when it is displayed on the public chatbot pages (the consent page and the pre-survey page).</li> <li>CHANGE All markdown links in the chatbot now open in a new tab.</li> <li>NEW: Team deletion modal allows user to select the subset of uses to asynchronously send a team deletion email: just the deleting user, all team admins, all team members.</li> </ul>"},{"location":"changelog/#feb-18-2025","title":"Feb 18, 2025:","text":"<ul> <li>CHANGE Pipelines are now sorted by name in ascending order in both the pipelines table and dropdown list on the experiment edit page.</li> <li>CHANGE Allow unauthenticated users (public chat users) to access attachments in their chats.</li> </ul>"},{"location":"changelog/#feb-14-2025","title":"Feb 14, 2025:","text":"<ul> <li>BUG Fixed an issue where the citations enabled toggle on an assistant node showed that it was disabled when the node was just added, where in reality it was actually enabled.</li> <li>CHANGE Update the embeddable chat widget to better support mobile devices &amp; allow more customization of the styles.</li> <li>CHANGE Allow pipeline bots to toggle conversational consent.</li> </ul>"},{"location":"changelog/#feb-06-2025","title":"Feb 06, 2025:","text":"<ul> <li>NEW Chatbots can now be embedded into external websites. See the documentation for more information.</li> <li>CHANGE Enhanced the participants filter in the experiment sessions view to support multiple participants. This allows you to export chats for selected participants only.</li> <li>CHANGE Consent forms have become optional. If no consent form is configured then the user will not be prompted to accept a consent form before starting a chat. Pre- and post-surveys are still displayed if there are any configured.</li> </ul>"},{"location":"changelog/#jan-29-2025","title":"Jan 29, 2025:","text":"<ul> <li>CHANGE Improved pipeline validation logic and display of errors.</li> <li>CHANGE Improve the changes UI when creating a new experiment version to show the details of referenced objects (e.g. pipelines, assistants, etc).</li> <li> <p>CHANGE Tag UI improvements to make it easier to add and remove tags from chats and messages.</p> </li> <li> <p>NEW Chatbot versioning released. See the documentation for more information.</p> </li> <li>NEW New nodes added to Pipelines.<ul> <li>Python Node: Allows you to run Python code in the pipeline.</li> <li>OpenAI Assistant Node: Allows you to use an OpenAI Assistant in the pipeline.</li> <li>Static Router Node: Allows you to route messages to different nodes based on a static mapping.</li> </ul> </li> <li>NEW Pipelines can now be tested via the edit UI. This does not currently support history or participant data.</li> <li>NEW \"Trigger Bot Message\" API endpoint. This allows you to send a prompt to a bot which will trigger a message to the specified user. See the API documentation for more information.</li> <li>NEW Custom Actions allow bots to connect to external APIs and services.</li> <li>NEW The web chat UI now supports  and  reactions to bot messages. These reactions are saved as tags on the message.</li> </ul>"},{"location":"changelog/#dec-12-2024","title":"Dec 12, 2024:","text":"<ul> <li>NEW When an experiment is in \u201cdebug mode\u201d, errors will be shown to the user in the chat UI.</li> </ul>"},{"location":"changelog/#nov-6-2024","title":"Nov 6, 2024:","text":"<ul> <li> <p>CHANGE Auto populate channel names with the experiment\u2019s name and improve the channel help text</p> </li> <li> <p>NEW Added the ability to toggle whether responses should include source citations in the case of assistant based   experiments. To toggle this, you\u2019ll need to have the \u201cassistant\u201d type selected in the experiment\u2019s edit screen, then   go   to the \u201cAdvanced\u201d tab. There you will see a \u201cCitations enabled\u201d setting. When this setting is \u201coff\u201d, no citations   will   be present in the response.</p> </li> </ul>"},{"location":"changelog/#oct-29-2024","title":"Oct 29, 2024:","text":"<ul> <li> <p>CHANGE [Security update] Participants will now have to verify their email address before they can continue to   the chat   session.</p> </li> <li> <p>NEW Added a new \u201cdebug mode\u201d for experiments. When an experiment is in debug mode, the user will be able to see.</p> </li> </ul>"},{"location":"changelog/#aug-16-2024","title":"Aug 16, 2024:","text":"<ul> <li>CHANGE API update: Ability to filter experiment session results using a newly supported \u201ctags\u201d parameter.</li> </ul>"},{"location":"changelog/#aug-16-2024_1","title":"Aug 16, 2024:","text":"<ul> <li>NEW Integration with Langfuse for tracing.</li> <li>NEW Dynamic voice support in a multi bot setup. Users can now choose to let the child bot\u2019s voice be used if it   generated the output in a multi-bot setup. This behaviour is disabled by default, but can be enabled by going to the   experiment\u2019s voice configuration settings and checking the Use processor bot voice box.</li> </ul>"},{"location":"changelog/#aug-14-2024","title":"Aug 14, 2024:","text":"<ul> <li>BUG Fixed tagging in the case where assistants were used as child bots in a router setup</li> <li>CHANGE Assistants cannot be used as router bots anymore, since this messes up the conversation history on OpenAI\u2019s   side.</li> </ul>"},{"location":"changelog/#aug-5-2024","title":"Aug 5, 2024:","text":"<ul> <li>NEW Allow sorting of the \u201cStarted\u201c and \u201cLast Message\u201c columns in the experiment sessions view</li> <li>NEW Terminal bot. You can now configure a bot (from an experiment\u2019s homepage) to run at the end of every inference   call. This terminal bot will change the input according to the configured prompt, even in a multi-bot configuration. A   terminal bot is useful when you want to ensure that the bot always responds in a certain language.</li> <li>CHANGE The {source_material} and {participant_data} prompt variables can now only be used once in a prompt.   Including this variable more than once in the same prompt will not be allowed.</li> <li>BUG Fixed an issue where assistant generated Word (.docx) files (and possibly others) were being corrupted.</li> </ul>"},{"location":"changelog/#aug-1-2024","title":"Aug 1, 2024:","text":"<ul> <li>CHANGE Improved data extraction to handle long contexts</li> </ul>"},{"location":"changelog/#july-26-2024","title":"July 26, 2024:","text":"<ul> <li>NEW File download support for assistant bots. Cited files that were uploaded by the user or generated by the   assistant   can now be downloaded from within the chat. Please note that this only applies to webchats, and the user must be a   logged in user to download these files.</li> <li>NEW Twilio numbers will now be verified at the selected provider account before allowing the user to link the   whatsapp account to an experiement. Please note that this will not be done for Turn.io numbers, since they do not   provide a mechanism for checking numbers.</li> </ul>"},{"location":"changelog/#july-19-2024","title":"July 19, 2024:","text":"<ul> <li>NEW In-conversation file uploads for assistant bots on web based chats. These files will be scoped only to the   current   chat/OpenAI thread, so other sessions with the same bot will not be able to query these files.</li> </ul>"},{"location":"changelog/#july-15-2024","title":"July 15, 2024:","text":"<ul> <li>NEW Participant data extraction through pipelines.   You need the \u201cPipelines\u201d feature flag enabled.   Usage information can be found here.</li> <li>BUG Normalize numbers when adding or updating a whatsapp channel. This helps to avoid accidentally creating   another whatsapp channel with the same number that is in a different format.</li> <li>BUG Verify Telegram token when adding a telegram channel</li> </ul>"},{"location":"changelog/#july-8-2024","title":"July 8, 2024:","text":"<ul> <li>NEW Add {current_datetime} prompt variable to inject the current date and time into the prompt</li> <li>BUG Fixed a bug with syncing files to OpenAI assistants vector stores</li> <li>BUG Ensure API keys have the current team attached to them</li> <li>BUG Enforce team slugs to be lowercase</li> <li>BUG Update chat history compression to take the full prompt into account including source material, participant   data   etc.</li> <li>CHANGE Redo UI to show team on all pages and use dropdown for team switching and team management links</li> <li>CHANGE Hide API sessions from \u2018My Sessions\u2019 list</li> <li>NEW User interface for creating and editing Experiment Routes (parent / child experiments)   New tab on the main experiment page</li> </ul> <p>API changes</p> <ul> <li>NEW API documentation and schema now available at https://chatbots.dimagi.com/api/docs/</li> <li>NEW Experiment session list, detail and create API</li> <li>CHANGE Channel message API now takes an optional session field in the request body to specify the ID of a specific     session</li> <li>CHANGE Experiment API output renamed experiment_id to id</li> <li> <p>CHANGE Update participant data API POST body format changed. New format:</p> <p><code>[{\"experiment\": \"&lt;experiment_id&gt;\", \"data\": {\"property1\": \"value1\"}}]</code></p> </li> <li> <p>NEW OpenAI compatible \u2018Chat Completions\u2019 API for experiments. See API docs.</p> </li> </ul>"},{"location":"changelog/#jun-24-2024","title":"Jun 24, 2024","text":"<ul> <li>NEW Tagging messages based on which bot generated that response in a multi-bot setup</li> </ul>"},{"location":"changelog/#jun-17-2024","title":"Jun 17, 2024","text":"<ul> <li>NEW Pipelines (alpha)<ul> <li>Look for \"Pipelines\" in the sidebar</li> <li>Ability to run a pipeline based on event triggers</li> <li>Ability to create a pipeline visually</li> </ul> </li> <li>NEW Participant view<ul> <li>View participants with their data</li> <li>We added a view where users can see all participants, when they joined initially and what experiments they participated in.</li> <li>Experiment admins will also be able to add or update data to be associated with a participant. When the experiment prompt includes the <code>{participant_data}</code> variable, this data will be visible to the bot and participant specific details can be referenced or considered during the conversation.</li> <li>Note that participant data is scoped to the specific experiment. This means that you have to manually add data pertaining to a particular participant to each experiment manually.</li> </ul> </li> <li>NEW Slack integration<ul> <li>Ability to connect to a slack workspace as a messaging provider</li> <li>Ability to link an experiment to a Slack Channel (via an experiment channel)</li> <li>Ability to have one 'fallback' experiment which can respond to messages on channels where no other experiment is assigned</li> <li>Experiment responds to 'mentions' and creates a new session as a Slack thread</li> </ul> </li> </ul>"},{"location":"changelog/#jun-4-2024","title":"Jun 4, 2024","text":"<ul> <li>CHANGE Individual tool selection<ul> <li>Users can now choose which tools to give to the bot to use.</li> <li>Previously this was obscured by a single \u201ctools enabled\u201d checkbox which - when enabled - gave the bot all the tools that existed at that time.</li> <li>Tools include: One-off Reminder, Recurring Reminder and Schedule Update</li> <li>One-off Reminder: This allows the bot to create a one-time reminder message for some time in the future.</li> <li>Recurring Reminder: This allows the bot to create recurring reminder messages.</li> <li>Schedule Update: This allows the bot to update existing scheduled messages. Please note that this tool cannot update reminders created with the one-off and recurring reminder tools, since those are legacy tools using a different framework. Future work will fix this.</li> </ul> </li> <li>CHANGE When and error occurs during the processing of a user message, the bot will tell the user that something went     wrong instead of keeping quiet</li> </ul>"},{"location":"changelog/#may-21-2024","title":"May 21, 2024","text":"<ul> <li>NEW OpenAI Assistants v2 support</li> <li>NEW Multi-experiment bots<ul> <li>Allows users to combine multiple bots into a single bot, called the \u201cparent\u201d bot. The parent bot will decide which of the \u201cchild\u201d bots the user query should be routed to, based on the prompt.</li> <li>The prompt should give clear instruction to the parent bot on how it should decide to route the user query.</li> </ul> </li> </ul>"},{"location":"changelog/#may-15-2024","title":"May 15, 2024","text":"<ul> <li>CHANGE Experiment search improvements<ul> <li>The search string will be used to search for experiments by name first, then by description</li> </ul> </li> <li>NEW OpenChatStudio API: A few endpoints are exposed to allow chatting to a bot using the API</li> </ul>"},{"location":"changelog/#may-10-2024","title":"May 10, 2024","text":"<ul> <li>NEW Bots are given knowledge of the date and time</li> <li>NEW Added a new event type called \u201cA new participant joined the experiment\u201d</li> <li>NEW Added a new event handler to create scheduled messages</li> </ul>"},{"location":"changelog/#may-9-2024","title":"May 9, 2024","text":"<ul> <li>NEW Show participant data in the session review page<ul> <li>If participant data exists and was included in the prompt, then reviewers will be able to see the data in the session review page. Seeing the data that the bot had access to might help with understanding the conversation.</li> </ul> </li> <li>CHANGE Anthropic now also supports tool usage!</li> </ul>"},{"location":"changelog/#apr-30-2024","title":"Apr 30, 2024","text":"<ul> <li>NEW Participant data<ul> <li>Participant data allows the bot to tailor responses by considering details about the participant.</li> <li>To include participant data in the prompt, go to the experiment edit page and add the <code>{participant_data}</code> variable to an appropriate place in the prompt.</li> <li>Currently we have to create it manually through the admin dashboard, but a near future release will include a dedicated page to view/edit participant data.</li> <li>Please note that participant data is experiment specific, meaning that data we have for a participant in one experiment may not be the same for the next experiment.</li> </ul> </li> </ul>"},{"location":"domain_migration/","title":"Domain Migration","text":"<p>We will be migrating from the current chatbots.dimagi.com domain to www.openchatstudio.com.</p> <p>We will be configuring automatic redirects for most traffic, however, some features will require you to make changes to external systems. Please follow the migration guides below.</p>"},{"location":"domain_migration/#timeline","title":"Timeline","text":"<p>The timeline is not fully set yet, however, the new domain is live, and we recommend users start using the new domain.</p>"},{"location":"domain_migration/#migration-guides","title":"Migration Guides","text":""},{"location":"domain_migration/#messaging-providers","title":"Messaging Providers","text":"<p>We are still investigating our options for migrating messaging providers. Where possible, we will perform automated migrations. If manual steps are required, we will provide clear instructions and sufficient advanced notice.</p>"},{"location":"domain_migration/#apis","title":"APIs","text":"<p>Users of the APIs will be required to update their API clients to use the new domain name.</p>"},{"location":"domain_migration/#embedded-chat-widget","title":"Embedded Chat Widget","text":"<p>The recommended approach is to upgrade the chat widget to a version <code>&gt;= v0.4.9</code>.</p> <p>If you are using a version <code>&lt; 0.4.0</code>, you should consider upgrading anyway. We will be phasing out the endpoints used by that version of the widget in the future.</p>"},{"location":"api/","title":"API","text":"<p>Open Chat Studio provides a REST API that enables you to create chat sessions, send messages, manage experiments, and access session data programmatically.</p>"},{"location":"api/#api-schema-and-docs","title":"API Schema and Docs","text":"<p>See the following links for documentation on the API endpoints:</p> <ul> <li>API docs</li> <li>OpenAPI Schema</li> </ul>"},{"location":"api/#overview","title":"Overview","text":"<p>The API is organized around REST principles and uses standard HTTP methods and status codes. All API endpoints return JSON responses and require authentication.</p> <p>Base URL: <code>https://chatbots.dimagi.com/api/</code></p>"},{"location":"api/#authentication","title":"Authentication","text":"<p>The API supports multiple authentication methods:</p> <ul> <li>API Key Authentication: Include your API key in the <code>X-api-key</code> header</li> <li>Token Authentication: Use Bearer token authentication in the <code>Authorization</code> header  </li> <li>Cookie Authentication: Session-based authentication using cookies (for web integrations)</li> </ul>"},{"location":"api/#error-handling","title":"Error Handling","text":"<p>The API uses standard HTTP status codes:</p> <ul> <li><code>200 OK</code>: Request successful</li> <li><code>201 Created</code>: Resource created successfully  </li> <li><code>202 Accepted</code>: Request accepted for processing</li> <li><code>400 Bad Request</code>: Invalid request data</li> <li><code>401 Unauthorized</code>: Authentication required</li> <li><code>404 Not Found</code>: Resource not found</li> <li><code>500 Internal Server Error</code>: Server error</li> </ul> <p>Error responses include a JSON object with error details:</p> <pre><code>{\n  \"detail\": \"Error description\"\n}\n</code></pre>"},{"location":"api/#rate-limiting","title":"Rate Limiting","text":"<p>To ensure optimal performance: - Chat polling should not exceed once every 30 seconds - API requests are subject to reasonable rate limits - Use pagination for large data sets</p>"},{"location":"api/#llm-docs","title":"LLM Docs","text":"<p>The following documents are a simplified version of the API for consumption by LLMs:</p> <ul> <li>Channels</li> <li>Chat</li> <li>Sessions</li> <li>Experiments</li> <li>Files</li> <li>OpenAI</li> <li>Participants</li> </ul>"},{"location":"chat_widget/","title":"Open Chat Studio Widget","text":"<p>The Open Chat Studio Widget is a customizable chat component that allows you to easily embed conversational AI bots into any website. Create engaging user experiences with minimal setup and extensive customization options.</p>"},{"location":"chat_widget/#features","title":"Features","text":"<ul> <li>Easy Integration: Add to any website with just a few lines of code</li> <li>Flexible Embedding: Choose between widget component or iframe methods</li> <li>Custom Styling: Match your brand with CSS variables and custom themes</li> <li>Welcome Messages: Greet users with personalized messages</li> <li>Starter Questions: Guide users with pre-defined clickable questions</li> <li>File Uploads: Allow users to attach files to their messages</li> <li>Responsive Design: Works seamlessly across desktop and mobile devices</li> </ul> <ul> <li> View the reference docs</li> <li> Changelog and upgrade info</li> </ul>"},{"location":"chat_widget/#getting-started","title":"Getting Started","text":"<p>Before embedding, you must create a bot in Open Chat Studio.</p> <ol> <li> <p>Add the widget script to your site's <code>&lt;head&gt;</code> section:</p> <pre><code>&lt;script type='module' src='https://unpkg.com/open-chat-studio-widget@0.5.1/dist/open-chat-studio-widget/open-chat-studio-widget.esm.js' async&gt;&lt;/script&gt;\n</code></pre> </li> <li> <p>Obtaining Embed Code</p> <ol> <li>Log in to Open Chat Studio.</li> <li>Navigate to the Experiment you wish to embed.</li> <li>Click on the  Web channel and select  Share</li> <li>Copy the provided embed code snippet.</li> </ol> </li> <li> <p>Insert the widget where you want the chat button.</p> <p>The embed code snippet should look something like this:</p> <pre><code>&lt;open-chat-studio-widget\n  visible=\"false\"\n  chatbot-id=\"{CHATBOT_ID}\"\n  button-text=\"Let's Chat\"\n  position=\"right\"\n  expanded=\"false\"&gt;\n&lt;/open-chat-studio-widget&gt;\n</code></pre> </li> </ol>"},{"location":"chat_widget/#test-the-chatbot","title":"Test the Chatbot","text":"<ol> <li>Open your website in a web browser.</li> <li>Ensure the chatbot appears and functions as expected.</li> <li>Try sending a message to confirm it responds correctly.</li> </ol>"},{"location":"chat_widget/#troubleshooting","title":"Troubleshooting","text":"<p>If the chatbot does not appear:</p> <ul> <li>Ensure you copied and pasted the embed code correctly.</li> <li>Clear your browser cache and refresh the page.</li> <li>Check that your website allows embedding external scripts.</li> </ul>"},{"location":"chat_widget/changelog/","title":"Changelog","text":""},{"location":"chat_widget/changelog/#general-upgrade-guide","title":"General upgrade guide","text":"<p>This guide will help you upgrade from previous versions of the Open Chat Studio Widget to the latest version. Please follow these steps carefully to ensure a smooth transition and also review any changes and upgrade steps between your current version and the latest version.</p>"},{"location":"chat_widget/changelog/#quick-upgrade-steps","title":"Quick Upgrade Steps","text":""},{"location":"chat_widget/changelog/#1-update-the-script-tags","title":"1. Update the script tags","text":"<p>Update your script tags to use the latest version: <pre><code>&lt;script \n  src=\"https://unpkg.com/open-chat-studio-widget{LATEST_VERSION_NUMBER}/dist/open-chat-studio-widget/open-chat-studio-widget.js\"\n  type=\"module\"\n  async\n  &gt;&lt;/script&gt;\n</code></pre></p>"},{"location":"chat_widget/changelog/#2-review-your-implementation","title":"2. Review Your Implementation","text":"<p>Check your current HTML implementation and compare it with the latest properties reference.</p>"},{"location":"chat_widget/changelog/#changelog_1","title":"Changelog","text":""},{"location":"chat_widget/changelog/#v051","title":"v0.5.1","text":"<ul> <li>Change language codes for Italian and Portuguese to use standard codes: <code>ita</code> -&gt; <code>it</code>, <code>por</code> -&gt; <code>pt</code></li> </ul>"},{"location":"chat_widget/changelog/#v050","title":"v0.5.0","text":"<ul> <li>Allow users to drag and reposition the chat-widget launch button when it\u2019s fixed, to avoid obscuring page content.</li> <li>NEW: Internationalization Support<ul> <li>Added built-in translations for 9 languages: English, Spanish, French, Arabic, Hindi, Italian, Portuguese, Swahili, and Ukrainian</li> <li>New <code>language</code> property to set widget UI language (e.g., <code>language=\"es\"</code>)</li> <li>New <code>translations-url</code> property to load custom translations from a JSON file</li> <li>All UI strings can now be customized through translations</li> <li>Content properties (<code>button-text</code>, <code>header-text</code>, <code>welcome-messages</code>, <code>starter-questions</code>, <code>typing-indicator-text</code>) can be overridden in translation files</li> </ul> </li> </ul>"},{"location":"chat_widget/changelog/#deprecation-warnings","title":"Deprecation Warnings","text":"<ul> <li>The following HTML text attributes are now deprecated and will be removed in a future major release. You should migrate these to use the new translation system:<ul> <li><code>header-text</code> -&gt; <code>branding.headerText</code> translation key</li> <li><code>typing-indicator-text</code> -&gt; <code>status.typing</code> translation key</li> <li><code>new-chat-confirmation-message</code> -&gt; <code>modal.newChatBody</code> translation key</li> </ul> </li> </ul>"},{"location":"chat_widget/changelog/#upgrade-guide","title":"Upgrade Guide","text":"<ol> <li>No immediate action required - existing implementations continue to work unchanged</li> <li>To enable internationalization:<ul> <li>Add <code>language=\"xx\"</code> attribute for built-in language support</li> <li>Or add <code>translations-url=\"https://yoursite.com/translations.json\"</code> for custom translations</li> </ul> </li> <li>To migrate content to translations (recommended):<ul> <li>Create a custom translations JSON file with your content</li> <li>Remove corresponding deprecated HTML attributes and use the translation file instead</li> <li>See the internationalization documentation for details</li> </ul> </li> </ol>"},{"location":"chat_widget/changelog/#v048","title":"v0.4.8","text":"<ul> <li>Fix horizontal scrollbar styling.</li> <li>Improve scrolling behavior for new messages.</li> <li>Scroll to bottom when loading the window.</li> </ul>"},{"location":"chat_widget/changelog/#v047","title":"v0.4.7","text":"<ul> <li>Fix regression in font size consistency.</li> </ul>"},{"location":"chat_widget/changelog/#v046","title":"v0.4.6","text":"<ul> <li>Add support for sending messages with attachments.<ul> <li>Enabled by setting <code>allow-attachments=\"true\"</code></li> <li>See the file attachments section in the style guide for available CSS properties.</li> </ul> </li> <li>Update the 'Start a new session' icon.</li> <li>Add a confirmation dialog when starting a new chat.<ul> <li>Customize the text using the <code>new-chat-confirmation-message</code> attribute.</li> <li>See the confirmation dialog section in the style guide for available CSS properties.</li> </ul> </li> <li>Customize the typing indicator text using the <code>typing-indicator-text</code> attribute.</li> <li>Update message background and text colors.<ul> <li>See the messages section of the style guide.</li> </ul> </li> <li>Update link CSS styling.<ul> <li>New properties <code>--message-user-link-color</code>, <code>--message-assistant-link-color</code>, <code>--message-system-link-color</code>.</li> </ul> </li> <li>Removed unnecessary CSS properties for padding (<code>--*-padding*</code>).</li> <li>Added <code>--success-text-color</code> CSS property.</li> <li>Error handling improvements.</li> <li>Fix full screen mode layout.</li> </ul>"},{"location":"chat_widget/changelog/#v045","title":"v0.4.5","text":"<ul> <li>Internal API changes</li> </ul>"},{"location":"chat_widget/changelog/#v044","title":"v0.4.4","text":"<ul> <li>Merge width &amp; height vars:<ul> <li><code>--button-icon-width</code>, <code>--button-icon-height</code> -&gt; <code>--button-icon-size</code> </li> </ul> </li> <li>Fix launch button styling.<ul> <li>Correctly apply font size and borders.</li> </ul> </li> <li>Add variables to control header font and icon size:<ul> <li><code>--header-font-size</code> </li> <li><code>--header-button-icon-size</code> </li> </ul> </li> <li>Support for placing text in the window header using the <code>header-text</code>.<ul> <li>Use <code>--header-text-font-size</code> and <code>--header-text-color</code> to style it independently of the other header elements. </li> </ul> </li> </ul>"},{"location":"chat_widget/changelog/#v043","title":"v0.4.3","text":"<ul> <li>Fix markdown styling</li> <li>Allow customizing the chat window width and height using the following CSS vars:<ul> <li><code>--chat-window-width</code> </li> <li><code>--chat-window-height</code> </li> <li><code>--chat-window-fullscreen-width</code></li> <li>See the styling guide for details.</li> </ul> </li> <li>Change size units from <code>rem</code> to <code>em</code>.</li> </ul>"},{"location":"chat_widget/changelog/#v042","title":"v0.4.2","text":"<ul> <li>Fully configurable styling via CSS properties.</li> </ul>"},{"location":"chat_widget/changelog/#v041","title":"v0.4.1","text":"<ul> <li>Improved styling.</li> <li>Replaced 'expand' with 'fullscreen' mode.</li> </ul>"},{"location":"chat_widget/changelog/#attribute-changes","title":"Attribute changes","text":"<p>Added</p> <ul> <li><code>allow-full-screen</code>: Allow the user to make the chat window full screen. </li> </ul> <p>Removed</p> <ul> <li><code>expanded</code></li> </ul>"},{"location":"chat_widget/changelog/#v040","title":"v0.4.0","text":"<p>Warning</p> <p>This is a full rebuild of the widget and includes breaking changes. See the upgrade guide for details.</p> <ul> <li>Enhanced Mobile Experience<ul> <li>Improved responsive design for mobile devices</li> <li>Better touch interactions and scrolling</li> </ul> </li> <li>Draggable Chat Window<ul> <li>Desktop users can now drag the chat window to reposition it</li> </ul> </li> <li>Button Customization<ul> <li>You can now set a button icon and change the button shape</li> </ul> </li> <li>Welcome Messages &amp; Starter Questions<ul> <li>Display welcome messages when the chat opens</li> <li>Provide clickable starter questions to help users get started</li> <li>Both support rich markdown formatting</li> </ul> </li> <li>Session persistence across page loads<ul> <li>Store session data in browser local storage to allow resuming sessions across page loads. </li> </ul> </li> </ul>"},{"location":"chat_widget/changelog/#upgrading-from-03x","title":"Upgrading from 0.3.x","text":"<p>The minimal steps required to upgrade are to replace the <code>bot-url</code> attribute with the <code>chatbot-id</code> attribute:</p> <pre><code>  &lt;open-chat-studio-widget\n-     bot-url=\".../experiments/e/{CHATBOT_ID}/embed/start/\"\n+     chatbot-id=\"{CHATBOT_ID}\"\n  &lt;/open-chat-studio-widget&gt;\n</code></pre> <p>The <code>chatbot_id</code> can be extracted from the <code>bot-url</code> by copying the UUID from the URL as shown above.</p>"},{"location":"chat_widget/changelog/#upgrade-checklist","title":"Upgrade Checklist","text":"<p>\u2705 Check These Items</p> <p>Property Names: Ensure all property names use kebab-case (e.g., chatbot-id, not chatbotId) JSON Properties: Verify that welcome-messages and starter-questions are properly formatted JSON strings: <pre><code>&lt;!-- \u2705 Correct --&gt;\nwelcome-messages='[\"Message 1\", \"Message 2\"]'\n\n&lt;!-- \u274c Incorrect --&gt;\nwelcome-messages=\"Message 1, Message 2\"\n</code></pre></p> <p>API Base URL: If you were using a custom API URL, ensure the api-base-url property is set correctly</p> <p>Boolean Properties: Use string values for boolean properties: <pre><code>&lt;!-- \u2705 Correct --&gt;\nvisible=\"true\"\nexpanded=\"false\"\n\n&lt;!-- \u274c Incorrect --&gt;\nvisible={true}\nexpanded={false}\n</code></pre></p>"},{"location":"chat_widget/reference/","title":"Reference Docs","text":"<p>Learn how to customize the Open Chat Studio widget to match your brand and improve user experience.</p>"},{"location":"chat_widget/reference/#button-customization","title":"Button Customization","text":"<p>The widget button can be customized using the following properties:</p> <pre><code>&lt;open-chat-studio-widget\n  button-text=\"Chat with us\"\n  button-shape=\"round\"\n  icon-url=\"https://your-domain.com/custom-chat-icon.svg\"&gt;\n&lt;/open-chat-studio-widget&gt;\n</code></pre> <p>Button Text</p> <ul> <li>When button-text is provided, the button displays both icon and text</li> <li>When button-text is empty or not provided, only an icon is shown</li> </ul> <p>Button Shape</p> <ul> <li>round - Circular button</li> <li>square - Rectangular button with rounded corners</li> </ul> <p>Icon URL</p> <p>If no icon-url is provided, the default Open Chat Studio avatar is used.</p>"},{"location":"chat_widget/reference/#button-position","title":"Button position","text":"<p>Customize the button position using CSS variables or a CSS class attached to the widget element:</p> <pre><code>open-chat-studio-widget {\n    position: fixed;\n    right: 20px;\n    bottom: 20px;\n}\n</code></pre> <p>Drag the button</p> <p>The button can be dragged by the user to anywhere on the screen. This allows the user to move the button if it is obstructing other page content. The button will return to its original position on the next page load.</p> <ul> <li> See CSS Styling for more customization options.</li> </ul>"},{"location":"chat_widget/reference/#embed-authentication","title":"Embed Authentication","text":"<p>Secure your embedded widgets with authentication keys for controlled access to specific channels.</p>"},{"location":"chat_widget/reference/#overview","title":"Overview","text":"<p>The embed authentication feature allows you to:</p> <ul> <li>Restrict widget access to authorized embeddings only</li> <li>Authenticate specific embedded channel instances</li> <li>Provide secure access control for sensitive or premium content</li> <li>Track and manage different embedded deployments</li> </ul>"},{"location":"chat_widget/reference/#implementation","title":"Implementation","text":"<pre><code>&lt;open-chat-studio-widget\n  chatbot-id=\"your-chatbot-id\"\n  embed-key=\"your-secure-embed-key\"&gt;\n&lt;/open-chat-studio-widget&gt;\n</code></pre> <p>When an embed key is provided, it's automatically sent as an <code>X-Embed-Key</code> header with all API requests to authenticate the widget instance.</p>"},{"location":"chat_widget/reference/#user-identification","title":"User Identification","text":"<p>Control how users are identified across chat sessions to enable personalized experiences and session continuity.</p>"},{"location":"chat_widget/reference/#overview_1","title":"Overview","text":"<p>The chat widget uses user identification to:</p> <ul> <li>Maintain chat history across page reloads and different visits</li> <li>Separate conversations for different users on shared devices</li> <li>Personalize interactions with user names and context</li> <li>Enable analytics and user tracking in your chat system</li> </ul>"},{"location":"chat_widget/reference/#basic-implementation","title":"Basic Implementation","text":"<p>Anonymous Users (Default) <pre><code>&lt;open-chat-studio-widget\n  chatbot-id=\"your-chatbot-id\"&gt;\n&lt;/open-chat-studio-widget&gt;\n</code></pre> Identified Users <pre><code>&lt;open-chat-studio-widget\n  chatbot-id=\"your-chatbot-id\"\n  user-id=\"user_12345\"\n  user-name=\"Sarah Johnson\"&gt;\n&lt;/open-chat-studio-widget&gt;\n</code></pre></p>"},{"location":"chat_widget/reference/#auto-generated-user-ids","title":"Auto-Generated User IDs","text":"<p>When no user-id is provided, the widget automatically creates a unique identifier:</p> <ul> <li>Example: ocs:1703123456789_a7x9k2m8f</li> </ul> <p>Persistence Behavior:</p> <ul> <li>Same browser/device: ID persists across sessions</li> <li>Different browser/device: Gets new auto-generated ID</li> <li>Incognito mode: New ID that's cleared when session ends</li> </ul>"},{"location":"chat_widget/reference/#dynamic-user-management","title":"Dynamic User Management","text":"<p>Update user identification when authentication state changes: <pre><code>function updateChatUser(user) {\n  const widget = document.querySelector('open-chat-studio-widget');\n\n  if (user) {\n    widget.userId = user.id;\n    widget.userName = user.name;\n  }\n}\n</code></pre></p>"},{"location":"chat_widget/reference/#welcome-messages","title":"Welcome Messages","text":"<p>Enhance user experience by displaying personalized greeting messages when the chat opens. These messages appear as bot messages at the beginning of the conversation. Welcome messages are perfect for:</p> <ul> <li>Greeting users and introducing your bot's capabilities</li> <li>Providing context about what kind of help is available</li> <li>Creating a warm, engaging first impression</li> </ul> <p>Pass welcome messages as a JSON array string. Each message appears as a separate message bubble.</p> <pre><code>&lt;open-chat-studio-widget\n welcome-messages=\"['Hi! Welcome to our support chat.', 'How can I assist you today?']\"\n&gt;\n&lt;/open-chat-studio-widget&gt;\n</code></pre>"},{"location":"chat_widget/reference/#starter-questions","title":"Starter Questions","text":"<p>Accelerate user engagement with pre-defined clickable questions that address common queries. These starter questions help users quickly find what they're looking for without having to type, which improves the user experience. Starter questions are ideal for:</p> <ul> <li>Highlighting your most frequently asked questions</li> <li>Guiding users toward key features or information</li> <li>Improving accessibility for users who prefer clicking to typing</li> </ul> <p>These questions appear as blue-outlined buttons aligned to the right (similar to user messages), making it clear that they are user actions. When clicked, they automatically send that question as a user message, initiating the conversation flow. The starter questions disappear after the user clicks one or starts typing their own message.</p> <pre><code>&lt;open-chat-studio-widget\n starter-questions=\"[\n   'I need technical support',\n   'Tell me about pricing',\n   'Schedule a demo',\n   'Contact sales team'\n ]\"&gt;\n&lt;/open-chat-studio-widget&gt;\n</code></pre>"},{"location":"chat_widget/reference/#file-attachments","title":"File Attachments","text":"<p>Enable users to send files along with their messages. This feature is perfect for support scenarios where users need to share screenshots, documents, or other files.</p> <pre><code>&lt;open-chat-studio-widget\n allow-attachments=\"true\"&gt;\n&lt;/open-chat-studio-widget&gt;\n</code></pre>"},{"location":"chat_widget/reference/#supported-file-types","title":"Supported File Types","text":""},{"location":"chat_widget/reference/#documents","title":"Documents:","text":"<ul> <li>Text files: .txt</li> <li>PDF documents: .pdf</li> <li>Microsoft Word: .doc, .docx</li> <li>Microsoft Excel: .xls, .xlsx</li> <li>Spreadsheets: .csv</li> </ul>"},{"location":"chat_widget/reference/#images","title":"Images:","text":"<ul> <li>Common formats: .jpg, .jpeg, .png, .gif, .bmp, .webp</li> <li>Vector graphics: .svg</li> </ul>"},{"location":"chat_widget/reference/#media","title":"Media:","text":"<ul> <li>Video files: .mp4, .mov, .avi</li> <li>Audio files: .mp3, .wav</li> </ul>"},{"location":"chat_widget/reference/#file-size-limits","title":"File Size Limits","text":"<ul> <li>Maximum file size: 50MB per individual file</li> <li>Maximum total size: 50MB for all files combined in a single message</li> <li>Multiple files: Users can attach multiple files as long as the total doesn't exceed 50MB</li> </ul>"},{"location":"chat_widget/reference/#user-experience","title":"User Experience","text":"<ol> <li>Users click the paperclip icon next to the send button to select files</li> <li>Selected files appear in a preview area above the input field</li> <li>Files show name, size, and upload status</li> <li>Users can remove files before sending by clicking the X button</li> <li>Error messages appear for unsupported file types or files exceeding size limits</li> <li>Files are uploaded when the message is sent</li> </ol> <p>See CSS Styling for customization options</p>"},{"location":"chat_widget/reference/#internationalization","title":"Internationalization","text":"<p>The chat widget supports multiple languages and custom translations for all UI text elements.</p>"},{"location":"chat_widget/reference/#built-in-language-support","title":"Built-in Language Support","text":"<p>The widget includes built-in translations for the following languages:</p> <ul> <li>English (<code>en</code>) - Default</li> <li>Spanish (<code>es</code>)</li> <li>French (<code>fr</code>)</li> <li>Arabic (<code>ar</code>)</li> <li>Hindi (<code>hi</code>)</li> <li>Italian (<code>it</code>)</li> <li>Portuguese (<code>pt</code>)</li> <li>Swahili (<code>sw</code>)</li> <li>Ukrainian (<code>uk</code>)</li> </ul> <pre><code>&lt;open-chat-studio-widget\n  chatbot-id=\"your-chatbot-id\"\n  language=\"es\"&gt;\n&lt;/open-chat-studio-widget&gt;\n</code></pre>"},{"location":"chat_widget/reference/#custom-translations","title":"Custom Translations","text":"<p>You can provide custom translations using a JSON file hosted on your server:</p> <pre><code>&lt;open-chat-studio-widget\n  chatbot-id=\"your-chatbot-id\"\n  translations-url=\"https://yoursite.com/custom-translations.json\"&gt;\n&lt;/open-chat-studio-widget&gt;\n</code></pre>"},{"location":"chat_widget/reference/#translation-file-format","title":"Translation File Format","text":"<p>Provide translations as a flat JSON object that uses dot-notation keys. These keys loosely group strings by the part of the widget they affect:</p> <pre><code>{\n  \"launcher.open\": \"Open chat\",\n  \"window.close\": \"Close\",\n  \"window.newChat\": \"Start new chat\",\n  \"window.fullscreen\": \"Enter fullscreen\",\n  \"window.exitFullscreen\": \"Exit fullscreen\",\n  \"attach.add\": \"Attach files\",\n  \"attach.remove\": \"Remove file\",\n  \"attach.success\": \"File attached\",\n  \"status.starting\": \"Starting chat...\",\n  \"status.typing\": \"Preparing response\",\n  \"status.uploading\": \"Uploading\",\n  \"modal.newChatTitle\": \"Start New Chat\",\n  \"modal.newChatBody\": \"Starting a new chat will clear your current conversation. Continue?\",\n  \"modal.cancel\": \"Cancel\",\n  \"modal.confirm\": \"Confirm\",\n  \"composer.placeholder\": \"Type a message...\",\n  \"composer.send\": \"Send message\",\n  \"error.fileTooLarge\": \"File too large\",\n  \"error.totalTooLarge\": \"Total file size too large\",\n  \"error.unsupportedType\": \"Unsupported file type\",\n  \"error.connection\": \"Connection error. Please try again.\",\n  \"error.sessionExpired\": \"Session expired. Please start a new chat.\",\n  \"branding.poweredBy\": \"Powered by\",\n  \"branding.buttonText\": \"\",\n  \"branding.headerText\": \"\",\n  \"content.welcomeMessages\": [],\n  \"content.starterQuestions\": []\n}\n</code></pre>"},{"location":"chat_widget/reference/#translation-key-reference","title":"Translation Key Reference","text":"<p>Use the following reference when creating or updating translation bundles (mirrors the widget's <code>en.json</code>):</p> <ul> <li>launcher</li> <li><code>launcher.open</code> \u2014 Launcher button label, aria-label, and tooltip.</li> <li>window</li> <li><code>window.close</code> \u2014 Closes the chat window.</li> <li><code>window.newChat</code> \u2014 Menu item to start a new chat.</li> <li><code>window.fullscreen</code> \u2014 Enters fullscreen mode.</li> <li><code>window.exitFullscreen</code> \u2014 Leaves fullscreen mode.</li> <li>attach</li> <li><code>attach.add</code> \u2014 Adds file attachments.</li> <li><code>attach.remove</code> \u2014 Removes a pending attachment.</li> <li><code>attach.success</code> \u2014 Upload queued confirmation.</li> <li>status</li> <li><code>status.starting</code> \u2014 Shown while the session initializes.</li> <li><code>status.typing</code> \u2014 Typing indicator text (previously <code>typingIndicatorText</code>).</li> <li><code>status.uploading</code> \u2014 Attachment upload in progress.</li> <li>modal</li> <li><code>modal.newChatTitle</code> \u2014 \"Start new chat\" dialog title.</li> <li><code>modal.newChatBody</code> \u2014 Dialog body text.</li> <li><code>modal.cancel</code> \u2014 Dialog cancel button.</li> <li><code>modal.confirm</code> \u2014 Dialog confirmation button.</li> <li>composer</li> <li><code>composer.placeholder</code> \u2014 Message composer placeholder text.</li> <li><code>composer.send</code> \u2014 Send button text.</li> <li>error</li> <li><code>error.fileTooLarge</code> \u2014 Single file size violation.</li> <li><code>error.totalTooLarge</code> \u2014 Combined size violation.</li> <li><code>error.unsupportedType</code> \u2014 Unsupported file format.</li> <li><code>error.connection</code> \u2014 Generic connection failure.</li> <li><code>error.sessionExpired</code> \u2014 Session expiration prompt.</li> <li>branding</li> <li><code>branding.poweredBy</code> \u2014 Footer \"Powered by\" label.</li> <li><code>branding.buttonText</code> \u2014 Optional launcher text override; leave blank to use widget props.</li> <li><code>branding.headerText</code> \u2014 Optional header title override; leave blank to use widget props.</li> <li>content</li> <li><code>content.welcomeMessages</code> \u2014 Array of initial bot messages (empty array falls back to props).</li> <li><code>content.starterQuestions</code> \u2014 Array of starter questions (empty array falls back to props).</li> </ul>"},{"location":"chat_widget/reference/#customizable-content","title":"Customizable Content","text":"<p>You can override specific widget content through translations.</p> <p>If you only want to override some text, include just those keys in your custom translation file. The widget will use the values from the default language file for provided languages or fall back to English. Arrays must remain arrays, and null values in <code>branding.buttonText</code> defer to the runtime HTML attribute or prop.</p>"},{"location":"chat_widget/reference/#translation-priority","title":"Translation Priority","text":"<p>The widget uses the following priority order for text content:</p> <ol> <li>Custom translations from <code>translations-url</code> (highest priority)</li> <li>Built-in language translations (if <code>language</code> is specified)</li> <li>Widget props / HTML attributes (used when translation keys are null or missing - \u26a0\ufe0f DEPRECATED for long-term use)</li> <li>English defaults (lowest priority)</li> </ol> <p>Deprecation Notice</p> <p>HTML attributes for text content (<code>header-text</code>, <code>typing-indicator-text</code>, <code>new-chat-confirmation-message</code>) are deprecated and will be removed in a future major release. Please migrate to using the translations system for better internationalization support. Leave translation values blank when you want the widget props to supply the text instead of duplicating it.</p>"},{"location":"chat_widget/reference/#persistent-sessions","title":"Persistent Sessions","text":"<p>By default, the widget will save the chat messages in the browser local storage. This allows users to continue sessions after reloading the page or navigating to a new page. In addition to automatic session expiration, the user can also use the 'new chat' button to start a new session.</p> <p>To disable this feature, set the <code>persistent-session=\"false\"</code> attribute on the widget element.</p> <p>Note</p> <p>The session persistence is associated with the <code>chatbot-id</code>. If the <code>chatbot-id</code> changes, any previous session data will be ignored.</p> <p>The session data is set to expire after 24 hours. This is also configurable by using the <code>persistent-session-expire</code> attribute. The value is interpreted as \"the number of minutes since the last message before the session expires\". Setting this attribute to <code>0</code> will disable the expiration entirely.</p> <p>Note</p> <p>Session persistence works in conjunction with User Identification. Different users will have separate persistent sessions.</p>"},{"location":"chat_widget/reference/#properties-reference","title":"Properties Reference","text":""},{"location":"chat_widget/reference/#core-configuration","title":"Core Configuration","text":"Property Type Required Default Description Example <code>chatbot-id</code> <code>string</code> REQUIRED - Your chatbot ID from Open Chat Studio <code>\"183312ac-cbe5-4c91-9e7b-d9df96b088e4\"</code> <code>api-base-url</code> <code>string</code> Optional <code>\"https://chatbots.dimagi.com\"</code> API base URL for your Open Chat Studio instance <code>\"https://your-domain.com\"</code> <code>embed-key</code> <code>string</code> Optional <code>undefined</code> Authentication key for embedded channels <code>\"your-embed-auth-key\"</code>"},{"location":"chat_widget/reference/#button-ui-customization","title":"Button &amp; UI Customization","text":"Property Type Required Default Validation Description Example <code>button-text</code> <code>string</code> Optional <code>undefined</code> Max 50 chars Button display text. If empty, shows icon only <code>\"Need Help?\"</code> <code>button-shape</code> <code>string</code> Optional <code>\"square\"</code> <code>\"round\"</code> | <code>\"square\"</code> Button shape style <code>\"round\"</code> for circular button <code>icon-url</code> <code>string</code> Optional OCS default logo Valid URL Custom icon for the chat button <code>\"https://yoursite.com/chat-icon.svg\"</code> <code>visible</code> <code>boolean</code> Optional <code>false</code> <code>true</code> | <code>false</code> Show widget immediately on page load <code>\"true\"</code> to auto-open <code>position</code> <code>string</code> Optional <code>\"right\"</code> <code>\"left\"</code> | <code>\"center\"</code> | <code>\"right\"</code> Initial widget position on screen <code>\"left\"</code> for left side placement <code>header-text</code> <code>string</code> Optional <code>undefined</code> Max 100 chars \u26a0\ufe0f DEPRECATED: Text displayed in chat window header. Use <code>branding.headerText</code> in translations instead <code>\"Customer Support\"</code>"},{"location":"chat_widget/reference/#user-management","title":"User Management","text":"Property Type Required Default Validation Description Example <code>user-id</code> <code>string</code> Optional Auto-generated Alphanumeric + underscore/dash Unique user identifier for session continuityAuto-format: <code>ocs:1703123456789_a7x9k2m8f</code> <code>\"user_12345\"</code> or <code>\"customer@email.com\"</code> <code>user-name</code> <code>string</code> Optional <code>undefined</code> Max 200 chars Display name sent to chat API for personalization <code>\"John Smith\"</code> or <code>\"Customer #12345\"</code>"},{"location":"chat_widget/reference/#chat-behavior-sessions","title":"Chat Behavior &amp; Sessions","text":"Property Type Required Default Validation Description Example <code>persistent-session</code> <code>boolean</code> Optional <code>true</code> <code>true</code> | <code>false</code> Save chat history in browser localStorage <code>\"false\"</code> to disable session saving <code>persistent-session-expire</code> <code>number</code> Optional <code>1440</code> (24 hours) 0-43200 (30 days) Minutes before session expires <code>720</code> for 12 hours, <code>0</code> for never expire <code>allow-full-screen</code> <code>boolean</code> Optional <code>true</code> <code>true</code> | <code>false</code> Enable fullscreen mode button <code>\"false\"</code> to hide fullscreen option <code>allow-attachments</code> <code>boolean</code> Optional <code>false</code> <code>true</code> | <code>false</code> Enable file upload functionalityLimits: 50MB per file, 50MB total per message <code>\"true\"</code> to enable file uploads"},{"location":"chat_widget/reference/#messages-content","title":"Messages &amp; Content","text":"Property Type Required Default Validation Description Example <code>welcome-messages</code> <code>string</code> Optional <code>undefined</code> Valid JSON arrayMax: 5 messages, 500 chars each Welcome messages shown when chat opensFormat: <code>'[\"Message 1\", \"Message 2\"]'</code> <code>'[\"Welcome!\", \"How can I help?\"]'</code> <code>starter-questions</code> <code>string</code> Optional <code>undefined</code> Valid JSON arrayMax: 6 questions, 100 chars each Clickable question buttons to start conversationFormat: <code>'[\"Question 1\", \"Question 2\"]'</code> <code>'[\"Check my order\", \"Technical support\"]'</code> <code>typing-indicator-text</code> <code>string</code> Optional <code>\"Preparing response\"</code> Max 50 chars \u26a0\ufe0f DEPRECATED: Text shown while bot is typing. Use <code>status.typing</code> in translations instead <code>\"AI is thinking...\"</code> <code>new-chat-confirmation-message</code> <code>string</code> Optional <code>\"Starting a new chat will clear your current conversation. Continue?\"</code> Max 200 chars \u26a0\ufe0f DEPRECATED: Confirmation dialog text for new chat button. Use <code>modal.newChatBody</code> in translations instead <code>\"Start over? Your current chat will be lost.\"</code>"},{"location":"chat_widget/reference/#internationalization-translations","title":"Internationalization &amp; Translations","text":"Property Type Required Default Validation Description Example <code>language</code> <code>string</code> Optional <code>\"en\"</code> Valid language code Language code for widget UI (en, es, fr, ar, hi, it, pt, sw, uk) <code>\"es\"</code> for Spanish <code>translations-url</code> <code>string</code> Optional <code>undefined</code> Valid URL URL to custom JSON translations file for widget strings <code>\"https://yoursite.com/translations.json\"</code>"},{"location":"chat_widget/styling/","title":"CSS Styling","text":"<p>Customization of the widget appearance can be done using CSS variables as follows:</p> <pre><code>open-chat-studio-widget {\n    --button-background-color: #d1d5db;\n}\n</code></pre> <p>The following tables contain the full list of CSS variables that are available.</p>"},{"location":"chat_widget/styling/#launch-button","title":"Launch button","text":"<p>Customize the button position using CSS variables or a CSS class attached to the widget element:</p> <pre><code>open-chat-studio-widget {\n    position: fixed;\n    right: 20px;\n    bottom: 20px;\n}\n</code></pre> <p>Use the following CSS variables to style the button appearance.</p> Name Description <code>--button-background-color</code> Button background color  (#ffffff) <code>--button-background-color-hover</code> Button background color on hover  (#f3f4f6) <code>--button-border-color</code> Button border color  (#6b7280) <code>--button-border-color-hover</code> Button border color on hover  (#374151) <code>--button-font-size</code> Button text font size (0.875em) <code>--button-icon-size</code> Button icon size (1.5em) <code>--button-text-color</code> Button text color  (#111827) <code>--button-text-color-hover</code> Button text color on hover  (#1d4ed8)"},{"location":"chat_widget/styling/#chat-window","title":"Chat Window","text":"<p>Tip</p> <p>All font sizes and some margins / padding are relative to the widget element's font size which can be set using the <code>--chat-window-font-size</code> and <code>--button-font-size</code> variables.</p> Name Description <code>--chat-window-width</code> Chat window height in pixels or percent (25%) <code>--chat-window-height</code> Chat window width in pixels or percent (60%) <code>--chat-window-fullscreen-width</code> Chat window fullscreen width in pixels or percent (80%) <code>--chat-window-bg-color</code> Chat window background color  (#ffffff) <code>--chat-window-border-color</code> Chat window border color  (#d1d5db) <code>--chat-window-font-size</code> Default font size for text in the chat window (0.875em) <code>--chat-window-font-size-sm</code> Font size for small text in the chat window (0.75em) <code>--chat-window-shadow-color</code> Chat window shadow color  (rgba(0, 0, 0, 0.1))"},{"location":"chat_widget/styling/#header","title":"Header","text":"Name Description <code>--header-bg-color</code> Header background color (transparent) <code>--header-bg-hover-color</code> Header background color on hover  (#f9fafb) <code>--header-border-color</code> Header border color  (#f3f4f6) <code>--header-button-bg-hover-color</code> Header button background on hover  (#f3f4f6) <code>--header-button-text-color</code> Header button text color  (#6b7280) <code>--header-button-icon-size</code> Icon size for buttons in the header (1.5em) <code>--header-font-size</code> Header font size (1em) <code>--header-text-color</code> Color for the text in the header  (#525762) <code>--header-text-font-size</code> Font size for the text in the header (1em)"},{"location":"chat_widget/styling/#messages","title":"Messages","text":"Name Description <code>--message-user-bg-color</code> User message background color  (#e4edfb) <code>--message-user-text-color</code> User message text color  (#1f2937) <code>--message-user-link-color</code> User message link color  (#155dfc) <code>--message-assistant-bg-color</code> Assistant message background color  (#eae7e8) <code>--message-assistant-text-color</code> Assistant message text color (--message-user-text-color) <code>--message-assistant-link-color</code> Assistant message text color (--message-user-link-color) <code>--message-system-bg-color</code> System message background color  (#fbe4f8) <code>--message-system-text-color</code> System message text color (--message-user-text-color) <code>--message-system-link-color</code> System message text color (--message-user-link-color) <code>--message-timestamp-assistant-color</code> Assistant message timestamp color  (rgba(75, 85, 99, 0.7)) <code>--message-timestamp-color</code> User message timestamp color  (rgba(255, 255, 255, 0.7))"},{"location":"chat_widget/styling/#starter-question","title":"Starter question","text":"Name Description <code>--starter-question-bg-color</code> Starter question background color (transparent) <code>--starter-question-bg-hover-color</code> Starter question background on hover  (#eff6ff) <code>--starter-question-border-color</code> Starter question border color  (#3b82f6) <code>--starter-question-border-hover-color</code> Starter question border on hover  (#2563eb) <code>--starter-question-text-color</code> Starter question text color  (#3b82f6)"},{"location":"chat_widget/styling/#confirmation-dialog","title":"Confirmation dialog","text":"Name Description <code>--confirmation-overlay-bg-color</code> Confirmation dialog overlay background color  (rgba(0, 0, 0, 0.5)) <code>--confirmation-dialog-bg-color</code> Confirmation dialog background color (uses --chat-window-bg-color) <code>--confirmation-dialog-border-color</code> Confirmation dialog border color (uses --chat-window-border-color) <code>--confirmation-dialog-shadow-color</code> Confirmation dialog shadow color (uses --chat-window-shadow-color) <code>--confirmation-title-color</code> Confirmation dialog title text color  (#111827) <code>--confirmation-title-font-size</code> Confirmation dialog title font size (1.125em) <code>--confirmation-message-color</code> Confirmation dialog message text color (uses --loading-text-color) <code>--confirmation-message-font-size</code> Confirmation dialog message font size (1em) <code>--confirmation-button-cancel-bg-color</code> Cancel button background color (uses --button-background-color-hover) <code>--confirmation-button-cancel-bg-hover-color</code> Cancel button background on hover  (#e5e7eb) <code>--confirmation-button-cancel-text-color</code> Cancel button text color (uses --header-button-text-color) <code>--confirmation-button-confirm-bg-color</code> Confirm button background color (uses --error-text-color) <code>--confirmation-button-confirm-bg-hover-color</code> Confirm button background on hover (uses --error-text-color) <code>--confirmation-button-confirm-text-color</code> Confirm button text color (uses --send-button-text-color)"},{"location":"chat_widget/styling/#input-bar","title":"Input bar","text":"Name Description <code>--input-bg-color</code> Input area background color (transparent) <code>--input-border-color</code> Input field border color  (#d1d5db) <code>--input-outline-focus-color</code> Input field focus ring color  (#3b82f6) <code>--input-placeholder-color</code> Input placeholder text color  (#6b7280) <code>--input-text-color</code> Input text color  (#111827)"},{"location":"chat_widget/styling/#file-attachments","title":"File Attachments","text":"Name Description <code>--file-attachment-button-bg-color</code> Attach file button background color (transparent) <code>--file-attachment-button-bg-hover-color</code> Attach file button background hover color (--header-button-bg-hover-color) <code>--file-attachment-button-text-color</code> Attach file button text color (--header-button-text-color) <code>--file-attachment-button-text-disabled-color</code> Attach file button disabled text color (--send-button-text-disabled-color) <code>--selected-file-bg-color</code> Selected file item background color (--message-system-bg-color) <code>--selected-file-font-size</code> Selected file item font size (--chat-window-font-size-sm) <code>--selected-file-icon-size</code> Selected file item icon size (1.25em) <code>--selected-file-name-color</code> Selected file name color (--message-assistant-text-color) <code>--selected-file-remove-icon-color</code> Selected file remove icon color (--error-text-color) <code>--selected-file-remove-icon-hover-color</code> Selected file remove icon hover  (#dc2626) <code>--selected-file-size-color</code> Selected file size display color (--input-placeholder-color) <code>--selected-files-bg-color</code> Selected files container background color (--chat-window-bg-color) <code>--selected-files-border-color</code> Selected files container border color (--header-border-color) <code>--message-attachment-icon-size</code> Message attachment icon size (1em)"},{"location":"chat_widget/styling/#send-button","title":"Send button","text":"Name Description <code>--send-button-bg-color</code> Send button background color  (#3b82f6) <code>--send-button-bg-disabled-color</code> Send button background when disabled  (#d1d5db) <code>--send-button-bg-hover-color</code> Send button background on hover  (#2563eb) <code>--send-button-text-color</code> Send button text color  (#ffffff) <code>--send-button-text-disabled-color</code> Send button text when disabled  (#6b7280)"},{"location":"chat_widget/styling/#loading-indicators","title":"Loading indicators","text":"Name Description <code>--loading-spinner-fill-color</code> Loading spinner fill color  (#3b82f6) <code>--loading-spinner-size</code> Loading spinner size (1.25em) <code>--loading-spinner-track-color</code> Loading spinner track color  (#e5e7eb) <code>--loading-text-color</code> Loading text color  (#6b7280) <code>--typing-progress-bg-color</code> Typing progress bar background color  (#ade3ff)"},{"location":"chat_widget/styling/#markdown-code","title":"Markdown code","text":"<p>By default, the Markdown code colours are relative to the respective message text and background colours, but they can be overridden. </p> Name Description <code>--code-bg-assistant-color</code> Code background in assistant messages (--message-assistant-bg-color + 50% white) <code>--code-border-assistant-color</code> Code border in assistant messages (--message-assistant-bg-color + 10% black) <code>--code-text-assistant-color</code> Code text color in assistant messages (--message-assistant-text-color) <code>--code-bg-user-color</code> Code background in user messages (--message-user-bg-color + 20% white) <code>--code-border-user-color</code> Code border in user messages (--message-user-bg-color + 20% black) <code>--code-text-user-color</code> Code text color in user messages (--message-user-text-color)"},{"location":"chat_widget/styling/#error-message","title":"Error message","text":"Name Description <code>--error-text-color</code> Error text color  (#ef4444) <code>--success-text-color</code> Success text color  (#10b981)"},{"location":"chat_widget/styling/#z-index","title":"Z-Index","text":"<p>If the chatbot appears below other elements on the page you can increase the <code>z-index</code> of the chatbot by setting the <code>--chat-z-index</code> CSS variable. The default value is <code>50</code>.</p> <pre><code>open-chat-studio-widget {\n    --chat-z-index: 100;\n}\n</code></pre> <p>In some cases, it may also be necessary to reduce the z-index of other elements on the page.</p>"},{"location":"concepts/","title":"Conceptual Guide","text":"<p>This guide provides explanations of the key concepts behind the Open Chat Studio platform and AI applications more broadly.</p> <p>The conceptual guide does not cover step-by-step instructions or specific examples \u2014 those are found in the How-to guides.</p>"},{"location":"concepts/#terms","title":"Terms","text":"Assistant A chatbot that uses OpenAI`s Assistant API. Assistants can write and execute code and search and reference   information in uploaded files. Authentication Provider Authentication providers allow you to authenticate with external systems to access data or services. Channel The platform through which a chat occurs (e.g., WhatsApp, Telegram, Web, Slack). Consent Forms Forms that provide context to chatbot users on how their data will be used and who to contact regarding any concerns. Custom Actions Custom actions are a way to extend the functionality of the bot by integrating with external systems via HTTP APIs. Evaluations Evaluations is a testing system for measuring chatbot performance against different metrics. Events Events are a way to trigger actions in the bot based on specific conditions. Experiment The current name used in Open Chat Studio to refer to a chatbot. An experiment links all the configuration and data for a chatbot, including user sessions, data, actions, etc. Large Language Models (LLMs) Large language models are a type of AI model that can generate human-like text, images and audio. Messaging Provider Messaging providers hold the configuration required to send messages to users on a specific channel. Participant Data Data that persists across sessions and is tied to the same <code>User, Channel, Chatbot</code> scope. It helps retain long-term user preferences and contextual information beyond a single session. Pipelines A pipeline is a way to build a bot by combining one or more steps together. Prompt A prompt is the instructions that are given to the LLM to generate a response. Prompts can include text, source material, and other variables. Session The scope of conversations between a user and a chatbot within a specific channel. Sessions are isolated, ensuring data privacy and contextual continuity for the duration of an interaction. Source Material Additional information that can be included in the bot prompt using the <code>{source_material}</code> prompt variable. Tracing Provider Tracing providers hold the configuration required to send traces to the tracing provider. Versions The ability to create and manage different versions of a chatbot."},{"location":"concepts/assistants/","title":"OpenAI Assistants","text":"<p>Deprecation Warning</p> <p>OpenAI has deprecated Assistants and will completely remove support on 2026-08-26.</p> <p>Open Chat Studio supports all the features of Assistants in other ways. To find out about migrating away from Assistants see the migration guide.</p>"},{"location":"concepts/assistants/#syncing-with-openai","title":"Syncing with OpenAI","text":"<p>The in-sync status with OpenAI is automatically checked each time a user visits the edit screen of an assistant. If the assistant in OCS has the identical configurations and files with the assistant in OpenAI, then an in-sync status will appear under the assistant id: </p> <p>Otherwise, a warning will be displayed explaining what is out of sync. For example, in the image below, there are files uploaded in OpenAI that are not uploaded in OCS. This may result in unexpected behavior from the assistant. To resolve, upload the listed files in the edit screen. </p> <p>Assistants in versioned experiments</p> <p>Although an assistant cannot be modified in OCS once an experiment is released that references that assistant, it can still be modified in OpenAI. A new assistant in OpenAI will be created at the time of the experiment's release, and it is recommended not to modify that assistant to maintain the expected functionality of the released experiment.</p>"},{"location":"concepts/assistants/#archiving","title":"Archiving","text":"<ul> <li>Goal: Archiving an assistant in OCS deletes the associated assistant in OpenAI. This is an easy way to stop incurring costs and ensure that the assistant is closed for all experiments and pipelines that reference it within a project.</li> <li>OCS first checks if any published or unreleased experiments and pipelines reference the assistant. If so, the archival process is blocked, and a modal appears listing those experiments and pipelines preventing the archival. If only released versions reference the assistant, they are archived automatically.</li> <li>If any published or unreleased experiments reference the assistant, they are listed under the \u201cExperiments\u201d section of the modal. For published experiments, navigate to those versions and archive them. For unreleased versions, you must navigate to the version and either (1) remove the assistant reference or (2) archive the experiment.</li> <li>If a pipeline that references an assistant is not referenced by an experiment, the pipeline must be archived. These pipelines are listed under the \u201cPipelines\u201d section of the modal. Navigate to the pipeline and either archive it or remove the assistant reference to unblock the assistant archival.</li> <li>If a pipeline references an assistant and that pipeline is used by a published experiment, you must archive the experiment. These experiments are listed under the \u201cExperiments Referencing Pipeline\u201d section of the modal. The links direct you to the experiments where the pipeline is used. Navigate to the version listed in the modal and archive it.</li> </ul> <p>Archiving an assistant with versions</p> <p>If the assistant you are trying to archive has versions, the same checks apply to all versions of the assistant and are displayed together in the modal. Once confirmed, all assistant versions will be archived.</p>"},{"location":"concepts/channels/","title":"Channels","text":"<p>To enable users to interact with your bot through external social media platforms and similar services, OCS integrates with various messaging providers. This integration allows you to deploy your bot to external platforms. Once a platform is linked to your bot, users can communicate with it through that platform. In OCS, the term \"channel\" is synonymous with \"platform.\"</p> <p>The currently supported channels are:</p> <ul> <li>Telegram</li> <li>WhatsApp</li> <li>Facebook Messenger</li> <li>Slack</li> <li>API</li> <li>SureAdhere</li> </ul>"},{"location":"concepts/channels/#see-also","title":"See also","text":"<ul> <li>Deploying your bot to different channels</li> </ul>"},{"location":"concepts/consent/","title":"Consent Forms","text":"<p>Consent forms allow chatbot makers to provide context to chatbot users on how their data will be used, and who to contact regarding any concerns. Consent forms are displayed to users before they start interacting with the chatbot.</p> <p></p> <p>Using consent forms on WhatsApp, Telegram and other channels</p> <p>If you are deploying your chatbot on WhatsApp, Telegram or any channels other than the Web channel, you can still include consent forms in your chatbot by enabling the 'Conversational Consent' option for the chatbot.</p> <p>A default consent form is created for each team. You can customize this default form or create new forms by navigating to the \"Consent Forms\" section of the platform.</p>"},{"location":"concepts/consent/#what-to-put-in-a-consent-form","title":"What to put in a consent form","text":"<p>Some common elements you may want to include in a consent form are:</p> <ul> <li>A disclaimer stating that the accuracy of chatbot responses is not guaranteed.</li> <li>How you might use data from chatbot interactions.</li> <li>An email address and phone number for the relevant team responsible for managing the chatbot.</li> </ul>"},{"location":"concepts/custom_actions/","title":"Custom Actions","text":"<p>Custom Actions enable bots to communicate with external services via HTTP API calls.  This feature allows you to extend the functionality of your bot by integrating it with other services.</p> <p>This feature is analogous to the OpenAI's GPT Actions feature.</p>"},{"location":"concepts/custom_actions/#custom-action-fields","title":"Custom Action Fields","text":""},{"location":"concepts/custom_actions/#authentication-provider","title":"Authentication Provider","text":"<p>Before you create a Custom Action will need to create an Authentication Provider for your action to use (unless the API you are using does not require authentication). You can do this by navigating to the Authentication Providers section in Team Settings and creating a new Authentication Provider.</p>"},{"location":"concepts/custom_actions/#base-url","title":"Base URL","text":"<p>This is the URL of the external service you want to communicate with. For example: <code>https://www.example.com</code>. Only HTTPS URLs are supported.</p>"},{"location":"concepts/custom_actions/#api-schema","title":"API Schema","text":"<p>This is a JSON or YAML OpenAPI Schema document.</p> <p>You should be able to get this from the service you want to connect to. For example, the default location for the schema for FastAPI services is <code>/openapi.json</code> (https://fastapi.tiangolo.com/tutorial/first-steps/#openapi-and-json-schema).</p>"},{"location":"concepts/custom_actions/#how-custom-actions-work","title":"How Custom Actions work","text":"<p>When you create a custom action, each API endpoint in the OpenAPI schema will be available as a separate action in the Experiment configuration. This gives you full control over which actions are available to your bot.</p> <p>When you add a Custom Action to your Experiment, the bot will be able to make HTTP requests to the external service using the API endpoints you have configured. The bot will send the request and receive the response from the external service, which it can then use to generate a response to the user.</p>"},{"location":"concepts/events/","title":"Events","text":"<p>Open Chat Studio provides an event system that allows you to define actions triggered by specific events within a chat session. This functionality enables you to automate responses, manage session states, and enhance user interactions effectively.</p>"},{"location":"concepts/events/#overview","title":"Overview","text":"<p>Events in Open Chat Studio are categorized into two types:</p> <ol> <li>Static Events: Triggered by specific actions or occurrences within the chat session.</li> <li>Timeout Events: Triggered after a specified duration of inactivity following the last interaction.</li> </ol> <p>Each event has one action associated with it that is executed when the event occurs.</p>"},{"location":"concepts/events/#static-events","title":"Static Events","text":"<p>Static events are predefined triggers that occur based on specific actions or conditions within the chat session. The available static events are:</p> <ul> <li>Conversation End: Triggered when the conversation ends.</li> <li>Last Timeout: Triggered when the last timeout of any configured timeout events occur.</li> <li>Human Safety Layer Triggered: Triggered when the safety layer is activated by a message from the user.</li> <li>Bot Safety Layer Triggered: Triggered when the safety layer is activated by a response from the bot.</li> <li>Conversation Start: Triggered when a new conversation is started.</li> <li>New Human Message: Triggered when a new human message is received.</li> <li>New Bot Message: Triggered when a new bot message is received.</li> <li>Participant Joined Experiment: Triggered when a participant starts interacting with the bot for the very first time.</li> </ul>"},{"location":"concepts/events/#event-actions","title":"Event Actions","text":"<p>Each event is associated with one action. The available actions are:</p> <ul> <li>End the conversation: Ends the conversation with the user. See Resetting Sessions.</li> <li>Prompt the bot to message the user: Prompts the bot to message the user.</li> <li>Trigger a schedule: This will create a once off or recurring schedule. Each time the schedule is triggered, the bot will be prompted to message the user.</li> <li>Start a pipeline: This will run the given pipeline when the event triggers. The input to the pipeline can be configured.</li> </ul>"},{"location":"concepts/llm/","title":"Large Language Models (LLMs)","text":"<p>Definition</p> <p>A Large Language Model (or LLM) is a type of artificial intelligence software that is trained on a vast amount of text data. Its primary function is to understand, interpret, and generate human language. This training allows it to produce text-based responses, answer questions, translate between languages, and perform various other language-related tasks. </p> <p>The term \"large\" in its name refers to the extensive volume of data it has been trained on and the complexity of its design, enabling it to handle complex language tasks.</p> <p>The definition above was authored by the famous LLM that powers ChatGPT: GPT-4 developed by OpenAI.</p> <p>When building chatbots, an LLM powers the chatbot's ability to understand and respond to user inputs, effectively acting as the brain behind the chatbot.</p>"},{"location":"concepts/llm/#which-large-language-models-are-supported-by-open-chat-studio","title":"Which Large Language Models are supported by Open Chat Studio?","text":"<p>Open Chat Studio is developed to support a range of LLMs. The platform is designed to be flexible and can work with any LLM that has an API. The platform currently supports all the models provided by the following APIs:</p> <ul> <li>OpenAI</li> <li>Azure OpenAi</li> <li>Anthropic</li> <li>Groq</li> <li>Perplexity</li> <li>Deepseek</li> </ul>"},{"location":"concepts/llm/#model-configuration-parameters","title":"Model Configuration Parameters","text":""},{"location":"concepts/llm/#temperature","title":"Temperature","text":"<p>Temperature controls the creativity or randomness of the chatbot's responses.</p> <ul> <li>A low temperature (e.g., 0.1) makes the chatbot more deterministic, providing straightforward and predictable answers.</li> <li>A high temperature (e.g., 0.9) makes responses more creative, varied, or even surprising.</li> </ul>"},{"location":"concepts/llm/#example","title":"Example:","text":"<ul> <li>Low temperature: What's a dog? \u2192 A dog is a domesticated animal.</li> <li>High temperature: What's a dog? \u2192 A dog is a loyal companion, a furry friend who fills your life with wagging tails and boundless joy.</li> </ul> <p>The default temperature of 0.7 is a balanced choice designed to provide responses that are both varied and  interesting, while still being coherent.</p>"},{"location":"concepts/llm/#prompt","title":"Prompt","text":"<p>A prompt is the input or instructions given to the LLM to guide its response. It sets the context for the chatbot. Prompts can be as simple as a user question or as detailed as a conversation framework or role-play setup.</p>"},{"location":"concepts/llm/#example_1","title":"Example:","text":"<p>You are a helpful assistant. Answer questions clearly and concisely.</p>"},{"location":"concepts/llm/#tokens","title":"Tokens","text":"<p>Tokens are the building blocks of text that the LLM processes. A token might be a word, a part of a word, or even just punctuation.</p>"},{"location":"concepts/llm/#example_2","title":"Example:","text":"<p>The sentence \"Chatbots are cool.\" is broken into 4 tokens: <code>Chatbots | are | cool | .</code></p> <p>Tokens are important because they determine the cost and the processing complexity of an LLM's response.</p>"},{"location":"concepts/llm/#max-token-limit","title":"Max Token Limit","text":"<p>The max token limit is the maximum number of tokens the LLM can handle in a single interaction, including both the input (prompt) and output (response).</p>"},{"location":"concepts/llm/#example_3","title":"Example:","text":"<p>If the max token limit is 4096 tokens: - A long prompt with 2000 tokens leaves 2096 tokens available for the response.</p> <p>Understanding the token limit helps you create effective prompts without truncating responses.</p>"},{"location":"concepts/participant_data/","title":"Participant Data","text":"<p>Participant data is the information collected from participants during their interactions with the system.</p> <p>Participant data is unique to each combination of channel platform, channel identifier, and chatbot. This means that the data for each bot may be different. For example, if the same person interacts with two different bots over the same  channel, e.g. WhatsApp, the data for each bot will be different. Furthermore, if the same person interacts with the same bot over two different channels, e.g. WhatsApp and Telegram, the data for each channel will be different.</p> <p>The reason for this is to ensure that the data is only accessible to the bot that it is intended for. There is no way for the system to know whether the same person is interacting with the bot on different channels.</p> <p>Participant data can be viewed and edited on the \"Participant Details\" page. This page can be accessed by clicking on the participant's name in the list of participants on the \"Participants\" list page.</p> <p>You can also export and import participant data from the \"Participants\" list page.</p>"},{"location":"concepts/participant_data/#using-participant-data","title":"Using participant data","text":""},{"location":"concepts/participant_data/#prompt-variable","title":"Prompt Variable","text":"<p>You can access the participant data using the <code>{participant_data}</code> prompt variable. This variable is a JSON object that contains the data for the participant. You can use this data to personalize the responses from the bot. For example, you can use the participant's name to personalize the greeting. For more information on prompt variables see here.</p>"},{"location":"concepts/participant_data/#pipeline-nodes","title":"Pipeline Nodes","text":"<p>Other than using the prompt variable described above, there are also various pipeline nodes which allow you to access the participant data:</p> <ul> <li>Python node: This node allows you to access the participant data using Python code.</li> </ul> <p>For more information, see the node documentation.</p>"},{"location":"concepts/participant_data/#system-properties","title":"System properties","text":"<p>There is only one system property that is set automatically by the system and only if the user interacts with a bot via the web channel. This is the <code>timezone</code> property. It is set to the timezone of the participant's browser. This is useful for localizing datetime variables in prompts.</p>"},{"location":"concepts/participant_data/#updating-participant-data","title":"Updating participant data","text":"<p>You can manually update the participant data using the Web UI. Participant data can also be updated dynamically using the methods described below.</p>"},{"location":"concepts/participant_data/#tools","title":"Tools","text":"<p>Open Chat Studio provides some tools that allow bots to update the participant data. These tools are available in the \"Tools\" tab of the chatbot edit page.</p> <p>These tools allow the bot to update the data in real time as the user is interacting with the bot.</p>"},{"location":"concepts/participant_data/#pipelines-nodes","title":"Pipelines Nodes","text":"<p>Both the \"Update Participant Data Node\" and the \"Python Node\" can be used to make updates to participant data. The \"Update participant data\" node is primarily used in conjunction with events (see below). The \"Python Node\" can be used to update the data using Python code as part of any pipeline.</p> <p>For more information, see the node documentation.</p>"},{"location":"concepts/participant_data/#events","title":"Events","text":"<p>You can also update the participant data using events. This is useful if you want to update the data based on the context of the conversation. This method also allows you to specify the schema for the data that is being updated.</p> <p>An example of this is extracting tasks from the conversation history using a timeout event. The event could be configured to run 15 minutes after the last message was sent. The event would execute a pipeline which would extract the tasks from the conversation history and update the participant data using the appropriate pipeline node. In this example the following schema could be used:</p> <pre><code>{\n  \"tasks\": [\n    {\n      \"name\": \"name of the task\",\n      \"due_date\": \"due date of the task in the format YYYY-MM-DD\"\n    }\n  ]\n}\n</code></pre>"},{"location":"concepts/participant_data/#api","title":"API","text":"<p>You can also update the participant data using the API. This is useful if you want full control over the data or when you want to update the data based from an external system. You can use the following endpoint to update the participant data:</p> <p><code>POST /api/participants/</code></p> <pre><code>{\n  \"platform\": \"Name of the channel platform e.g. WhatsApp, Telegram etc.\",\n  \"identifier\": \"ID of the participant on the specified platform\",\n  \"name\": \"Optional name for the participant\",\n  \"data\": [\n    {\n      \"experiment\": \"ID of the experiment the data is for\",\n      \"data\": {\n        \"key\": \"value\"\n      }\n    }\n  ]\n}\n</code></pre> <p>See the API docs for more information on the API.</p>"},{"location":"concepts/prompt_variables/","title":"Prompt variables","text":"<p>Prompt variables are a great way to make your prompt dynamic or tailored to the participant by injecting data into specified placeholders. These variables are predefined and look like this:</p> <pre><code>{variable}\n</code></pre> <p>The following variables are currently supported:</p> <ul> <li><code>{source_material}</code> - The source material linked to your bot.</li> <li><code>{participant_data}</code> - Information specific to this participant, bot and channel. See here for more information. </li> <li><code>{current_datetime}</code> - This refers to the date and time at which the response is generated.</li> <li><code>{media}</code> - (pipelines only) This refers to the linked media collection.</li> <li><code>{temp_state}</code> - (pipelines only) Access to the pipeline temporary state. See Temporary State.</li> <li><code>{session_state}</code> - (pipelines only) Access to the session state. See Session State</li> </ul> <p>Localizing injected datetime</p> <p>The injected datetime will be localized to the participant's timezone if it exists in their participant data. When a participant uses the web UI, their browser's timezone will automatically be saved to their participant data.</p> <p>A note on prompt caching</p> <p>Some LLM providers, like OpenAI, use a technique called \"prompt caching\" to reduce latency and costs (See here). This happens automatically. However, caching is only effective for static data, i.e. data that does not change. To take full advantage of this caching mechanism, you should place prompt variables near the end of your prompt whenever possible</p>"},{"location":"concepts/prompt_variables/#nested-data-access","title":"Nested data access","text":"<p>The following prompt variables allow referencing specific parts of the data:</p> <ul> <li><code>{participant_data}</code></li> <li><code>{temp_state}</code></li> <li><code>{session_state}</code></li> </ul> <p>Subsets of the data can be accessed using dot notation. For example, if you have a participant data object that looks like this:</p> <pre><code>{\n  \"name\": \"John Doe\",\n  \"address\": {\n    \"street\": \"123 Main St\"\n  },\n  \"tasks\": [\n    {\n      \"name\": \"Fix the roof\"\n    }\n  ]\n}\n</code></pre> <p>You can access specific parts of the data using the following prompt variables:</p> <pre><code>{participant_data.name}\n{participant_data.address.street}\n{participant_data.tasks[0].name}  # lists are zero-indexed\n</code></pre>"},{"location":"concepts/sessions/","title":"Chat Sessions","text":""},{"location":"concepts/sessions/#overview","title":"Overview","text":"<p>Chat sessions in Open Chat Studio define the scope of conversations between a user and a chatbot within a specific channel. Sessions are isolated, ensuring data privacy and contextual continuity for the duration of an interaction.</p>"},{"location":"concepts/sessions/#session-scope","title":"Session Scope","text":"<p>A session is uniquely defined by:</p> <ul> <li>User: The individual engaging with the chatbot.</li> <li>Channel: The platform through which the chat occurs (e.g., WhatsApp, Telegram, Web, Slack). See channels.</li> <li>Chatbot: The specific chatbot handling the conversation.</li> </ul> <p>Each session is independent, meaning:</p> <ul> <li>The session's data is bound to that session only and is not shared with other sessions.</li> <li>When a user interacts with a chatbot, the bot receives the session's history to maintain context.</li> <li>Multi-Session Channels: Channels such as Web, API, and Slack allow multiple active sessions per user, enabling parallel conversations.</li> <li>Single-Session Channels: Platforms like WhatsApp, Telegram, and SureAdhere support only one active session per user at a time.</li> </ul>"},{"location":"concepts/sessions/#history-management","title":"History Management","text":"<ul> <li>As conversations progress, all previous messages within a session are stored as <code>history</code>.</li> <li>If the session history exceeds a predefined maximum length, it is summarized, and the bot will only receive:</li> <li>A summary of older interactions.</li> <li>The most recent exchanges to maintain context.</li> </ul>"},{"location":"concepts/sessions/#participant-data","title":"Participant Data","text":"<p>Aside from session-specific data, Open Chat Studio maintains participant data, which:</p> <ul> <li>Persists across sessions.</li> <li>Is tied to the same <code>User, Channel, Chatbot</code> scope.</li> <li>Helps retain long-term user preferences and contextual information beyond a single session.</li> </ul>"},{"location":"concepts/sessions/#anonymous-sessions","title":"Anonymous Sessions","text":"<p>On the Web channel, users can have anonymous sessions, where:</p> <ul> <li>Participant data is only available for the duration of the session.</li> <li>Since user identity cannot be verified, data cannot persist beyond the session.</li> </ul>"},{"location":"concepts/sessions/#resetting-sessions","title":"Resetting Sessions","text":"<p>For Single-Session Channels like WhatsApp and Telegram, the current session continues indefinitely. However, sessions can be reset either manually by the user or automatically using Events or the API. When a session is reset:</p> <ul> <li>The current session is marked as completed.</li> <li>A new session is started with a fresh history.</li> </ul> <p>This means that, aside from participant data, the bot loses all information about the previous conversation \u2014 including the fact that it even took place.</p>"},{"location":"concepts/sessions/#manual-resets","title":"Manual resets","text":"<p>The chat user can manually reset the session (start a new session) by sending the <code>/reset</code> command. This command is available on all channels except Web and Slack.</p>"},{"location":"concepts/sessions/#automatic-resets","title":"Automatic resets","text":"<p>There are two ways to automatically reset a session:</p> <ul> <li>Events: You can configure an event to end the current session when the event is triggered. This will not automatically create a new session; however, if the user sends a message after the session is ended, a new session will be created. See Events.</li> <li>API: When using the Trigger Bot Message API, you can set <code>\"start_new_session\": true</code>, which will end the current session and start a new one before messaging the user.</li> </ul> <p>By structuring sessions in this way, Open Chat Studio ensures privacy-conscious, context-aware, and seamless interactions across different communication channels.</p>"},{"location":"concepts/source_material/","title":"Source material","text":"<p>Source Material is a feature that allows you to provide specific content, information, or data which the chatbot can use as a reference or knowledge base. This is particularly useful for LLM-based chatbots, as it helps tailor the chatbot\u2019s responses to be more aligned with your specific needs or the theme of the chatbot.</p>"},{"location":"concepts/source_material/#how-to-use-source-material","title":"How to use source material","text":"<ul> <li> <p>Content Integration: You can integrate specific documents or text into the chatbot. This could be information on a given health area in an FAQs format, program-specific data, or any other relevant content that you want your chatbot to reference. </p> </li> <li> <p>Contextual Relevance: By providing this material, you're essentially giving the chatbot a more focused and relevant context to operate within. This means your chatbot can provide more accurate and tailored responses based on the Source Materials you've provided.</p> </li> <li> <p>Updating Information: Keep your Source Materials up-to-date. As you update program materials (for example guidelines for counselling or home visits), updating your Source Materials will help ensure your chatbot remains relevant and effective. </p> </li> </ul>"},{"location":"concepts/source_material/#best-practices","title":"Best Practices","text":"<ul> <li> <p>Relevance and Accuracy: Ensure the materials are relevant to the conversations your chatbot will engage in. </p> </li> <li> <p>Organization and Structure: Well-organized Source Materials make it easier for the chatbot to retrieve and use the information. It's useful to structure your content in a clear, concise manner, with appropriate labels for different sections. </p> </li> </ul>"},{"location":"concepts/source_material/#see-also","title":"See also","text":"<ul> <li>Prompt variables</li> <li>Add a_knowledge base</li> </ul>"},{"location":"concepts/tags/","title":"Tags","text":"<p>Tags are labels applied to chats and messages to categorize and organize interactions. For instance, tags can be used to mark messages that require follow-up or to segment users based on their interactions.</p> <p>Tags can be created using the \u201cManage Tags\u201d section.</p> <p></p>"},{"location":"concepts/tags/#types-of-tags","title":"Types Of Tags","text":"<p>There are 3 types of tags.</p> <ul> <li> <p>System tags These are tags generated by the system, such as those used in multi-prompt architectures to differentiate between parent and child bots.</p> </li> <li> <p>Session tags These tags are manually added to sessions.</p> </li> <li> <p>Message tags These tags are manually added to specific messages within a user session.</p> </li> </ul> <p></p>"},{"location":"concepts/versioning/","title":"Versioning","text":"<p>Versioning is now enabled by default for all projects on Open Chat Studio. This comes with a few important changes that modify the default behavior of the platform.</p>"},{"location":"concepts/versioning/#terms","title":"Terms","text":"<p>OCS uses the following terms:</p> <ul> <li> <p>Unreleased Version. This is the version of the chatbot that exists when you click the edit button on the experiment. It can also be considered a \"draft\" or that it has \"unsaved changes\".</p> </li> <li> <p>Published Version. This is the version that users will interact with through the web, WhatsApp or any other configured channel--including the public link.</p> </li> </ul> <p>A note on version functionality</p> <p>Once a version is made, it cannot be edited or modified. This ensures that the users' experience remains stable even if the authors may be changing the unreleased version.</p> <p>Chatting to the unreleased version</p> <p>To chat to the unreleased version, navigate to the Experiment home page and click on the speech bubble icon at the top right corner of the page. There, a drop down will say either \"Unreleased Version\" or \"Published Version\". Select the Unreleased Version, and that will open a web chat. Only bot authors can chat with the unreleased version as it is not available through channels.This is a change in the default behavior of the platform as prior to versioning, all channels chatted to the unreleased version at all times.</p>"},{"location":"concepts/versioning/#changing-the-published-version","title":"Changing the Published Version","text":"<p>The published version can be selected from any released version of the experiment. To modify which version is the published version:</p> <ul> <li>Select \"View Details\" of the version</li> <li>Press the \"Set as Published Version\" button at the button of the dialog box.</li> </ul> <p>Alternatively, when a new version is being created, it can be set as the published version by marking the checkbox \"Set as Published Version\".</p> <p>Only one version can be the published version at a time.</p>"},{"location":"concepts/versioning/#workflow","title":"Workflow","text":"<p>When a new experiment is first created, there exists two versions, a published version and a unreleased version as shown in the version table: </p> <p>Then, when you would like to create another version after making changes to the unreleased version, you can either press the create version button on the table, or navigate to the edit experiment page and scroll to the bottom to locate the create version button. Note: this button will only be enabled if changes have been made to the version. </p> <p>That will take you the the create new version page which will show you the difference between the previous version (note not the published version) and the unreleased version. Here, you can also set this newly created version as the published version. Also, there is an option to add a description to the version that will be shown in the version table to quickly remember the changes between versions. </p> <p>Tada! There you have a new released version! You will be directed back the experiment verisons table where it may take a few minutes for the version to be fully available. Then you can chat with the version and view its details. When you select view details it shows the the detailed specifications of that version and if you navigate to the bottom, you are able to set as the published version and archive from that screen. </p> <p>If you click on the webchat button, for an unpublished version, there will be a banner indication that it's the unpublished version, and which version it is: </p> <p>For this demo, I released a few more versions for this experiment and also changed the published version. To easily see which is the published version for the experiment, look right of the experiment name at the top of the experiment home screen at the icon in green. For this example, you'll see \"v2\" which indicates that the version 2 is the published version. You will also be able to see in the table looking at the published version row for the checkmark. </p> <p>Versioning experiments that use OpenAI Assistants</p> <p>Can this be done? Yes! When an experiment is released that has an OpenAI Assistant, there is no additional configuration required. However, please note that a read-only copy of the OpenAI Assistant is made in Open Chat Studio (see in the Assistants tab) and also in OpenAI. This includes all reference files. The existing OpenAI Assistant prior to creating the the version will still be available and be able to be modified in the unreleased experiment version.</p> <p>Modifying Assistants in OpenAI referenced by released versions</p> <p>As mentioned above, the copied assistant will be read-only in OCS, however, in OpenAI changes can still be made to that copy of the assistant. We recommend advising your team to not modify this assistant if it references a released version. This can cause unexpected behavior to the version and to its end users. To ensure that the released version acts as expected this assistant should remain as-is.</p>"},{"location":"concepts/chatbots/","title":"Chatbots","text":"<p>Info</p>"},{"location":"concepts/chatbots/#announcement-chatbots-feature-rolling-out","title":"Announcement: Chatbots Feature Rolling Out","text":"<p>We are excited to announce that we are releasing a new way to build chatbots in Open Chat Studio. You may already be using 'Pipelines' to build 'Experiments'. We are now transitioning to the term 'Chatbots' and away from 'Experiments'. This change is part of our ongoing effort to improve the user experience and make it easier for you to create and manage your chatbots. The term 'Experiment' will be phased out as we fully adopt 'Chatbots' instead. Bot building will shift from the current 'form-based' method to primarily using the pipeline approach. All existing experiments will continue to work without disruption during this rollout.</p> <p>Some key improvements include:</p> <ul> <li> <p>Simplified Workflow: Chatbots introduces a cleaner, more intuitive interface for building chatbots</p> </li> <li> <p>Streamlined Bot Building: We're transitioning from the 'form-based' approach to make pipelines the primary (and eventually only) method for bot building</p> </li> <li> <p>Enhanced Features: The new system allows building more bots with greater flexibility and complexity.</p> </li> <li> <p>Enhanced Features: Chatbots includes features that are not available to legacy 'Experiments' including:</p> <ul> <li>LLM tools such as 'web search', 'code interpreter', and 'file search'</li> </ul> </li> </ul> <p>Self-Managed Rollout: Team administrators can now control feature flags for their teams, allowing for more granular control over when new features are enabled. This means teams can adopt the Chatbots feature at their own pace rather than following a global rollout schedule.</p> <p>For more information check out our FAQ page</p>"},{"location":"concepts/chatbots/#what-is-a-chatbot","title":"What is a Chatbot?","text":"<p>A chatbot is a program that simulates conversation with people. It can use simple rules or advanced AI to understand and respond to messages. </p> <p>Within the context of Open Chat Studio, a chatbot is a specific configuration of a language model (LLM) that is designed to interact with users conversationally. It can be customized to perform various tasks, such as answering questions, providing information, or assisting with specific workflows.</p> <p>The specific way that a chatbot is configured with Open Chat Studio is through the use of a Pipeline. You can think of a pipeline as a flowchart that starts with user input on one end and ends with the chatbot\u2019s response at the other. Each message sent to the bot follows a specific path through the pipeline to generate the final output. </p> <p>This approach can be useful if you want to build a complex bot that performs different tasks depending on the user\u2019s request. Generally, trying to make a single bot prompt do multiple functions doesn\u2019t work well, so it is better to create multiple prompts for each task and then combine them using a Pipeline. </p>"},{"location":"concepts/chatbots/#what-is-a-pipeline","title":"What is a Pipeline?","text":"<p>As described above, a pipeline is a flowchart-like structure that defines how a chatbot processes user input and generates responses. It consists of a series of connected nodes, each representing a specific step in the workflow. Each node can perform a specific function, such as processing user input, generating responses, or integrating with external systems.</p> <p>The simplest pipeline is a single node that takes user input and generates a response. More complex pipelines can include multiple nodes that work together to create a more sophisticated interaction.</p> <p>You can read more about pipelines in the Pipelines documentation.</p>"},{"location":"concepts/chatbots/rollout_faq/","title":"Frequently Asked Questions","text":""},{"location":"concepts/chatbots/rollout_faq/#timeline","title":"Timeline","text":"<p>Now - September 10th: Team admins can enable the Chatbot Feature Flag for their team via the feature flag management page.</p> <p>September 10th: Chatbot Feature Flag is enabled for all teams</p> <p>October 1st: All legacy experiments are migrated to chatbots. Pipeline experiments are filtered out from the pipeline table.</p>"},{"location":"concepts/chatbots/rollout_faq/#are-my-existing-experiments-going-to-be-lost-after-the-upgrade-from-experiments-to-chatbots","title":"Are my existing Experiments going to be lost after the upgrade from Experiments to Chatbots?","text":"<p>No! This update is solely in the bot building experience and not in the end product the user sees. All existing experiments will be seamlessly transferred over to Chatbots without change in any of the chatbot functionality.</p>"},{"location":"concepts/chatbots/rollout_faq/#what-do-i-have-to-do-during-the-experiments-to-chatbots-transition","title":"What do I have to do during the Experiments to Chatbots transition?","text":"<p>Nothing! All of your experiments will be transferred over to use chatbots automatically. You will receive updates on the progress of the transition via banners on the site.</p>"},{"location":"concepts/chatbots/rollout_faq/#how-do-i-adjust-the-global-settings-of-my-chatbot","title":"How do I adjust the global settings of my chatbot?","text":"<p>Different from an Experiment, the settings have been moved to a tab on the Chatbot homepage.</p> The settings tab <p>Here, you are able to modify name, description, voice, tracing, consent, surveys, participant allowlist, and seed message.</p>"},{"location":"concepts/chatbots/rollout_faq/#how-can-i-control-feature-rollouts-for-my-team","title":"How can I control feature rollouts for my team?","text":"<p>Team administrators can manage the chatbot feature for their team.</p> <p>To access feature flag management:</p> <ol> <li>Navigate to your team's settings page</li> <li>Click on the \"Manage Feature Flags\" button</li> </ol> Feature Flag Management Feature Flag Management Page"},{"location":"concepts/collections/","title":"Collections","text":"<p>Collections are only supported with pipeline bots</p> <p>A collection in OCS refers to a collection of files. There are two types of collections:</p> <ul> <li>Media collection</li> <li>Indexed Collection (for RAG applications)</li> </ul>"},{"location":"concepts/collections/#adding-a-collection-to-a-bot","title":"Adding a collection to a bot","text":"<p>Navigate to the Collections section in the sidebar and click \"Add new\". Once the collection is created, you will be able to upload files to it.</p> <p>After your collection has been created and populated with files, you can link it to any LLM node.</p>"},{"location":"concepts/collections/#how-are-attachments-sent","title":"How are attachments sent?","text":"<p>Whenever your bot references a particular file or document, it will be sent to the user as an attachment. Depending on the channel, attachments are delivered in different ways.</p>"},{"location":"concepts/collections/#web-channels","title":"Web channels","text":"<p>Attachments are directly downloadable by clicking on them.</p>"},{"location":"concepts/collections/#multimedia-unsupported-channels","title":"Multimedia-unsupported channels","text":"<p>By default, attachments are sent as download links appended to the bot's message. The user will see the file name and a corresponding download link at the end of the message. These channels do not yet support sending multimedia files:</p> <ul> <li>Telegram</li> <li>WhatsApp (Turn.io provider)</li> <li>SureAdhere</li> <li>Facebook Messenger</li> <li>Slack</li> </ul>"},{"location":"concepts/collections/#multimedia-supported-channels","title":"Multimedia-supported channels","text":"<p>Channels that support sending multimedia files will receive each attachment as a separate message, following the bot's initial response. If a file type is not supported by the channel, or the file size exceeds the allowed limit, a download link will be appended to the bot's message instead. These channels support sending multimedia files:</p> <ul> <li>API - See the API documentation for more information</li> <li>WhatsApp (Twilio Provider) - Consult the Twilio docs for supported file types.</li> </ul>"},{"location":"concepts/collections/indexed/","title":"Indexed Collection (for RAG applications)","text":""},{"location":"concepts/collections/indexed/#when-should-i-use-this","title":"When should I use this?","text":"<p>When you want your bot's responses to be grounded in your uploaded documents.</p> <p>Definition</p> <p>Retrieval-Augmented Generation (RAG) is a technique where a language model retrieves relevant information from a set of documents to ground its answers in real data. Instead of relying solely on its built-in knowledge, the model uses indexes\u2014specialized databases that store document content as vectors (numerical representations of meaning). This makes it easy for the model to find and use the most relevant parts of your uploaded files when answering questions.</p> <p>Indexed collections will replace OpenAI Assistants' file search functionality in the future</p> <p>Consult the migration guide if you have assistants that you want to replace with indexed collections.</p> <p>If you\u2019ve used the OpenAI Assistants\u2019 file search capability in OCS, you\u2019ve already interacted with an index behind the scenes.</p> <p>In OCS, there are two types of indexes:</p> <ul> <li>Remote Index</li> <li>Local Index</li> </ul>"},{"location":"concepts/collections/indexed/#remote-index","title":"Remote Index","text":"<p>Remote indexes are hosted and managed by an LLM provider. Files and index configuration are uploaded to the provider, which maintains and manages the index. The embedding model used to create file embeddings is selected by the provider.</p>"},{"location":"concepts/collections/indexed/#supported-providers","title":"Supported providers","text":"<ul> <li>OpenAI (using the responses API)</li> </ul>"},{"location":"concepts/collections/indexed/#supported-file-types","title":"Supported file types","text":"<p>Supported files are determined by the selected provider:</p> <ul> <li>OpenAI - See the OpenAI docs</li> </ul>"},{"location":"concepts/collections/indexed/#local-index","title":"Local Index","text":"<p>Local indexes are a new feature in OCS. We are actively working to support additional file types and embedding models, allowing you to better customize your index with models that suit your needs.</p> <p>Local indexes are hosted and managed by OCS. When you create a local index, you can select which embedding model to use for generating file embeddings. Embedding models are provided by LLM providers. Just as different LLM models have varying strengths and weaknesses, different embedding models also have their own strengths and weaknesses. Thus, choosing the right embedding model is important to ensure the best performance for your specific use case.</p>"},{"location":"concepts/collections/indexed/#supported-providers_1","title":"Supported providers","text":"<ul> <li>OpenAI</li> </ul>"},{"location":"concepts/collections/indexed/#supported-file-types_1","title":"Supported file types","text":"<ul> <li>pdf</li> <li>txt</li> <li>csv</li> <li>docx</li> </ul>"},{"location":"concepts/collections/indexed/#supported-embedding-models","title":"Supported embedding models","text":"<p>You can see the supported embedding models for each provider when creating or editing the provider in your team settings.</p>"},{"location":"concepts/collections/indexed/#chunking-and-optimization","title":"Chunking and Optimization","text":"<p>When you upload a document to an index, it\u2019s broken up into smaller parts called chunks. These chunks are then converted into vectors and stored in the index. Chunking is a key part of how RAG works, as it affects how accurately the model can retrieve relevant information.</p> <p>In most cases, it will not be necessary to change the default chunking strategy, but you\u2019ll have the option to customize the chunking strategy for each set of uploaded files:</p> <ul> <li>Chunk size \u2013 how large each chunk is (in tokens)</li> <li>Chunk overlap \u2013 how much each chunk overlaps with the next, to preserve context</li> </ul> <p>Choosing the right chunking strategy can improve retrieval accuracy, especially for technical documents.</p> <p>This flexibility helps tailor the index to your use case\u2014whether it\u2019s short notes or long, complex reports.</p>"},{"location":"concepts/collections/indexed/#document-sources","title":"Document Sources","text":"<p>In addition to manually uploading documents to a collection, you can also configure document sources from which Open Chat Studio will automatically load and index documents.</p> <p>The primary advantage of document sources over manual uploads is that Open Chat Studio can check for updates periodically, which eliminates the need for manual updates.</p> <p>The following document source types are currently supported:</p>"},{"location":"concepts/collections/indexed/#confluence","title":"Confluence","text":"<p>Load pages from a Confluence site. Pages can be filtered using the space key, label, CQL, or individual page IDs.</p> <p>Authentication</p> <p>Use a Basic Auth authentication provider with your Atlassian username and use your API Key as the password.</p> <p>Configuration</p> Field Description Site URL The URL of the Confluence site (e.g. https://yoursite.atlassian.net/wiki) Max Pages The maximum number of pages to load Space Key Load pages from this space Label Load pages with this label CQL CQL query to use to search for pages to load Page IDs Load only these specific pages <p>Note</p> <p>Only one of the <code>Space Key</code>, <code>Lable</code>, <code>CQL</code> and <code>Page IDs</code> fields can be used at a time.</p>"},{"location":"concepts/collections/indexed/#github","title":"GitHub","text":"<p>Load pages from a GitHub repository. Files can be filtered by path and by matching patterns against the filenames.</p> <p>Authentication</p> <p>Use a Bearer Token authentication provider.</p> <p>Configuration</p> Field Description Repository URL GitHub repository URL (e.g. https://github.com/user/repo) Branch Git branch to sync from File Pattern File patterns to include. Prefix with '!' to exclude matching files. Path Filter Optional path prefix to filter files (e.g., docs/)"},{"location":"concepts/collections/media/","title":"Media collections","text":""},{"location":"concepts/collections/media/#when-should-i-use-this","title":"When should I use this?","text":"<p>When you want your bot to be able to send multimedia files to users.</p>"},{"location":"concepts/collections/media/#currently-supported-file-types","title":"Currently supported file types","text":"<p>Documents .txt, .pdf, .doc, .docx, .xls, .xlsx, .csv</p> <p>Images .jpg, .jpeg, .png, .gif, .bmp, .webp, .svg</p> <p>Video .mp4, .mov, .avi</p> <p>Audio .mp3, .wav</p> <p>Once a collection is linked, your bot will be able to send one or more files from it to users\u2014either as a download link or directly\u2014depending on the specific channel\u2019s support for the file type and file size.</p>"},{"location":"concepts/collections/media/#how-does-it-work","title":"How does it work?","text":""},{"location":"concepts/collections/media/#how-does-the-bot-know-when-to-attach-a-file","title":"How does the bot know when to attach a file?","text":"<p>When you create a collection and upload files to it, you'll be prompted to add a summary for each file. These summaries are included in the system prompt when you link a collection to your bot. This allows the bot to accurately determine when a particular file is relevant to a conversation.</p> <p>Additionally, OCS automatically provides the bot with a tool that enables it to attach files to its responses.</p> <p>The location in the prompt where these summaries are included is defined by the {media} prompt variable.</p> <p>Here\u2019s an example of how file details appear in the system prompt: <pre><code>You are a friendly assistant. Here's some files that you can attach to your responses when you think the user will benefit from it:\n{media}\n</code></pre> becomes</p> <pre><code>You are a friendly assitant. Here's some files that you can attach to your responses when you think the user will benefit from it:\n* File (id=22, content_type=image/png): This is an image of a border collie\n* File (id=23, content_type=application/pdf): This file contains information about the behaviours of border collies\n</code></pre>"},{"location":"concepts/evaluations/","title":"Evaluations","text":"<p>Evaluations is a testing system for measuring chatbot performance against different metrics.</p> <p>Evaluations can be run against existing conversation messages in the system or uploaded custom test datasets. Metrics can be defined either as python code, or output from an LLM.</p>"},{"location":"concepts/evaluations/#overview","title":"Overview","text":"<p>Evaluations are made up of a dataset and one or more evaluators.</p>"},{"location":"concepts/evaluations/#dataset","title":"Dataset","text":"<p>Datasets are collections of messages that serve as the foundation for running evaluations.</p> <p>Datasets can either be created directly from existing sessions, manually created in the UI, or uploaded with a CSV.</p>"},{"location":"concepts/evaluations/#evaluator","title":"Evaluator","text":"<p>Evaluators define the logic for analyzing messages and generating evaluation metrics. Each evaluator takes individual messages from a dataset and optionally a generated response, then outputs structured results in a table. You can apply many evaluators to a dataset in parallel, and the outputs of each will be added as new columns to the table.</p>"},{"location":"concepts/evaluations/#chatbot-generation","title":"Chatbot Generation","text":"<p>Messages can also optionally be passed in to a chatbot, whose generation output will be available to the evaluators.</p>"},{"location":"concepts/evaluations/#evaluation-execution","title":"Evaluation Execution","text":"<p>When an evaluation is run, each message from the dataset is first passed in to the defined chatbot (if applicaple). The result, with the added generation output is then passed in to each evaluator in parallel. The evaluators output structured data. This data is compiled into a table, whose rows are each message and the columns are the evaluator output.</p> <pre><code>flowchart LR\n    dataset([Dataset Message]) --&gt; generation{Chatbot Generation}\n    generation --&gt; evaluator1([Evaluator])\n    generation --&gt; evaluator2([Evaluator])\n    evaluator1 --&gt; structured_output([Structured Output])\n    evaluator2 --&gt; structured_output</code></pre>"},{"location":"concepts/evaluations/dataset/","title":"Evaluation Datasets","text":"<p>Datasets are collections of messages that serve as the foundation for running evaluations.</p> <p>Each dataset contains messages with the following structure:</p> <ul> <li>Input: The human message or prompt (required)</li> <li>Output: The expected AI response (required)</li> <li>Context: Additional metadata and context variables that can later be accessed in the evaluators (optional)</li> <li>History: Previous conversation messages for context (optional)</li> <li>Participant Data: Information about the participant that can be accessed during evaluation (optional)</li> <li>Session State: Session-specific state data that can be accessed during evaluation (optional)</li> </ul>"},{"location":"concepts/evaluations/dataset/#managing-datasets","title":"Managing Datasets","text":"<p>Datasets can be created by cloning an existing session, manually created in the UI, or uploaded with a CSV.</p>"},{"location":"concepts/evaluations/dataset/#cloning-a-session","title":"Cloning a session","text":"<p>When cloning a session, dataset messages are created automatically from messages from past conversations, including chat history and metadata. Selecting multiple sessions from the list will clone all the message from those sessions. Selecting \"filtered messages\" will only clone the messages that match the filter parameters. Selecting \"All messages\" will clone all the messages in that session.</p> <p>Messages that are cloned from a session will be \"connected\" to their actual message in OCS, and you will be able to follow links back to their original conversation when viewing the output of an evaluator. However, modifying or updating a cloned message will break this link.</p>"},{"location":"concepts/evaluations/dataset/#manually-creating-a-dataset","title":"Manually creating a dataset","text":"<p>Messages can be manually added in the UI. You must enter a Human Message, an AI response, and optionally the history for this message and any context.</p> <p>History must be entered as a new-line separated list of messages prepended with either <code>user:</code> or <code>assistant:</code>. For example:</p> <pre><code>user: Hello, how are you?\nassistant: I am doing well, thank you for asking. How can I help you?\nuser: Please tell me the time.\nassistant: It is currently 12:05 PM in Ankara.\n</code></pre>"},{"location":"concepts/evaluations/dataset/#csv-upload","title":"CSV Upload","text":"<p>A CSV can also be uploaded to populate the dataset. There should be columns for human messages, ai responses, and any context data. You can also include <code>participant_data</code> and <code>session_state</code> fields.</p> <p>An example structure for this csv might be:</p> Human Message AI Response Datetime History participant_data.name session_state.count What's the weather like? I don't have access to weather data 2024-03-15T10:30:00Z user: Helloassistant: Hi there!user: How are you?assistant: I'm doing well! John 1 Tell me a joke Why don't scientists trust atoms? Because they make up everything! 2024-03-15T10:32:00Z user: What's the weather like?assistant: I don't have access to weather data John 2 What is 2+2? 2+2 equals 4 2024-03-15T10:35:00Z Jane 1"},{"location":"concepts/evaluations/dataset/#participant-data-and-session-state","title":"Participant Data and Session State","text":"<p>When including participant_data or session_state in your CSV, you can use dot notation to specify nested keys. For example:</p> <ul> <li><code>participant_data.name</code> - Sets the participant's name</li> <li><code>participant_data.age</code> - Sets the participant's age</li> <li><code>session_state.counter</code> - Sets a session state counter</li> <li><code>context.foo</code> - Sets the <code>foo</code> field in the context</li> </ul> <p>You can also include complex JSON structures directly in these fields. For example, if you have a column named <code>participant_data.tasks</code> you can include JSON data like <code>[\"Buy socks\", \"Feed the dog\", \"Clean the car\"]</code>.</p> <p>You can also specify participant data and session state as raw JSON objects by using the keys <code>participant_data</code> and <code>session_state</code> (without dot notation).</p>"},{"location":"concepts/evaluations/dataset/#csv-history","title":"CSV History","text":"<p>When uploading a CSV, you can populate the history automatically from previous messages, or add it as a separate column.</p> <p>Adding the history automatically assumes the CSV is the transcript of a single conversation, in chronological order.</p>"},{"location":"concepts/evaluations/evaluators/","title":"Evaluators","text":"<p>Evaluators define the logic for analyzing messages and generating evaluation metrics. Each evaluator takes individual messages from a dataset and optionally a generated response, then outputs structured results in a table.</p>"},{"location":"concepts/evaluations/evaluators/#evaluator-types","title":"Evaluator Types","text":""},{"location":"concepts/evaluations/evaluators/#llm-evaluator","title":"LLM Evaluator","text":"<p>The LLM Evaluator uses language models to evaluate responses based on a custom prompt. This can be used as an LLM-as-judge to evaluate the performance of a chatbot, or to gain insight properties of both the user and assistant messages.</p> <p>Example prompt: <pre><code>Rate the helpfulness and accuracy of this response on a scale of 1-5:\n\nUser question: {input.content}\nExpected answer: {output.content}\nGenerated answer: {generated_response}\n\nConsider the conversation context: {context.topic}\n</code></pre></p> <p>Template Variables: The following variables are available to be used in the LLM prompt.</p> <ul> <li><code>{input.content}</code>: The human message content</li> <li><code>{output.content}</code>: The expected AI response content</li> <li><code>{generated_response}</code>: The generated response from your chatbot (if generation enabled)</li> <li><code>{context.[parameter]}</code>: Access any context variables, e.g., <code>{context.topic}</code></li> <li><code>{full_history}</code>: Complete conversation history as formatted text</li> </ul>"},{"location":"concepts/evaluations/evaluators/#output-schema","title":"Output Schema","text":"<p>The output schema defines the metrics that the LLM should attempt to output. These will always be output as a string. Each item in the schema will become a column in the output table.</p> Column Name Description expected_helpfulness The helpfulness, on a scale of 1-5 of the expected assistant message actual_helpfulness The helpfulness, on a scale of 1-5 of the actual assistant message user_sentiment The sentiment, on a scale of 1-5 of the user message"},{"location":"concepts/evaluations/evaluators/#python-evaluator","title":"Python Evaluator","text":"<p>The Python Evaluator allows custom code execution against each message.</p> <p>The code must define a <code>main</code> function which takes the <code>input</code>, <code>output</code>, <code>full_history</code>, and <code>generated_response</code>. It should return a <code>dict</code> whose keys will become columns in the output table.</p> <p>Function Arguments:</p> Argument Type Description Example <code>input</code> dict The human message data with <code>content</code> and <code>role</code> keys <code>{'content': 'What is 2+2?', 'role': 'human'}</code> <code>output</code> dict The expected AI response with <code>content</code> and <code>role</code> keys <code>{'content': '2+2 equals 4', 'role': 'ai'}</code> <code>context</code> dict Additional metadata and variables <code>{'topic': 'math', 'difficulty': 'easy', 'user_id': '123'}</code> <code>full_history</code> str Complete conversation history <code>\"user: Hello\\nassistant: Hi there!\\nuser: What is 2+2?\"</code> <code>generated_response</code> str AI-generated response being evaluated <code>\"The answer is 4. Is there anything else I can help with?\"</code> <p>Example:</p> <pre><code>def main(input: dict, output: dict, context: dict, full_history: str, generated_response: str, **kwargs) -&gt; dict:\n    \"\"\"Evaluates response quality based on accuracy, length, and politeness.\n    \"\"\"\n\n    expected_answer = output['content'].lower()\n    actual_answer = generated_response.lower()\n    has_correct_answer = expected_answer in actual_answer\n\n    response_length = len(generated_response.split())\n    is_polite = any(word in actual_answer for word in ['please', 'thank', 'help', 'happy'])\n\n    return {\n        'correct_answer': has_correct_answer,\n        'response_length': response_length,\n        'politeness_score': 1.0 if is_polite else 0.0,\n        'topic': context.get('topic', 'unknown')\n    }\n</code></pre>"},{"location":"concepts/experiment/","title":"Experiments","text":"<p>An 'Experiment' is the current name used in Open Chat Studio to refer to a 'chatbot'. This will soon be a legacy term as we transition fully to the term 'Chatbots'.</p> <p>An Experiment links all the configuration and data for a chatbot including user sessions, data, actions etc.</p> <p>Deprecation Warning</p> <p>The term will be phased out as we fully adopt 'Chatbots' instead. Bot building will shift from the current 'form-based' method to primarily using the pipeline approach. All existing experiments will be smoothly migrated with adequate notice, and users can contact the Dimagi team for assistance during this transition with any questions.</p>"},{"location":"concepts/experiment/#experiment-types","title":"Experiment Types","text":"<p>There are three different types of chatbots that you can build in Open Chat Studio:</p> <ul> <li>Base language model</li> <li>Assistant</li> <li>Pipeline</li> </ul>"},{"location":"concepts/experiment/#base-language-model","title":"Base language model","text":"<p>This kind of bot is the most commonly used and simple to configure. It is backed the standard language model APIs such as the OpenAI chat completions API, Anthropic messages API or Google Gemini API.</p> <p>Bots configured in this way have all the basic features (memory, source material etc.) and can also use some of the advanced features like Scheduling and Reminders.</p>"},{"location":"concepts/experiment/#assistant","title":"Assistant","text":"<p>Assistant bots make use of OpenAI Assistants. The main advantage of using Assistants is that your bot gets access to the OpenAI tools:</p>"},{"location":"concepts/experiment/#code-interpreter","title":"Code Interpreter","text":"<p>This allows the bot to write and execute code to accomplish tasks.</p> <p>For more information see the OpenAI docs.</p>"},{"location":"concepts/experiment/#file-search","title":"File Search","text":"<p>Warning</p> <p>The functionality described here is planned to be replaced by Indexed Collections in the future. It\u2019s recommended to start using Indexed Collections instead to ensure forward compatibility.</p> <p>This allows the bot to search and reference information provided in uploaded files. Unless your bot needs either of these capabilities, you should use a Base Language Model type bot.</p> <p>For more information see the OpenAI docs.</p>"},{"location":"concepts/experiment/#pipeline","title":"Pipeline","text":"<p>Pipelines allow you to create more complex bots by defining a \u2018graph\u2019 (in the computer science sense) of nodes. You can think of this graph as a workflow that flows from input to output. Each message to the bot is processed by the graph to produce a final output. A single response from the chatbot will be one successful path through the graph from the input node to the output node.</p> <p>This can be useful if you want to build a complex bot that performs different tasks depending on the user\u2019s request. Generally, trying to make a single bot prompt do multiple functions doesn\u2019t work well so it is better to create multiple prompts for each task and then combine them using a Pipeline. This is similar to the Multi-bot setup but allows more flexibility and complexity.</p>"},{"location":"concepts/pipelines/","title":"Pipelines","text":"<p>A pipeline is a way to build a bot by combining one or more steps together.</p> <p>Pipelines are the future</p> <p>Pipelines are currently becoming the default way to build bots in Open Chat Studio. They are a superset of existing functionality, enabling complex safety layers, routing and conditionals. The transition is now underway, and we're providing communication as we begin phasing out other bot building approaches.</p>"},{"location":"concepts/pipelines/#overview","title":"Overview","text":"<p>Here is an example of a very simple pipeline that uses an LLM to respond to the users input. This pipeline has a single step that uses the LLM to generate a response.</p> A simple pipeline <p>Analyzing this pipeline from left to right:</p> <ul> <li>the user sends a message to the bot (this is the <code>input</code>)</li> <li>the message is then passed to the LLM which generates a response</li> <li>the response is then sent back to the user (this is the <code>output</code>)</li> </ul> <pre><code>graph LR\n  A@{ shape: stadium, label: \"Input\" } --&gt; B(LLM);\n  B --&gt; C@{ shape: stadium, label: \"Output\" };</code></pre> <p>Each time a user sends a message to the bot, the pipeline is executed and the final output is sent back to the user.</p> <p>Each 'step' in a pipeline is called a 'node' and pipelines can have multiple nodes. To learn more about the different types of nodes that can be used in a pipeline, see the node types documentation.</p>"},{"location":"concepts/pipelines/#pipeline-execution","title":"Pipeline Execution","text":"<p>Open Chat Studio  runs your application in organized steps. Think of it like a well-coordinated team where different parts of your application (pipeline nodes) communicate through shared channels (pipeline edges / connections).</p> <p>Here's how each step works:</p> <p>Plan \u2192 Execute \u2192 Update \u2192 Repeat</p> <ol> <li> <p>Plan: Decide which nodes should run next. Initially, this includes nodes that need your input data. In later steps, it includes nodes that are ready to process new information.</p> </li> <li> <p>Execute: Run all selected nodes at the same time. Each node does its work independently and can't see changes from other nodes until the next step.</p> </li> <li> <p>Update: Share the results from all nodes so they're available for the next step.</p> </li> </ol> <p>This process repeats until either all work is complete or a step limit is reached. This approach ensures your application runs efficiently while maintaining predictable behavior.</p> <p>See Parallel Pipelines for information about running nodes in parallel.</p>"},{"location":"concepts/pipelines/history/","title":"History Modes","text":"<p>There are several supported history modes for LLM-based nodes in a pipeline. Each is designed to solve a unique problem. In complex pipelines it is expected that a variety of history modes will be used across different nodes.</p> <p>Only valid for LLM nodes</p> <p>Note that <code>history</code> is only applicable to nodes that have an LLM response as they affect the conversational history sent to that LLM during inference, or completion. </p>"},{"location":"concepts/pipelines/history/#no-history","title":"No History","text":"<p>Nodes will default to <code>No History</code> as their history mode. This means that when a completion is requested from the LLM, no conversational history will be supplied. One common use case might be a formatting or translation node where the previous history may not be applicable to generating the correct output.</p>"},{"location":"concepts/pipelines/history/#node","title":"Node","text":"<p><code>Node</code> history will maintain a specific history for this particular node. The input to the node will be saved, along with the output from the LLM. </p> <p>LLM output is not necessarily the same as node output</p> <p>In a LLM Router node, the <code>output</code> from the node will be the same as the <code>input</code> to that node. That is, once it has done its routing, it will be a passthrough for the <code>input</code>. The output of the LLM however, will be the classification label. This is an important distinction to keep in mind.</p> <p>A common use case will be in a LLM Router node where we want to maintain a history of the node outputs (e.g., for continuity of what 'part' of the chatbot the user is interacting with), and we want to ensure that the history is using LLM outputs so that we don't unintentionally supply the LLM with few-shot examples of the wrong type of output.</p>"},{"location":"concepts/pipelines/history/#global","title":"Global","text":"<p>Nodes with <code>Global</code> history will supply the conversational history that the user would see to the LLM. The simple example uses a global history as the user is interacting directly with a single LLM. </p>"},{"location":"concepts/pipelines/history/#named","title":"Named","text":"<p>The final history mode is called <code>Named</code> and allows you to specify a specific, named, history that can be shared between nodes. Each node using the same shared history will contribute their <code>input</code> and LLM output to the history.</p> <p>Named history is updated immediately</p> <p>If there are multiple nodes serially that use the same <code>Named</code> history, then each node will add to the history. In the case of serial nodes, this will result in multiple new history entries for every processed user message.</p> <p>The most common use case to this will be when we have multiple parallel nodes after an LLM Router. In the Advanced Pipelines Example, the general, quiz, and roleplay LLM nodes would all likely use the same shared history, giving each node visibility into the larger conversation.</p> <p>Note that for this particular example, each of the nodes could use a <code>Global</code> history to achieve the same thing. However, if there was a translation or formatting node at before the final <code>output</code>, then the <code>Named</code> history mode would enable the interim nodes to share a history in the original language / formatting.</p>"},{"location":"concepts/pipelines/history/#history-compression-management-options","title":"History Compression Management Options","text":"<p>In addition to the basic history modes, pipeline authors have access to advanced history management options. These controls appear after selecting a basic history mode (No History, Node, Global, or Named) and help optimize token usage and conversation length.</p> <p>The UI presents a \"History Mode\" dropdown with three options:</p>"},{"location":"concepts/pipelines/history/#summarize","title":"Summarize","text":"<p>The Summarize option compresses history when it exceeds a token limit by summarizing older messages while preserving more recent ones. If the token count exceeds the limit, older messages will be summarized while keeping the last few messages intact.</p> <p>Input field Token Limit: Sets the maximum number of tokens before summarization occurs. When this threshold is reached, the system will summarize older messages to reduce token count.</p>"},{"location":"concepts/pipelines/history/#truncate-tokens","title":"Truncate Tokens","text":"<p>The Truncate Tokens option removes older messages when a token limit is reached, ensuring the total token count stays below a specified threshold. If the token count exceeds the limit, older messages will be removed until the token count is below the limit.</p> <p>Input field Token Limit: Sets the maximum number of tokens before truncation occurs. When this threshold is reached, the system will remove older messages to reduce token count.</p>"},{"location":"concepts/pipelines/history/#max-history-length","title":"Max History Length","text":"<p>the last N (where N is the specified number) messages.</p> <p>Input field Max History Length: Specifies how many of the most recent messages to keep in the history. Only this number of messages will be sent to the LLM.</p> <p>These history management options help pipeline authors balance context preservation with performance and cost considerations, particularly for long-running conversations or complex applications.</p>"},{"location":"concepts/pipelines/nodes/","title":"Node Types","text":"<p>Note</p> <p>See cookbook for example usage. </p>"},{"location":"concepts/pipelines/nodes/#llm","title":"LLM","text":"<p>Use an LLM to respond to the node input. This node can be configured with a prompt to give the LLM instructions on how to respond. It can also be configured to use tools which enable it to perform additional actions.</p>"},{"location":"concepts/pipelines/nodes/#routers","title":"Routers","text":"<p>Router nodes allow you to route the input to one of the linked nodes. This is useful if you want your bot to behave differently depending on the input or some persistent context. For example, you might want to route the input to a different node if the user is asking for help with a specific topic.</p> <p>Router nodes share some common configuration such as the list of route options. Router nodes can also be configured to tag the output message with the selected route. This is useful for debugging and for tracking the flow of messages through the pipeline. The format of the tag is <code>&lt;node_name&gt;:&lt;route_name&gt;</code> where <code>&lt;route_name&gt;</code> is the name of the route selected by the router node.</p>"},{"location":"concepts/pipelines/nodes/#llm-router","title":"LLM Router","text":"<p>Routes the input to one of the linked nodes using an LLM. In this case, the LLM acts as a classifier using the prompt provided to classify an incoming message into a set of discrete categories that allow messages to be routed.</p> <p>The <code>outputs</code> listed by the node are the available classification labels. These should match the classification categories specified in your prompt. They can be adjusted through the <code>Advanced</code> settings for the node. The top output, which is prepended by a blue <code>*</code> is the default label. In the event that the LLM generates a response outside of the specified <code>outputs</code>, the route with the default label will be taken.</p> <p>Best practices for configuring a LLM Router</p> <p>It is advisable to use the Node history mode for an LLM Router to avoid unintentionally supplying few-shot examples to the node with an incorrect output format.</p>"},{"location":"concepts/pipelines/nodes/#static-router","title":"Static Router","text":"<p>The Static Router node allows you to route the input to one of the linked nodes based on the value of a specific key in the data source. This is useful if you want your bot to behave differently depending on the value of a specific key in the data source.</p> <p>The data source can be any of the following:</p> <ul> <li>Participant Data</li> <li>Session State</li> <li>Temporary State</li> </ul> <p>The key should be a name of a field in the data source and supports selecting nested fields via the <code>&lt;field&gt;.&lt;subfield&gt;</code> syntax. For example, if the data source is a JSON object with the following structure:</p> <pre><code>{\n    \"user\": {\n        \"name\": \"John\",\n        \"age\": 30\n    }\n}\n</code></pre> <p>You can select the <code>name</code> field using the key <code>user.name</code> and the <code>age</code> field using the key <code>user.age</code>.</p> <p>If the field is not present in the data source, the router will not route the input to the first linked node.</p>"},{"location":"concepts/pipelines/nodes/#assistant","title":"Assistant","text":"<p>Use an OpenAI assistant to respond to the input.</p>"},{"location":"concepts/pipelines/nodes/#python-node","title":"Python Node","text":"<p>The Python node allows the bot builder to execute custom Python code to perform logic, data processing, or other tasks.</p> <p>The code must define a <code>main</code> function which takes the node input as a string and returns a string to pass to the next node. The <code>main</code> function must also accept arbitrary keyword arguments to support future features. Here is an example of what the code might look like:</p> <pre><code>def main(input, **kwargs) -&gt; str:\n    # Put your code here\n    return input\n</code></pre> <p>The <code>input</code> parameter is a string that contains the input to the node. The return value of the function is a string that will be passed to the next node in the pipeline.</p>"},{"location":"concepts/pipelines/nodes/#additional-keyword-arguments","title":"Additional Keyword Arguments","text":"<p>The following additional arguments are provided:</p> <ul> <li><code>node_inputs: list[str]</code> - A list of all the inputs to the node at the time of execution. This will be the same as <code>[input]</code> except when the node is part of a workflow with parallel branches.</li> </ul> <p>Warning</p> <p>All the code must be encapsulated in a <code>main</code> function. You can write other functions but they must be within the scope of the <code>main</code> function. For example:</p> <pre><code>def main(input, **kwargs):\n    def important(arg):\n        return arg + \"!\"\n\n    return important(input)\n</code></pre>"},{"location":"concepts/pipelines/nodes/#utility-functions","title":"Utility Functions","text":"<p>The Python node provides a set of utility functions that can be used to interact with the user's data and the pipeline state.</p>"},{"location":"concepts/pipelines/nodes/#python_node.get_participant_data","title":"<code>get_participant_data() -&gt; dict</code>","text":"<p>Returns the current participant's data as a dictionary.</p>"},{"location":"concepts/pipelines/nodes/#python_node.set_participant_data","title":"<code>set_participant_data(data: dict) -&gt; None</code>","text":"<p>Updates the current participant's data with the provided dictionary. This will overwrite any existing data.</p>"},{"location":"concepts/pipelines/nodes/#python_node.set_participant_data_key","title":"<code>set_participant_data_key(key_name: str, data: any) -&gt; None</code>","text":"<p>Updates the current participant's data with the provided value at the specified key.</p>"},{"location":"concepts/pipelines/nodes/#python_node.append_to_participant_data_key","title":"<code>append_to_participant_data_key(key_name: str, data: any) -&gt; None</code>","text":"<p>Appends the provided value to the participant's data at the specified key. If the value at the key is not a list, it will be converted to a list containing the provided value.</p>"},{"location":"concepts/pipelines/nodes/#python_node.increment_participant_data_key","title":"<code>increment_participant_data_key(key_name: str, data: any) -&gt; None</code>","text":"<p>Increments the value at the participant's data key with the specified value</p>"},{"location":"concepts/pipelines/nodes/#python_node.get_participant_schedules","title":"<code>get_participant_schedules() -&gt; list</code>","text":"<p>Returns all active scheduled messages for the participant in the current experiment session.</p>"},{"location":"concepts/pipelines/nodes/#python_node.get_temp_state_key","title":"<code>get_temp_state_key(key_name: str) -&gt; str | None</code>","text":"<p>Returns the value of the temporary state key with the given name. If the key does not exist, it returns <code>None</code>.</p> <p>See also: Temporary State</p>"},{"location":"concepts/pipelines/nodes/#python_node.set_temp_state_key","title":"<code>set_temp_state_key(key_name: str, data: Any) -&gt; None</code>","text":"<p>Sets the value of the temporary state key with the given name to the provided data. This will override any existing data for the key.</p> <p>See also: Temporary State</p>"},{"location":"concepts/pipelines/nodes/#python_node.get_session_state_key","title":"<code>get_session_state_key(key_name: str) -&gt; str | None</code>","text":"<p>Returns the value of the session state key with the given name. If the key does not exist, it returns <code>None</code>.</p> <p>See also: Session State</p>"},{"location":"concepts/pipelines/nodes/#python_node.set_session_state_key","title":"<code>set_session_state_key(key_name: str, data: Any) -&gt; None</code>","text":"<p>Sets the value of the session state key with the given name to the provided data. This will override any existing data for the key.</p> <p>See also: Session State</p>"},{"location":"concepts/pipelines/nodes/#python_node.get_selected_route","title":"<code>get_selected_route(router_node_name: str) -&gt; str | None</code>","text":"<p>Returns the route selected by a specific router node with the given name. If the node does not exist or has no route defined, it returns <code>None</code>.</p>"},{"location":"concepts/pipelines/nodes/#python_node.get_node_path","title":"<code>get_node_path(node_name: str) -&gt; list | None</code>","text":"<p>Returns a list containing the sequence of nodes leading to the target node. If the node is not found in the pipeline path, returns a list containing only the specified node name.</p>"},{"location":"concepts/pipelines/nodes/#python_node.get_all_routes","title":"<code>get_all_routes() -&gt; dict</code>","text":"<p>Returns a dictionary containing all routing decisions made in the pipeline up to the current node. The keys are the node names and the values are the route keywords chosen by each router node.</p> <p>Note that in parallel workflows only the most recent route for a particular node will be returned.</p>"},{"location":"concepts/pipelines/nodes/#python_node.add_message_tag","title":"<code>add_message_tag(tag_name: str)</code>","text":"<p>Adds a tag to the output message. To add multiple tags, call this function multiple times.</p>"},{"location":"concepts/pipelines/nodes/#python_node.add_session_tag","title":"<code>add_session_tag(tag_name: str)</code>","text":"<p>Adds a tag to the chat session. To add multiple tags, call this function multiple times.</p>"},{"location":"concepts/pipelines/nodes/#python_node.get_node_output","title":"<code>get_node_output(node_name: str) -&gt; Any</code>","text":"<p>Returns the output of the specified node if it has been executed. If the node has not been executed, it returns <code>None</code>.</p>"},{"location":"concepts/pipelines/nodes/#python_node.require_node_outputs","title":"<code>require_node_outputs(*node_names)</code>","text":"<p>This function is used to ensure that the specified nodes have been executed and their outputs are available in the pipeline's state. If any of the specified nodes have not been executed, the node will not execute and the pipeline will wait for the required nodes to complete.</p> <p>This should be called at the start of the main function.</p> <pre><code>def main(input, **kwargs):\n    require_node_outputs(\"nodeA\", \"nodeB\")\n    return get_node_output(\"nodeA\") + get_node_output(\"nodeB\")\n</code></pre>"},{"location":"concepts/pipelines/nodes/#python_node.wait_for_next_input","title":"<code>wait_for_next_input()</code>","text":"<p>Advanced utility that will abort the current execution when not all inputs have been received. This is only useful in cases where the workflow has parallel branches which might result in the node being executed more than once.</p> <p>This is similar to <code>require_node_outputs</code> but useful where some node outputs may be optional.</p> <pre><code>def main(input, **kwargs):\n    a = get_node_output(\"a\")\n    b = get_node_output(\"b\")\n    if not a and not b:\n        wait_for_next_input()\n    # do something with a or b\n</code></pre>"},{"location":"concepts/pipelines/nodes/#python_node.abort_with_message","title":"<code>abort_with_message(message, tag_name: str = None) -&gt; None</code>","text":"<p>Calling this will terminate the pipeline execution. No further nodes will get executed in any branch of the pipeline graph.</p> <p>The message provided will be used to notify the user about the reason for the termination. If a tag name is provided, it will be used to tag the output message.</p> <pre><code># Abort pipeline with custom message\nif safety_violation_detected:\n    abort_with_message(\"Content policy violation detected\")\n</code></pre>"},{"location":"concepts/pipelines/nodes/#temporary-state","title":"Temporary State","text":"<p>The Python node can also access and modify the temporary state of the pipeline. The temporary state is a dictionary that is unique to each run of the pipeline (each new message from the user) and is not stored between sessions.</p> <p>The temporary state can be accessed and modified using the get_temp_state_key and set_temp_state_key utility functions.</p> <p>Temporary state contains the following keys by default. These keys can not be modified or deleted:</p> Key Description <code>user_input</code> The message sent by the user <code>outputs</code> The outputs generated by the previous node <code>attachments</code> A list of attachments passed in by the user. See Attachments <p>In addition to these keys, the temporary state can also contain custom key-value pairs that can be set and accessed by the Python node and by the Static Router node.</p> <p>Here is an example of a temporary state dictionary:</p> <pre><code>{\n    \"user_input\": \"Please help me with my problem\",\n    \"outputs\": {\n        \"Assistant\": \"I'm here to help! What can I do for you?\"\n    },\n    \"attachments\": [\n        Attachment(...),\n    ],\n    \"my_custom_key\": \"my_custom_value\",\n}\n</code></pre>"},{"location":"concepts/pipelines/nodes/#session-state","title":"Session State","text":"<p>The Python node can also access and modify the state of the participant's session. This state is a dictionary that is scoped to each session that the user might have with the bot.</p> <p>The session state can be accessed and modified using the get_session_state_key and set_session_state_key utility functions.</p>"},{"location":"concepts/pipelines/nodes/#attachments","title":"Attachments","text":"<p>Part of the temporary state is a list of attachments. Attachments are files that the user has uploaded to the bot. Each attachment has the following fields:</p> Field Description <code>name</code> The name of the file <code>size</code> The size of the file in bytes <code>content_type</code> The MIME type of the file <code>upload_to_assistant</code> Whether the file should be uploaded to the assistant as an attachment <code>read_bytes()</code> Reads the attachment content as bytes. <code>read_text()</code> Reads the attachment content as text. <p>Here is an example of an attachment object:</p> <pre><code>attachment = Attachment(\n    name=\"proposal.pdf\",\n    size=1234,\n    content_type=\"application/pdf\",\n    upload_to_assistant=False,\n)\ncontent = attachment.read_text()\n</code></pre>"},{"location":"concepts/pipelines/nodes/#supported-file-types","title":"Supported File Types","text":"<p>The Python node currently only supports reading the contents of the following file types:</p> <ul> <li>Text-based formats (TXT, CSV, HTML, JSON, XML, etc.)</li> <li>PDF</li> <li>DOCX</li> <li>XLSX</li> <li>XLS</li> <li>Outlook</li> <li>PPTX</li> </ul> <p>Other file types can still be uploaded to assistants but the Python Node is not able to read the file contents using the <code>read_text()</code> method on the attachment.</p>"},{"location":"concepts/pipelines/nodes/#template","title":"Template","text":"<p>Renders a Jinja template.</p>"},{"location":"concepts/pipelines/nodes/#available-template-variables","title":"Available Template Variables","text":"<p>The following variables are available in the template context:</p> Key Description Type <code>input</code> The input to the node String <code>node_inputs</code> The list of all inputs to the node in the case of parallel workflows List of strings <code>temp_state</code> Pipeline temporary state Dict <code>session_state</code> Session state Dict <code>participant_details</code> Participant details (<code>identifier</code>, <code>platform</code>) Dict <code>participant_data</code> Participant data Dict <code>participant_schedules</code> Participant schedule data List"},{"location":"concepts/pipelines/nodes/#sample-template","title":"Sample Template","text":"<pre><code>Input: {{ input }}\nNode Inputs: {{ node_inputs }}\nTemp State Key: {{ temp_state.my_key }}\nParticipant ID: {{ participant_details.identifier }}\nParticipant Platform: {{ participant_details.platform }}\nParticipant Data: {{ participant_data.custom_key }}\nSchedules: {{ participant_schedules }}\n</code></pre>"},{"location":"concepts/pipelines/nodes/#email","title":"Email","text":"<p>Send the input to the specified list of email addresses. This node acts as a passthrough, meaning the output will be identical to the input, allowing it to be used in a pipeline without affecting the conversation.</p>"},{"location":"concepts/pipelines/nodes/#extract-structured-data","title":"Extract Structured Data","text":"<p>Extract structured data from the input. This node acts as a passthrough, meaning the output will be identical to the input, allowing it to be used in a pipeline without affecting the conversation.</p>"},{"location":"concepts/pipelines/nodes/#update-participant-data","title":"Update Participant Data","text":"<p>Extract structured data and save it as participant data.</p>"},{"location":"concepts/pipelines/parallel/","title":"Parallel Pipelines","text":"<p>Nodes in a pipeline can run in parallel, allowing multiple operations to proceed simultaneously.</p> <pre><code>flowchart LR\n    start([Input]) --&gt; LLM1 &amp; LLM2\n    LLM1 --&gt; out([Output])\n    LLM2 --&gt; out</code></pre> <p>Limitations</p> <p>Cycles</p> <p>Configurations that result in cycles (recursive loops) are not supported.</p> <p>Multiple Exectuion</p> <p>In cases where the branches of a workflow do not have the same number of nodes and then merge, nodes after the merge will be executed more than once without special handling. See the section below on Uneven Banches</p>"},{"location":"concepts/pipelines/parallel/#dangling-nodes","title":"Dangling nodes","text":"<p>Nodes without connected outputs (dangling nodes) are supported and will execute in turn. The outputs of these nodes will still be recorded in the pipeline state.</p> <pre><code>flowchart LR\n    start([Input]) --&gt; LLM1 &amp; PythonNode\n    LLM1 --&gt; out([Output])\n    PythonNode</code></pre>"},{"location":"concepts/pipelines/parallel/#multiple-outputs","title":"Multiple outputs","text":"<p>Connecting multiple outputs from one node (e.g. a router node) to the output of another node is allowed. If more than one of the outputs from the node have a value, the first one will be passed to the next node as input.</p> <pre><code>flowchart LR\n    start([Input]) --&gt; Router\n    Router -.outputA.-&gt; PythonNode\n    Router -.outputB.-&gt; PythonNode\n    PythonNode --&gt; out([Output])</code></pre> <p>Outputs can also be connected to multiple other nodes:</p> <pre><code>flowchart LR\n    start([Input]) --&gt; Router\n    Router -.outputA.-&gt; PythonNode\n    Router -.outputB.-&gt; PythonNode\n    Router -.outputB.-&gt; LLM\n    LLM --&gt; out([Output])</code></pre>"},{"location":"concepts/pipelines/parallel/#uneven-branches","title":"Uneven branches","text":"<p>Consider the following graph:</p> <pre><code>flowchart LR\n    start([Input]) --&gt; NodeA\n    start --&gt; NodeB\n    NodeA --&gt; NodeC\n    NodeC --&gt; NodeD\n    NodeB --&gt; NodeD\n    NodeD --&gt; out([Output])</code></pre> <p>The execution steps are as follows:</p> <ol> <li><code>NodeA</code> and <code>NodeB</code> in parallel</li> <li><code>NodeC</code> and <code>NodeD</code> in parallel</li> <li><code>NodeD</code></li> </ol> <p>Notice how <code>NodeD</code> gets executed twice. The first time <code>NodeD</code> runs it will have the output from <code>NodeC</code> as it's input. The 2nd time it runs it will have both the outputs from <code>NodeB</code> and <code>NodeC</code> as its inputs.</p> <p>To understand why this happens you need to understand the execution model.</p> <p>You can manage this challenge by using a <code>PythonNode</code> with some utility functions:</p> <ul> <li><code>require_node_outputs</code>: This function will abort any node run if all the requested data is not available.</li> <li><code>wait_for_next_input</code>: This is a lower level function that can be used when <code>require_node_outputs</code> isn't suitable.</li> </ul> <p>In the example above, we could use the following code in <code>NodeD</code> to merge the outputs:</p> <pre><code>def main(input, **kwargs):\n    # this will abort the first run since only `NodeB` has outputs\n    require_node_outputs(\"NodeB\", \"NodeC\")\n    b = get_node_output(\"NodeB\")\n    c = get_node_output(\"NodeC\")\n    return f\"{b}\\n{c}\"\n</code></pre> <p>Using the lower level <code>wait_for_next_input</code> function we can do the same thing:</p> <pre><code>def main(input, **kwargs):\n    b = get_node_output(\"NodeB\")\n    c = get_node_output(\"NodeC\")\n    if b is None and c is None:\n        # abort until both are available\n        wait_for_next_input()\n    return f\"{b}\\n{c}\"\n</code></pre>"},{"location":"concepts/pipelines/parallel/#optional-parallel-branches","title":"Optional Parallel Branches","text":"<p>This shows a use case for the <code>wait_for_next_input</code> function. We have a pipeline which has parallel branches and a merge node but not all the branches will execute.</p> <pre><code>flowchart LR\n    start([Input]) --&gt; Router\n    start --&gt; NodeA\n    Router -.-&gt; NodeB\n    Router -.-&gt; NodeC\n    NodeA --&gt; Merge\n    NodeB --&gt; Merge\n    NodeC --&gt; Merge\n    Merge --&gt; out([Output])</code></pre> <p>The <code>Merge</code> node will get outputs from <code>NodeA</code> and either <code>NodeB</code> or <code>NodeC</code>. We can't use <code>require_node_outputs</code> because not all outputs will be generated. Instead we need to use the <code>wait_for_next_input</code> function:</p> Option 1Option 2 <pre><code>def main(input, **kwargs):\n    b = get_node_output(\"NodeB\")\n    c = get_node_output(\"NodeC\")\n    b_or_c = b or c\n    if not b_or_c:\n        # wait until we have either b or c \n        wait_for_next_input()\n    a = get_node_output(\"NodeA\")\n    return f\"{a}\\n{b_or_c}\"\n</code></pre> <p>Note that we don't need to check if we have output from <code>NodeA</code> since it will be guaranteed to be available by the time <code>NodeB</code> or <code>NodeC</code> execute due to the execution order.</p> <p>This option makes use of the <code>node_inputs</code> keyword argument which contains a list of all the inputs available to the current node execution. Since we want to wait until we have inputs from <code>NodeA and (NodeB or NodeC)</code> we can check that the inputs list has at least two values. </p> <pre><code>def main(input, **kwargs):\n    all_inputs = kwargs.get(\"node_inputs\", [])\n    if len(all_inputs) &lt; 2:\n        # wait until we have at least two inputs \n        wait_for_next_input()\n    return \"\\n\".join(all_inputs)\n</code></pre> <ul> <li> <p> More Example Workflows</p> <p> Workflow Cookbook</p> </li> </ul>"},{"location":"concepts/team/","title":"Teams","text":"<p>Open Chat Studio is a multitenant platform that can support multiple organizations using the same instance at the same time. Each 'tenant' is called a 'team'. Teams are created by an organization and can have multiple members. Each team has its own settings and experiments.</p> <p>A user can be a member of multiple teams and have a different set of permissions in each team.</p> <p>A team serves as the root container for all data in Open Chat Studio.</p>"},{"location":"concepts/team/#team-configuration","title":"Team configuration","text":"<p>There is a set of global configuration that can be set at the team level. This includes:</p> <ul> <li>LLM Service Providers</li> <li>Speech Service Providers</li> <li>Messaging Providers</li> <li>Authentication Providers</li> <li>Custom Actions</li> <li>Tracing providers</li> <li>User Management</li> </ul>"},{"location":"concepts/team/authentication_providers/","title":"Authentication Providers","text":"<p>Authentication Providers are used to authenticate with external services via HTTP API calls. Authentication Providers provide a centralized location to manage the credentials and tokens required to authenticate with external services.</p> <p>These credentials are used by features like Custom Actions.</p>"},{"location":"concepts/team/authentication_providers/#authentication-provider-types","title":"Authentication Provider Types","text":"<p>Open Chat Studio supports various different authentication types. You should select the type that matches the API service you will be using.</p>"},{"location":"concepts/team/authentication_providers/#basic-auth","title":"Basic Auth","text":"<p>Basic Auth is a simple authentication scheme built into the HTTP protocol.</p>"},{"location":"concepts/team/authentication_providers/#api-key","title":"API Key","text":"<p>API Key is a simple authentication scheme that involves sending a key with the request to authenticate the user. The key is sent in a header of the request. The name of the header can be customized when creating the Authentication Provider.</p>"},{"location":"concepts/team/authentication_providers/#bearer-token","title":"Bearer Token","text":"<p>Bearer Token is a type of access token that is sent with the request to authenticate the user. The token is sent in the Authorization header of the request.</p>"},{"location":"concepts/team/authentication_providers/#commcare","title":"CommCare","text":"<p>CommCare HQ uses a custom authentication scheme as described in the CommCare Documentation</p>"},{"location":"concepts/team/groups/","title":"User Groups on OCS","text":"<p>Users can be assigned to specific groups upon invitation to the OCS platform enabling tailored access to features and resources based on their role or requirements. Users can be put in one or multiple groups.</p>"},{"location":"concepts/team/groups/#permissions-table","title":"Permissions Table","text":"Permission Super Admin Team Admin Experiment Admin Chat Viewer Analysis Admin Analysis User Assistant Admin Event Admin Pipeline Admin Can See Experiments \u2705 \u274c \u2705 \u274c \u274c \u274c \u274c \u274c \u274c Can View Safety Layers, Source Material, Surveys, Consent Forms \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 Can See Tags \u2705 \u274c \u2705 \u274c \u274c \u274c \u274c \u274c \u274c Can Access Prompt Builder \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 Can View Graphs/Download Files \u2705 \u274c \u274c \u2705 \u274c \u274c \u274c \u274c \u274c Can Invite Participants \u2705 \u274c \u2705 \u274c \u274c \u274c \u274c \u274c \u274c Can Export Chat Transcripts \u2705 \u274c \u2705 \u274c \u274c \u274c \u274c \u274c \u274c Can Manage Assistants and Files \u2705 \u274c \u274c \u274c \u274c \u274c \u2705 \u274c \u274c Create and Manage Experiment Events \u2705 \u274c \u274c \u274c \u274c \u274c \u274c \u2705 \u274c Create and Manage Pipelines \u2705 \u274c \u274c \u274c \u274c \u274c \u274c \u274c \u2705 Additional Notes Full Access, Default Role - Cannot see sessions - - - - - -"},{"location":"concepts/team/llm_providers/","title":"LLM Service Providers","text":"<p>Building chatbots in Open Chat Studio requires access to LLMs. This requires providing Open Chat Studio with credentials needed to access the models provided by external services such as OpenAI, Anthropic, and Google.</p> <p>Open Chat Studio currently supports the following providers:</p> <ul> <li>OpenAI</li> <li>Anthropic</li> <li>Google Gemini</li> <li>Azure OpenAI</li> <li>Groq</li> <li>Perplexity</li> <li>DeepSeek</li> </ul>"},{"location":"concepts/team/llm_providers/#models","title":"Models","text":"<p>Each service provider is pre-configured with the most common models for both AI inference and text embedding. Should the service provider configuration not include a model which is available via the provider, it may be added directly via the user interface. This is done by editing the provider and using the  button in the 'Custom Models' section.</p>"},{"location":"concepts/team/messaging_providers/","title":"Messaging Providers","text":"<p>Messaging providers offer access to communication platforms such as WhatsApp, Facebook Messenger, Slack, and more. Connecting a chatbot to these services allows users to interact with the bot on the respective service.```</p>"},{"location":"concepts/team/messaging_providers/#supported-providers","title":"Supported providers","text":"<p>Below is a list of supported providers and their integrated platforms in OCS:</p> <ul> <li>Twilio<ul> <li>WhatsApp</li> <li>Facebook Messenger</li> </ul> </li> <li>Turn.io<ul> <li>WhatsApp</li> </ul> </li> <li>Slack</li> <li>SureAhdere</li> </ul>"},{"location":"concepts/team/messaging_providers/#see-also","title":"See also","text":"<ul> <li>Configure a messaging provider</li> </ul>"},{"location":"concepts/team/tracing_providers/","title":"Tracing Providers","text":"<p>LLM tracing involves capturing a model's output and decision-making process. Tracing providers offer platforms to visualize and analyze these traces, aiding bot developers in understanding their chatbot's behavior.</p> <p>Open Chat Studio (OCS) has built-in support for the following providers:</p> <ul> <li>LangFuse</li> <li>LangSmith</li> </ul> <p>Tracing will automatically begin with the next conversation after configuring a tracing provider.</p>"},{"location":"concepts/team/tracing_providers/#see-also","title":"See also","text":"<ul> <li>Configure a provider</li> </ul>"},{"location":"concepts/tools/","title":"Tools","text":"<p>Tools allow LLMs to affect change in the real world. An LLM on its own can only produce intentions, but it is not able to execute those intentions. Tools are a way of telling the LLM what requests it can make and of executing that request when it is made.</p> <p>Open Chat Studio provides a number of built-in tools as well as the ability to add your own tools in the form of Custom Actions.</p> <p>The current set of built-in tools are listed below. If you need to refer to the tool in a prompt, use the tool's name directly e.g. <code>update-user-data</code>.</p>"},{"location":"concepts/tools/#user-configurable-tools","title":"User configurable tools","text":""},{"location":"concepts/tools/#calculator","title":"Calculator","text":"<p>Allows to bot to do mathematical calculations reliably.</p> <ul> <li>Name: <code>calculator</code></li> <li>Arguments:</li> <li><code>expression</code>: The mathematical expression to evaluate.</li> </ul>"},{"location":"concepts/tools/#recurring-reminders","title":"Recurring reminders","text":"<p>Allows the bot to schedule recurring reminders for the participant.</p> <ul> <li>Name: <code>recurring-reminder</code></li> <li>Arguments:</li> <li><code>datetime_due</code>: The first (or only) reminder start date in ISO 8601 format.</li> <li><code>every</code>: Number of 'periods' to wait between reminders.</li> <li><code>period</code>: The time period used in conjunction with 'every'. One of <code>minutes</code>, <code>hours</code>, <code>days</code>, <code>weeks</code>, <code>months</code></li> <li><code>message</code>: The reminder message.</li> <li><code>schedule_name</code>: The name for this reminder.</li> <li><code>datetime_end</code>: The date of the last reminder in ISO 8601 format (optional).</li> <li><code>repetitions</code>: The number of messages to send before stopping (optional).</li> </ul>"},{"location":"concepts/tools/#one-off-reminder","title":"One-off Reminder","text":"<p>Allows the bot to schedule once-off reminders for the participant.</p> <ul> <li>Name: <code>one-off-reminder</code></li> <li>Arguments:</li> <li><code>datetime_due</code>: The datetime that the reminder is due in ISO 8601 format</li> <li><code>message</code>: The reminder message</li> <li><code>schedule_name</code>: The name for this reminder</li> </ul>"},{"location":"concepts/tools/#delete-reminder","title":"Delete Reminder","text":"<p>Allows the bot to delete existing reminders (either once-off or recurring)</p> <ul> <li>Name: <code>delete-reminder</code></li> <li>Arguments:</li> <li><code>message_id</code>: The ID of the scheduled message to delete.</li> </ul>"},{"location":"concepts/tools/#move-reminder-date","title":"Move Reminder Date","text":"<p>Allows the bot to update the reminder date</p> <ul> <li>Name: <code>move-scheduled-message-date</code></li> <li>Arguments:</li> <li><code>message_id</code>: The ID of the scheduled message to update.</li> <li><code>weekday</code>: The new day of the week (1-7 where 1 = Monday).</li> <li><code>hour</code>: The new hour of the day, in UTC.</li> <li><code>minute</code>: The new minute of the hour.</li> <li><code>specified_date</code>: A specific date to re-schedule the message for in ISO 8601 format</li> </ul>"},{"location":"concepts/tools/#update-participant-data","title":"Update Participant Data","text":"<p>Allows the bot to make changes to the participant data</p> <ul> <li>Name: <code>update-user-data</code></li> <li>Arguments:</li> <li><code>key</code>: The key in the user data to update.</li> <li><code>value</code>: The new value of the user data.</li> </ul>"},{"location":"concepts/tools/#append-to-participant-data","title":"Append to Participant Data","text":"<p>Append a value to participant data at a specific key. This will convert any existing value to a list and append the new value to the end of the list. Use this tool to track lists of items e.g. questions asked.</p> <ul> <li>Name: <code>append-to-participant-data</code></li> <li>Arguments:</li> <li><code>key</code>: The key in the user data to append to.</li> <li><code>value</code>: The value to append.</li> </ul>"},{"location":"concepts/tools/#increment-counter","title":"Increment Counter","text":"<p>Increment the value of a counter. The counter is stored in participant data with the key <code>_counter_{counter_name}</code>.</p> <ul> <li>Name: <code>increment-counter</code></li> <li>Arguments:</li> <li><code>counter</code>: The name of the counter to increment.</li> <li><code>value</code>: Integer value to increment the counter by (defaults to 1).</li> </ul>"},{"location":"concepts/tools/#end-session","title":"End Session","text":"<p>End the current chat session. This will mark the session as completed. New messages will result in a new session being created.</p> <ul> <li>Name: <code>end-session</code></li> <li>Arguments: (none)</li> </ul>"},{"location":"concepts/tools/#internal-tools","title":"Internal tools","text":"<p>The following tools are used internally by Open Chat Studio and enabled / disabled automatically depending on the chatbot configuration.</p>"},{"location":"concepts/tools/#attach-media","title":"Attach Media","text":"<p>Allows the bot to attach media when a media collection is configured. </p> <ul> <li>Name: <code>attach-media</code></li> <li>Arguments:</li> <li><code>file_id</code>: The ID of the media file to attach.</li> </ul>"},{"location":"concepts/tools/#file-search","title":"File Search","text":"<p>Allows the bot to search indexed collections when a collection is configured.</p> <ul> <li>Name: <code>file-search</code></li> <li>Arguments:</li> <li><code>query</code>: A natural language query to search for relevant information in the documents.</li> </ul>"},{"location":"concepts/tools/#llm-provider-tools","title":"LLM Provider Tools","text":"<p>In addition to the tools provided by Open Chat Studio, some LLM providers have their own set of tools which are executed interally by the provider.</p>"},{"location":"concepts/tools/#openai-tools","title":"OpenAI tools","text":""},{"location":"concepts/tools/#openai-web-search","title":"Web Search","text":"<ul> <li>Search the web and pass the results to the LLM.</li> <li>See https://platform.openai.com/docs/guides/tools-web-search</li> <li> Supported by OCS</li> </ul>"},{"location":"concepts/tools/#openai-code-interpreter","title":"Code Interpreter","text":"<ul> <li>Execute code to analyse data, generate graphs etc.</li> <li>See https://platform.openai.com/docs/guides/tools-code-interpreter</li> <li> Supported by OCS</li> </ul>"},{"location":"concepts/tools/#anthropic-tools","title":"Anthropic tools","text":""},{"location":"concepts/tools/#anthropic-web-search","title":"Web Search","text":"<ul> <li>Search the web and pass the results to the LLM.</li> <li>See https://docs.anthropic.com/en/docs/agents-and-tools/tool-use/web-search-tool</li> <li> Supported by OCS</li> </ul>"},{"location":"concepts/tools/#anthropic-code-execution","title":"Code Execution","text":"<ul> <li>Execute code to analyse data, generate graphs etc.</li> <li>See https://docs.anthropic.com/en/docs/agents-and-tools/tool-use/code-execution-tool</li> <li> Not supported by OCS</li> </ul>"},{"location":"concepts/tools/#gemini-tools","title":"Gemini tools","text":""},{"location":"concepts/tools/#gemini-web-search","title":"Grounding with search","text":"<ul> <li>Search the web and pass the results to the LLM.</li> <li>See https://ai.google.dev/gemini-api/docs/google-search</li> <li> Not supported by OCS</li> </ul>"},{"location":"concepts/tools/#gemini-code-execution","title":"Code Execution","text":"<ul> <li>Execute code to analyse data, generate graphs etc.</li> <li>See https://ai.google.dev/gemini-api/docs/code-execution</li> <li> Not supported by OCS</li> </ul>"},{"location":"how-to/","title":"How-to Guides","text":"<p>Here you\u2019ll find answers to \u201cHow do I...?\u201d types of questions. These guides are goal-oriented and concrete; they're meant to help you complete a specific task. </p> <p>For conceptual explanations, see the Conceptual guide.</p> <ul> <li> <p> Quickstart Guide</p> <p>If you're not sure where to start, start here!</p> <p> My first chatbot</p> </li> </ul>"},{"location":"how-to/add_a_knowledge_base/","title":"Add a knowledge base","text":"<p>Adding knowledge to your bot depends on the type of bot you are building.</p>"},{"location":"how-to/add_a_knowledge_base/#base-llm-and-pipeline","title":"Base LLM and Pipeline","text":""},{"location":"how-to/add_a_knowledge_base/#add-your-source-material","title":"Add your source material","text":"<p>Select the Source Material tab on the left-hand menu and click Add new</p>"},{"location":"how-to/add_a_knowledge_base/#select-the-source-material-for-your-bot","title":"Select the source material for your bot","text":"<p>Once you\u2019ve created your source material, it should appear in the list of source materials when editing your bot.</p>"},{"location":"how-to/add_a_knowledge_base/#reference-the-source-material-in-your-prompt","title":"Reference the source material in your prompt","text":"<p>To reference the source material, include the <code>{source_material}</code> prompt variable in your prompt. Be mindful of its placement\u2014it\u2019s best to include it in a separate section rather than within a sentence.</p> <p>Example prompt:</p> <pre><code>You are a friendly bot. Be sure to reference the source material before answering the user's query: \n\n### Source material\n{source_material}\n</code></pre>"},{"location":"how-to/add_a_knowledge_base/#assistant","title":"Assistant","text":"<p>To add knowledge to your assistant, you must upload files to serve as the source material. When creating or editing your assistant, select the file_search or code_interpreter checkboxes to allow the assistant to read files.</p> <ul> <li>File search: This allows the bot to search and reference information provided in uploaded files.</li> <li>Code Interpreter: This allows the bot to write and execute code to accomplish tasks.</li> </ul>"},{"location":"how-to/add_a_knowledge_base/#see-also","title":"See also","text":"<ul> <li>Source Material</li> </ul>"},{"location":"how-to/api_access/","title":"API access","text":"<p>Using the API allows you to interact with your bot programmatically. This comes in especially useful when you're using a thrid party system to evaluate your bot. API access is enabled by default for all bots. To get started, you will need an API key, which you can generate by going to your profile page.</p> <p>See the API documentation for more details.</p>"},{"location":"how-to/assistants_migration/","title":"Migrate Assistants","text":"<p>OpenAI has deprecated Assistants and will completely remove support on 2026-08-26.</p> <p>Open Chat Studio supports all the features of Assistants in alternative ways as shown in the table below:</p> Assistant Feature Replacement Feature Threads Open Chat Studio sessions Code Interpreter OpenAI Code Interpreter tool in LLM nodes File Search Indexed Collections"},{"location":"how-to/assistants_migration/#migrating-code-interpreter","title":"Migrating Code Interpreter","text":"<p>Info</p> <p>This guide assumes that you have enabled the chatbots feature</p> <p>To use OpenAI's code interpreter tool without using Assistants:</p> <ul> <li>Create a Chatbot with an LLM node.</li> <li>Select an OpenAI LLM Provider</li> <li>Check the \"Code Execution\" checkbox under the \"Builtin Tools\" section of the configuration.</li> </ul>"},{"location":"how-to/assistants_migration/#migrating-file-search","title":"Migrating File Search","text":"<p>Make sure you're familiar with the different types of Collections before continuing.</p>"},{"location":"how-to/assistants_migration/#general-steps","title":"General Steps","text":"<ol> <li>Create an indexed collection using the same files your assistant uses under its \"file search\" tool.</li> <li>Set up or update your chatbot to reference this collection.</li> </ol>"},{"location":"how-to/assistants_migration/#step-1-create-the-collection","title":"Step 1: Create the Collection","text":"<p>Click on the \"Collections\" tab in the sidebar and click the \"Create from Assistant\" button in the top right.</p> <ul> <li>Select the assistant you'd like to migrate.</li> <li>Give your new collection a name.</li> <li>Click \"Create Collection\".</li> </ul>"},{"location":"how-to/assistants_migration/#what-happens-behind-the-scenes","title":"What Happens Behind the Scenes?","text":"<ul> <li>A new indexed collection is created using the same LLM provider as your assistant.</li> <li>All files from the assistant\u2019s \"file search\" tool are copied to this new collection.</li> <li>A vector store is created at OpenAI for the collection.</li> <li>The assistant\u2019s original vector store and files remain unchanged.</li> </ul>"},{"location":"how-to/assistants_migration/#step-2-update-your-chatbot","title":"Step 2: Update your chatbot","text":"<p>Once your collection is created:</p> <ul> <li>Open your Chatbot's pipeline editor.</li> <li>Add an LLM node to the pipeline. If you have been using an assistant node, this LLM node should replace your assistant node.</li> <li>Within the node, select your newly created indexed collection.</li> </ul> <p>You're done!</p>"},{"location":"how-to/configure_providers/","title":"Configure Providers","text":"<p>Providers are configured in your team settings under \"LLM and Embedding Model Service Providers\". Before configuring a provider, ensure that you have an active account at the provider and access to the necessary integration credentials.</p>"},{"location":"how-to/configure_providers/#prerequisites","title":"Prerequisites","text":"<p>API Key Required: You cannot create a provider without a valid API key. The API key is used for authentication between Open Chat Studio and the provider. You must:</p> <ol> <li>Have an active account with the provider (OpenAI, Anthropic, Google, etc.)</li> <li>Generate an API key from your provider account</li> <li>Have access to the specific models you want to use</li> </ol>"},{"location":"how-to/configure_providers/#adding-a-new-provider","title":"Adding a New Provider","text":"<ol> <li>Go to your team settings</li> <li>Navigate to \"LLM and Embedding Model Service Providers\"</li> <li>Click \"Add Provider\"</li> <li>Select your provider from the dropdown</li> <li>Enter your API key</li> <li>Save the configuration</li> </ol>"},{"location":"how-to/configure_providers/#adding-custom-models","title":"Adding Custom Models","text":"<p>If your provider doesn't have a pre-configured model you want to use, you can add it under the \"Custom LLM Models\" section.</p>"},{"location":"how-to/configure_providers/#model-naming-conventions","title":"Model Naming Conventions","text":"<p>Important: Model names must match the exact format used by the provider's API. Use lowercase with hyphens as specified by the provider.</p>"},{"location":"how-to/configure_providers/#openai-models","title":"OpenAI Models","text":"<p>For OpenAI models, use the exact model names from their API documentation. Examples: - <code>gpt-4o</code> (not <code>GPT-4o</code> or <code>gpt4o</code>) - <code>gpt-5-nano-2025-08-07</code> (for specific snapshots)</p> <p>Find current model names at: https://platform.openai.com/docs/models (check the \"snapshots\" section for each model)</p>"},{"location":"how-to/configure_providers/#anthropic-models","title":"Anthropic Models","text":"<p>For Anthropic (Claude) models, use the names from the \"Claude API\" column in their documentation. Examples: - <code>claude-3-5-sonnet-20241022</code> - <code>claude-3-haiku-20240307</code></p> <p>Find current model names at: https://docs.claude.com/en/docs/about-claude/models/overview</p>"},{"location":"how-to/configure_providers/#google-models","title":"Google Models","text":"<p>For Google (Gemini) models, use the names from the \"Model Variant\" column. Examples: - <code>gemini-2.5-flash-exp</code> - <code>gemini-1.5-pro</code></p> <p>Find current model names at: https://ai.google.dev/gemini-api/docs/models</p>"},{"location":"how-to/configure_providers/#testing-your-configuration","title":"Testing Your Configuration","text":"<p>After adding a provider and models, it's recommended to: 1. Create a test bot 2. Configure it to use your new provider/model 3. Send a test message to verify everything works correctly</p>"},{"location":"how-to/deploy_to_different_channels/","title":"Deploy your bot to different platforms","text":"<p>To link a channel to your bot:</p> <p>Note</p> <p>Not all channels require a provider.</p> <ul> <li>Choose a provider for your channel.</li> <li>Configure a messaging provider. You will need to get the required credentials from your chosen provider.</li> <li>Once the provider is set up, navigate to your bot on OCS and click the plus icon in the \"Channels\" section.</li> <li>Choose your channel and complete the form. Follow the guide below to get the required information for each channel.</li> </ul>"},{"location":"how-to/deploy_to_different_channels/#web-and-api","title":"Web and API","text":"<p>The web channel uses the web interface and is enabled by default for all bots. Likewise, all bots can be accessed via the APIs.</p>"},{"location":"how-to/deploy_to_different_channels/#telegram","title":"Telegram","text":"<ul> <li>Follow this guide to create a Telegram bot.</li> <li>Copy the bot token and paste it into the form on OCS. It will look something like this: <code>4839574812:AAFD39kkdpWt3ywyRZergyOLMaJhac60qc</code>.</li> </ul> <p>Note</p> <p>Depending on your usecase, you probably want to disable group joins for your bot on Telegram. Since your telegram bot is public, anyone can add it to a group, which could end up costing you a lot. To achieve this, use the setjoingroups setting in BotFather.</p>"},{"location":"how-to/deploy_to_different_channels/#whatsapp","title":"WhatsApp","text":""},{"location":"how-to/deploy_to_different_channels/#setting-up-your-whatsapp-channel","title":"Setting Up Your WhatsApp Channel","text":"<ol> <li> <p>Add your WhatsApp number to the form in the Open Chat Studio channels section.</p> </li> <li> <p>Configure the webhook URL in your provider:</p> </li> </ol> <p>The webhook URL is always: <code>https://chatbots.dimagi.com/channels/whatsapp/incoming_message</code></p> <p>This URL is the same for all WhatsApp chatbots and channels on Open Chat Studio.</p>"},{"location":"how-to/deploy_to_different_channels/#provider-specific-configuration","title":"Provider-Specific Configuration","text":""},{"location":"how-to/deploy_to_different_channels/#for-new-whatsapp-numbers","title":"For New WhatsApp Numbers","text":"<p>If you're setting up a brand new WhatsApp number, you'll need: - Admin access to your Twilio/Turn.io account - To register the number with Meta/WhatsApp - To configure the webhook URL in your provider settings</p>"},{"location":"how-to/deploy_to_different_channels/#for-existing-whatsapp-numbers","title":"For Existing WhatsApp Numbers","text":"<p>If you're reusing an existing WhatsApp number that was previously configured for Open Chat Studio: - No additional webhook configuration needed - the number is already set up to forward messages to Open Chat Studio - Simply add the number to your bot's channels in Open Chat Studio</p>"},{"location":"how-to/deploy_to_different_channels/#provider-instructions","title":"Provider Instructions","text":"<ul> <li>For Twilio: See this page to configure the webhook URL in your messaging service</li> <li>For Turn.io: Go to Settings \u2192 API &amp; Webhooks \u2192 Add a webhook and paste the OCS webhook URL</li> </ul>"},{"location":"how-to/deploy_to_different_channels/#facebook-messenger","title":"Facebook Messenger","text":"<p>Note</p> <p>It is assumed that you already have a Facebook page and a Twilio account with the Facebook page linked. Follow this guide if this is not the case.</p> <ul> <li>Add the ID of your Facebook page.</li> <li>After you submit the form, you will be provided with a webhook URL. Copy this URL and navigate back to your provider's settings to configure it with this URL.<ul> <li>For Twilio, edit your Facebook page settings and paste the URL into the \"Webhook URL for incoming messages\" field.</li> </ul> </li> </ul>"},{"location":"how-to/deploy_to_different_channels/#slack","title":"Slack","text":"<ul> <li>Choose the channel mode.</li> <li>If you chose to link a specific channel, enter the name of the Slack channel you want your bot to be available on.</li> <li>Once the channel is linked, you will be able to chat with it using the <code>@Dimagi Bots</code> reference.</li> </ul>"},{"location":"how-to/deploy_to_different_channels/#sureadhere","title":"SureAdhere","text":"<ul> <li>Enter the Tenant ID that would have been provided to you when setting up your SureAdhere account.</li> <li>After you submit the form, you will be provided with a webhook URL. Copy this URL and navigate back to your provider's settings to configure it with this URL.</li> </ul>"},{"location":"how-to/first_chatbot/","title":"Create your first chatbot","text":"<p>In this video, I walk you through the basic steps to set up your first chatbot in Open Chat Studio. We start by creating an LLM provider, which allows us to connect with services like OpenAI or Anthropic; I chose Anthropic and entered my API key. I then created a chatbot and test it out. After testing the chatbot to ensure it functions correctly, I explain the importance of versioning to maintain user experience while making updates. I encourage you to follow along and create your own chatbot using these steps.</p>"},{"location":"how-to/global_search/","title":"Using Global Search","text":"<p>Open Chat Studio has a global search feature that allows you to find objects using their public UUIDs. This is useful when you want to quickly find an object without navigating through the UI, especially if you have copied the UUID from a trace or another source.</p> <p>To use the global search feature, navigate to <code>https://chatbots.dimagi.com/search?q=UUID</code> where <code>UUID</code> is the public UUID of the object you want to find.</p> <p>If the object is found, and you have permissions to access it, you will be redirected to the page for that object.</p>"},{"location":"how-to/global_search/#objects-that-are-currently-supported","title":"Objects that are currently supported","text":"<ul> <li>Experiments</li> <li>ExperimentSessions</li> <li>Participants</li> </ul>"},{"location":"how-to/remote_api/","title":"Connecting to a remote API","text":"<p>Open Chat Studio allows you to connect to external services via HTTP API calls. This feature is useful for extending the functionality of your bot by integrating it with other services. This feature is analogous to OpenAI's GPT Actions feature.</p> <p>To do this you will need to create an action by navigating to the \"Custom Actions\" section in Team Settings. See the Custom Actions guide for more information on creating a Custom Action.</p>"},{"location":"how-to/remote_api/#using-the-custom-action-in-your-bot","title":"Using the custom action in your bot","text":"<p>Once you have created a custom action you can add it to your bot by following these steps:</p> <ol> <li>Open your Experiment's edit page.</li> <li>Navigate to the Tools tab.</li> <li>Select the action you want to add from the Custom Actions checkbox list.</li> </ol>"},{"location":"how-to/remote_api/#testing-the-custom-action","title":"Testing the custom action","text":"<p>To test the custom action, you can open a chat with your Experiment and type a message that triggers the action. The bot will make an HTTP request to the external service and return the response to you.</p> <p>To see more detail about the request and response, you can enable tracing in the Advanced tab of your Experiment.</p>"},{"location":"how-to/setting_up_a_survey/","title":"Setting up a survey","text":"<p>This page provides an overview of how to utilize surveys in your OCS chatbot.</p>"},{"location":"how-to/setting_up_a_survey/#what-are-surveys-on-open-chat-studio","title":"What are Surveys on Open Chat Studio?","text":"<p>The Surveys feature allows chatbot makers to give users a link to a Google form (or any other link to a survey), both at the start and end of an OCS chatbot web session.\u00a0</p> <p>External Channel Survey Limitations &amp; Workarounds</p> <p>Surveys will not be automatically presented to the user before or after the chat if you deploy your chatbot on external channels like WhatsApp or Telegram. If you would still like to capture pre- or post-survey questions using these channels, you can: </p> <ul> <li>Incorporate survey questions in your prompt and structuring the prompt such that the chatbot starts and ends with questions as you would like it to.\u00a0</li> <li>Send users links to a Google form or other kind of survey directly, before or after providing them with the link to the chatbot.\u00a0</li> </ul> <p>Example of a pre-survey when using an OCS bot on the web </p> <p>Example of a post-survey when using an OCS bot on the web </p>"},{"location":"how-to/setting_up_a_survey/#create-a-survey","title":"Create a Survey","text":"<p>The very first step is to create a survey and generate a web link for that survey. For example, you might use Google forms to create pre- and post-surveys. Once this step is complete, navigate to the \"Surveys\" option on the left-hand menu on Open Chat Studio and follow the steps given below to add your survey(s) to a chatbot.\u00a0</p>"},{"location":"how-to/setting_up_a_survey/#select-add-new","title":"Select \"Add New\".","text":"<ul> <li>Name: This is a name for you or your team members on OCS to identify different surveys.</li> <li>URL:\u00a0Add the URL of your survey.\u00a0</li> <li>Confirmation text:\u00a0This is the text a user sees when they see the link to the survey, before they begin to use the chatbot. You can edit this text as you'd like.\u00a0Here, it's also important to add <code>{survey_link}</code> where you would like to show the URL to your survey.</li> </ul> <p>Example</p> <p>Before starting the experiment, we ask that you complete a short survey. Please click on the survey link, fill it out, and, when you have finished, select the checkbox to confirm you have completed it. Survey link: {survey_link}.\u00a0</p> <p>If you would like to include both a pre-survey and a post-survey, repeat the above process for each survey.</p>"},{"location":"how-to/setting_up_a_survey/#final-step","title":"Final Step","text":"<p>Now edit your chatbot and choose which survey to use as the pre- or post survey.</p>"},{"location":"how-to/setting_up_a_survey/#using-google-forms","title":"Using Google Forms","text":"<p>Go to Google Forms and create your form. In order to link a particular participant, session and experiment with a specific form, you'll need to include questions with the titles \"Participant ID\", \"Session ID\" and \"Experiment ID\".</p> <p>For example: </p> <p>From here, click on the 3 dots in the top right corner and go to \"Get Prefilled Link\". Now fill in the fields that you want prefilled. In this case, \"Participant ID\", \"Session ID\" and \"Experiment ID\".</p> <p> When you click on \"Get Link\", you'll get a link that looks something like:</p> <p>https://docs.google.com/forms/some/uri/viewform?usp=pp_url&amp;entry.1118764343=participant&amp;entry.791635770=session&amp;entry.784126073=experiment</p> <p>Replace the sections in the URL as follows:</p> <ul> <li>participant -&gt; {participant_id}</li> <li>session -&gt; {session_id}</li> <li>experiment -&gt; {experiment_id}</li> </ul> <p>This will result in a link that looks like this:</p> <p>https://docs.google.com/forms/some/uri/viewform?usp=pp_url&amp;entry.1118764343={participant_id}&amp;entry.791635770={session_id}&amp;entry.784126073={experiment_id}</p> <p>This new link should be used for your survey link.</p>"},{"location":"how-to/workflow_cookbook/","title":"Chatbot Workflow Cookbook","text":""},{"location":"how-to/workflow_cookbook/#split-bot-into-multiple-smaller-bots","title":"Split bot into multiple smaller bots","text":"<p>For complex bots it may be the case that a single LLM node with a large prompt does not perform well. For example, a bot that is expected to perform multiple different functions such as Role Play, Quiz, Q&amp;A.</p> <p>In such cases, it can be better to create smaller, narrowly focused prompts and use a router to select which 'mode' the bot is currently in.</p> <p>Here is a more complex example that uses a LLM Router to route the input to one of three linked nodes.</p> <pre><code>graph TB\n  A@{ shape: stadium, label: \"Input\" } --&gt; Router(\"`**LLM Router**\n  Route to one of the linked nodes using an LLM`\");\n  Router --&gt;|GENERAL| Gen(LLM);\n  Router --&gt;|ROLEPLAY| Rp(LLM);\n  Router --&gt;|QUIZ| Qz(LLM);\n  Gen --&gt; C@{ shape: stadium, label: \"Output\" };\n  Qz --&gt; C\n  Rp --&gt; C;</code></pre>"},{"location":"how-to/workflow_cookbook/#safety-check-in-parallel","title":"Safety check in parallel","text":"<p>In this example, we are using a Router to determine if the user input complies with the usage policy of the bot. The router has two outputs, safe and unsafe. The safe output is not connected to any other nodes but the unsafe output is connected to a Python Node which will abort the pipeline with an error message.</p> <pre><code>flowchart TD\n    start[\"start\"] --&gt; Safety[\"SafetyRouter\"] &amp; LLM\n    Safety -. safe .-&gt; Dangle:::hidden\n    Safety -. unsafe .-&gt; PythonNode[\"PythonNode\n    *abort_with_message('...')*\"]\n    LLM --&gt; __end__([\"&lt;p&gt;end&lt;/p&gt;\"])\n\n     start:::first\n     __end__:::last</code></pre> <p>If the Safety Router routes to the Python Node, the user will not see the output generated by the LLM node but will instead see a message generated by an LLM based on the message passed to the <code>abort_with_message</code> function.</p>"},{"location":"how-to/workflow_cookbook/#router-for-classification","title":"Router for classification","text":"<p>Router nodes can have unconnected outputs as seen above, enabling more flexible routing patterns where not all paths need to be explicitly handled. It is also OK to connect multiple router outputs to the same input of another node. This can be useful if you want to the router node to categorize the input but not actually affect the execution flow.</p> <pre><code>flowchart TD\n    start[\"start\"] --&gt; Router[RouterA]\n    Router -. categoryA .-&gt; PythonNode\n    Router -. categoryB .-&gt; PythonNode\n    PythonNode --&gt; LLM\n    LLM --&gt; __end__([\"&lt;p&gt;end&lt;/p&gt;\"])\n\n     start:::first\n     __end__:::last</code></pre> <p>You might use this to perform some logic in the PythonNode:</p> <pre><code>def main(input, **kwargs):\n    route = get_selected_route(\"RouterA\")\n    if route == \"categoryA\":\n        set_temp_state_key(\"question\", \"A\")\n    elif route == \"categoryB\":\n        set_temp_state_key(\"question\", \"B\")\n    return input\n</code></pre> <p>Then in the LLM node prompt you could use the temp_state to inject the category:</p> <pre><code>The current category is {temp_state.category}\n</code></pre>"},{"location":"how-to/workflow_cookbook/#reading-user-uploaded-files","title":"Reading user uploaded files","text":"<p>This workflow allows users (participants) to upload files that your chatbot can process and analyze. Supported file types are listed here.</p>"},{"location":"how-to/workflow_cookbook/#setup-steps","title":"Setup Steps","text":"<ol> <li>Enable file uploads: In your chatbot settings, enable the \"File uploads enabled\" option</li> <li>Create a Python Node: Use a Python node to read and process the uploaded file contents from the temporary state - specifically from the attachments key.</li> <li>Pass to LLM: Either return the user input along with the file contents directly to the LLM node, or save the file contents to the temporary state and inject them into your LLM prompt</li> </ol>"},{"location":"how-to/workflow_cookbook/#workflow-structure","title":"Workflow Structure","text":"<pre><code>flowchart TD\n    start[\"start\"] --&gt; PythonNode\n    PythonNode --&gt; LLM\n    LLM --&gt; __end__([\"&lt;p&gt;end&lt;/p&gt;\"])\n\n     start:::first\n     __end__:::last</code></pre> <p>Python Node Implementation:</p> <p>Option 1: Single File Processing Process only the first uploaded file:</p> <pre><code>def main(input: str, **kwargs) -&gt; str: \n    # Get uploaded files from temp state\n    attachments = get_temp_state_key(\"attachments\")\n    if not attachments:\n        return input\n\n    # Read the first file's content\n    file_content = attachments[0].read_text()\n    set_temp_state_key(\"file_contents\", file_content)\n\n    return input\n</code></pre> <p>Option 2: Multiple Files Processing Process all uploaded files:</p> <pre><code>def main(input: str, **kwargs) -&gt; str: \n    # Get uploaded files from temp state\n    attachments = get_temp_state_key(\"attachments\")\n    if not attachments:\n        return input\n\n    # Read all files and combine their contents\n    all_file_contents = []\n    for i, attachment in enumerate(attachments):\n        file_content = attachment.read_text()\n        filename = attachment.name if hasattr(attachment, 'name') else f\"File {i+1}\"\n        all_file_contents.append(f\"## {filename}\\n{file_content}\")\n\n    # Save combined contents to temp state\n    combined_contents = \"\\n\\n\".join(all_file_contents)\n    set_temp_state_key(\"file_contents\", combined_contents)\n\n    return input\n</code></pre> <p>In these examples, the Python node reads the uploaded file(s) and saves their contents to the temp state under the key \"file_contents\". The user's original input is passed through unchanged to the LLM node. </p> <p>LLM Node Configuration:</p> <p>Configure your LLM node to utilize the uploaded file contents by injecting them into the prompt using temp state variables.</p> <p>Basic Prompt Template: <pre><code>You are a helpful assistant. Answer the user's query as best you can.\n\nHere are some file contents that you should consider when generating your answer:\n\n## File Contents\n{temp_state.file_contents}\n\nUser Query: {input}\n\nInstructions:\n- If the file contents are empty or not provided, inform the user that no files were uploaded\n- Base your response on both the file contents and the user's query\n- Be specific about what you found in the uploaded files\n- If you cannot find relevant information in the files, clearly state this\n</code></pre></p>"}]}