{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Open Chat Studio","text":"<p>This is the home page for all documentation related to Open Chat Studio. Developed by Dimagi, Open Chat Studio is an easy-to-use, open source platform for rapidly prototyping, testing and deploying chatbots created using Large Language Models (LLMs).</p>"},{"location":"#what-can-i-do-on-open-chat-studio","title":"What can I do on Open Chat Studio?","text":"<ul> <li> <p>Make Your Own Chatbots: With Open Chat Studio (OCS), you can easily create your own chatbots using advanced   language technology. OCS is built for use by program staff and other teams - you don't need to be an engineer to get   started.</p> </li> <li> <p>Deploy: Use Open Chat Studio to launch your chatbots on the web and mobile apps such as Telegram and WhatsApp,   with more   options coming soon.</p> </li> <li> <p>Enable Access for All: Anyone you share a chatbot with will be able to access it, either through a web link or   directly   on platforms such as WhatsApp or Telegram. Chatbot users do not need to have an account on Open Chat Studio to use   your   chatbots.</p> </li> <li> <p>View and Download Data: View and export the data from interactions with your chatbots, formatted In CSV.</p> </li> </ul> <ul> <li> Quickstart Guide</li> <li> What is a Chatbot?</li> <li> Configuring your Team</li> <li> What is an LLM?</li> <li> Deploying your bot</li> </ul>"},{"location":"#how-do-i-use-open-chat-studio","title":"How do I use Open Chat Studio?","text":"<p>You can host your own instance of Open Chat Studio, or use the hosted version at openchatstudio.com.</p> <p>If you would like an account on Dimagi's hosted version of Open Chat Studio send an email to ocs-info@dimagi.com. </p>"},{"location":"about/","title":"About Open Chat Studio","text":"<p>Dimagi is developing Open Chat Studio (OCS) as an easy-to-use, open source platform for rapidly prototyping and testing chatbots created using Large Language Models (LLMs). Open Chat Studio makes it easy to develop and test LLM-based chatbots, and to instill a variety of guardrails to improve the safety and accuracy of these bots.</p> <p>Open Chat Studio can work with any LLM with an API such as the OpenAI Chat Completions API.</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#feb-11-2026","title":"Feb 11, 2026","text":"<ul> <li>NEW Python nodes can now attach files fetched via HTTP to AI response messages using the new <code>attach_file_from_response()</code> helper and <code>response_bytes</code> field on HTTP responses.</li> </ul>"},{"location":"changelog/#feb-10-2026","title":"Feb 10, 2026","text":"<ul> <li>CHANGE Authentication provider names in Python node HTTP requests are now case-insensitive, allowing <code>auth=\"My-Provider\"</code> and <code>auth=\"my-provider\"</code> to match the same provider.</li> </ul>"},{"location":"changelog/#feb-9-2026","title":"Feb 9, 2026","text":"<ul> <li>NEW Added <code>http_client</code> global to Python sandbox for making HTTP requests with security guardrails including SSRF prevention, request/response size limits, timeout clamping, automatic retries, and authentication provider integration.</li> </ul>"},{"location":"changelog/#feb-6-2026","title":"Feb 6, 2026","text":"<ul> <li>NEW Added support for Claude Opus 4.6 model with adaptive thinking control. Features configurable effort levels (low, medium, high, max), 200K context window, and 128K max output tokens.</li> <li>CHANGE LLM API calls now automatically retry with exponential backoff when rate limited by providers (OpenAI, Anthropic, Google), improving reliability during peak usage.</li> </ul>"},{"location":"changelog/#feb-3-2026","title":"Feb 3, 2026","text":"<ul> <li>BUG Fixed character encoding issues when reading plaintext files by automatically detecting and converting different encoding schemes to unicode.</li> </ul>"},{"location":"changelog/#feb-2-2026","title":"Feb 2, 2026","text":"<ul> <li>NEW Voice notes from users and bots are now displayed as attachments in the chat transcript, making it easier to review and access voice messages.</li> </ul>"},{"location":"changelog/#jan-31-2026","title":"Jan 31, 2026","text":"<ul> <li>BUG Fixed an issue where local collection index validation in LLM nodes incorrectly required all collections to use the same LLM provider as the node. This restriction now only applies to remote collections.</li> </ul>"},{"location":"changelog/#jan-30-2026","title":"Jan 30, 2026","text":"<ul> <li>CHANGE Indexed collections using OpenAI-hosted vectorstores are now limited to 2 remote collections per LLM node, enforcing OpenAI's vectorstore limit. Local indexes and non-OpenAI providers remain unaffected.</li> </ul>"},{"location":"changelog/#jan-27-2026","title":"Jan 27, 2026","text":"<ul> <li>CHANGE Router keywords are now automatically converted to uppercase. All router configurations will only accept and match uppercase keywords.</li> <li>NEW Dataset messages table rows can now be highlighted and shared via URL. Each row has a link and copy button to easily share specific dataset messages with others, with automatic scrolling to the highlighted message.</li> </ul>"},{"location":"changelog/#jan-26-2026","title":"Jan 26, 2026","text":"<ul> <li>NEW Custom actions now include health status monitoring. The system automatically checks custom action endpoints every 5 minutes to verify server availability, displaying the status in the custom actions table. Users can also manually trigger health checks.</li> </ul>"},{"location":"changelog/#jan-22-2026","title":"Jan 22, 2026","text":"<ul> <li>NEW Added API support for passing arbitrary context data with messages that gets merged into session state under the <code>remote_context</code> key, enabling API clients to provide contextual information.</li> <li>BUG Fixed an issue where cited and generated files from OpenAI assistants were not being properly annotated for download.</li> <li>BUG Fixed an issue where built-in tools and tool configurations were not cleared when switching LLM providers.</li> </ul>"},{"location":"changelog/#jan-21-2026","title":"Jan 21, 2026","text":"<ul> <li>CHANGE Router keywords are now automatically converted to lowercase. All router configurations will only accept and match lowercase keywords.</li> <li>MIGRATION Removed the defunct 'summarize' event action. All events using this action have been deleted, and team admins have been notified of affected chatbots.</li> </ul>"},{"location":"changelog/#jan-20-2026","title":"Jan 20, 2026","text":"<ul> <li>NEW Evaluation results table rows can now be highlighted and shared via URL. Each row has a link and copy button to easily share specific evaluation results with others.</li> <li>BUG Fixed an issue where provider compatibility checks between LLM nodes and indexed collections were skipped when only one collection was used.</li> <li>NEW Added more granular conversation end event types. Users can now create events based on who ended the conversation (participant, bot, event, admin or API). The generic conversation end trigger remains as a catch-all that fires whenever any conversation ends.</li> </ul>"},{"location":"changelog/#jan-14-2026","title":"Jan 14, 2026","text":"<ul> <li>NEW Added the ability to start a new session after ending the current one. Users can choose whether to trigger end conversation events and must provide a prompt for the bot's initial message (pre-filled with the seed message when available).</li> </ul>"},{"location":"changelog/#jan-9-2026","title":"Jan 9, 2026","text":"<ul> <li>NEW Added REST API endpoint for managing session tags. Sessions can now be tagged via POST requests (adds tags) and DELETE requests (removes tags), enabling external integrations to organize and filter sessions programmatically.</li> </ul>"},{"location":"changelog/#dec-19-2025","title":"Dec 19, 2025","text":"<ul> <li>NEW Collections now support bulk file downloads. When a collection contains multiple files, users can download all files as a zip archive with progress tracking. Downloaded archives expire after 24 hours.</li> </ul>"},{"location":"changelog/#dec-18-2025","title":"Dec 18, 2025","text":"<ul> <li>NEW Users can now trigger the bot to send a message to a participant from the participant details page.</li> </ul>"},{"location":"changelog/#dec-16-2025","title":"Dec 16, 2025","text":"<ul> <li>NEW Added the ability for users to manually end sessions.</li> </ul>"},{"location":"changelog/#dec-15-2025","title":"Dec 15, 2025","text":"<ul> <li>NEW LLM Evaluators now support type validation for output schemas with integer, float, string, and enum (choices) types.</li> <li>NEW Added support for GPT-5.2 and GPT-5.2-pro models.</li> <li>CHANGE Sessions generated during evaluation runs are now retained for 30 days (increased from 7 days) before being permanently deleted.</li> <li>BUG Fixed an issue where temperature and top_p parameters were shown in GPT-5.2 model configurations when effort level was set, causing configuration conflicts.</li> </ul>"},{"location":"changelog/#dec-10-2025","title":"Dec 10, 2025","text":"<ul> <li>BUG Fixed an issue where additional citation links were included in channel responses when using custom citation text.</li> </ul>"},{"location":"changelog/#nov-26-2025","title":"Nov 26, 2025","text":"<ul> <li>NEW OAuth2 authentication is now supported for API access. This enables secure third-party integrations using industry-standard OAuth2 with PKCE. See the OAuth2 integration guide for implementation details.</li> </ul>"},{"location":"changelog/#nov-11-2025","title":"Nov 11, 2025","text":"<ul> <li>NEW Users can now configure model parameters (temperature, max tokens, etc.) directly in LLM nodes alongside other node parameters, instead of requiring separate configuration.</li> </ul>"},{"location":"changelog/#nov-7-2025","title":"Nov 7, 2025","text":"<ul> <li>NEW Added \"Select all\" option to sessions table for bulk selection of sessions.</li> <li>CHANGE Improved client key security for chat widget. See the Chat Widget docs.</li> </ul>"},{"location":"changelog/#nov-3-2025","title":"Nov 3, 2025","text":"<ul> <li>NEW Chat Widget releases v0.5. Key features include:<ul> <li>Users can drag and reposition the chat widget launch button when fixed, to avoid obscuring page content</li> <li>Internationalization support with built-in translations for 9 languages (English, Spanish, French, Arabic, Hindi, Italian, Portuguese, Swahili, Ukrainian)</li> <li>New <code>language</code> property to set widget UI language and <code>translations-url</code> property for custom translations</li> <li>Updated default button logo to use the Open Chat Studio avatar</li> <li>See the widget changelog for full details</li> </ul> </li> <li>CHANGE Removed seed message processing from the chat API session creation endpoint. The <code>seed_message_task_id</code> field is no longer returned in API responses.</li> <li>CHANGE Improved version creation UI performance by truncating large change sets to 10 items and displaying the count of hidden changes.</li> </ul> <p>You can find older entries in the GitHub release notes: https://github.com/dimagi/open-chat-studio-docs/releases</p>"},{"location":"domain_migration/","title":"Domain Migration","text":"<p>We will be migrating from the current chatbots.dimagi.com domain to www.openchatstudio.com.</p> <p>We will be configuring automatic redirects for most traffic, however, some features will require you to make changes to external systems. Please follow the migration guides below.</p>"},{"location":"domain_migration/#timeline","title":"Timeline","text":"<p>The timeline is not fully set yet, however, the new domain is live, and we recommend users start using the new domain.</p>"},{"location":"domain_migration/#migration-guides","title":"Migration Guides","text":""},{"location":"domain_migration/#messaging-providers","title":"Messaging Providers","text":"<p>We are still investigating our options for migrating messaging providers. Where possible, we will perform automated migrations. If manual steps are required, we will provide clear instructions and sufficient advanced notice.</p>"},{"location":"domain_migration/#apis","title":"APIs","text":"<p>Users of the APIs will be required to update their API clients to use the new domain name.</p>"},{"location":"domain_migration/#embedded-chat-widget","title":"Embedded Chat Widget","text":"<p>The recommended approach is to upgrade the chat widget to a version <code>&gt;= v0.4.9</code>.</p> <p>If you are using a version <code>&lt; 0.4.0</code>, you should consider upgrading anyway. We will be phasing out the endpoints used by that version of the widget in the future.</p>"},{"location":"api/","title":"API","text":"<p>Open Chat Studio provides a REST API that enables you to create chat sessions, send messages, manage experiments, and access session data programmatically.</p>"},{"location":"api/#api-schema-and-docs","title":"API Schema and Docs","text":"<p>See the following links for documentation on the API endpoints:</p> <ul> <li>API docs</li> <li>OpenAPI Schema</li> </ul>"},{"location":"api/#overview","title":"Overview","text":"<p>The API is organized around REST principles and uses standard HTTP methods and status codes. All API endpoints return JSON responses and require authentication.</p> <p>Base URL: <code>https://openchatstudio.com/api/</code></p>"},{"location":"api/#authentication","title":"Authentication","text":"<p>The API supports multiple authentication methods:</p> <ul> <li>API Key Authentication: Include your API key in the <code>X-api-key</code> header</li> <li>Token Authentication: Use Bearer token authentication in the <code>Authorization</code> header  </li> <li>Cookie Authentication: Session-based authentication using cookies (for web integrations)</li> <li>OAuth2 Authentication: OAuth2 authentication using scopes. See the getting started page on how to get started.</li> </ul>"},{"location":"api/#error-handling","title":"Error Handling","text":"<p>The API uses standard HTTP status codes:</p> <ul> <li><code>200 OK</code>: Request successful</li> <li><code>201 Created</code>: Resource created successfully  </li> <li><code>202 Accepted</code>: Request accepted for processing</li> <li><code>400 Bad Request</code>: Invalid request data</li> <li><code>401 Unauthorized</code>: Authentication required</li> <li><code>404 Not Found</code>: Resource not found</li> <li><code>500 Internal Server Error</code>: Server error</li> </ul> <p>Error responses include a JSON object with error details:</p> <pre><code>{\n  \"detail\": \"Error description\"\n}\n</code></pre>"},{"location":"api/#rate-limiting","title":"Rate Limiting","text":"<p>To ensure optimal performance: - Chat polling should not exceed once every 30 seconds - API requests are subject to reasonable rate limits - Use pagination for large data sets</p>"},{"location":"api/#llm-docs","title":"LLM Docs","text":"<p>The following documents are a simplified version of the API for consumption by LLMs:</p> <ul> <li>Channels</li> <li>Chat</li> <li>Sessions</li> <li>Experiments</li> <li>Files</li> <li>OpenAI</li> <li>Participants</li> </ul>"},{"location":"api/getting_started_with_oauth/","title":"Getting started with OAuth2","text":"<p>OpenChatStudio uses OAuth2 with the Authorization Code Flow with PKCE (Proof Key for Code Exchange) to enable secure third-party integrations.</p>"},{"location":"api/getting_started_with_oauth/#how-oauth-20-works","title":"How OAuth 2.0 works","text":"<p>For a detailed explanation of the OAuth 2.0 authorization code flow, see OAuth 2 Simplified. OpenChatStudio follows this standard flow, with specific endpoints documented below.</p>"},{"location":"api/getting_started_with_oauth/#openchatstudio-oauth-endpoints","title":"OpenChatStudio OAuth Endpoints","text":"Endpoint URL Notes Authorization <code>https://www.openchatstudio.com/o/authorize/</code> Token <code>https://www.openchatstudio.com/o/token/</code> UserInfo <code>https://www.openchatstudio.com/o/userinfo/</code> Requires <code>openid</code> scope"},{"location":"api/getting_started_with_oauth/#step-1-register-your-application-with-openchatstudio","title":"Step 1: Register your application with OpenChatStudio","text":"<p>Register your application with Open Chat Studio.</p> <p>You'll receive:</p> <ul> <li>Client ID: A public identifier for your application</li> <li>Client secret: A confidential secret used for token exchange (keep this secure and server-side only!)</li> </ul>"},{"location":"api/getting_started_with_oauth/#step-2-initiate-the-authorization-call","title":"Step 2: Initiate the authorization call","text":"<p>Your application should redirect the user to OpenChatStudio's authorization endpoint to request permission.</p>"},{"location":"api/getting_started_with_oauth/#pkce-setup-required","title":"PKCE Setup (Required)","text":"<p>For security, you must implement PKCE:</p> <ol> <li>Generate a random <code>code_verifier</code> (43-128 characters, unreserved characters)</li> <li>Create a <code>code_challenge</code> by SHA256 hashing the verifier and base64url encoding it</li> <li>Include the <code>code_challenge</code> in your authorization request</li> </ol> <p>Here's example Python code to generate PKCE parameters:</p> <pre><code>import secrets\nimport string\nimport base64\nimport hashlib\n\n# Generate a random code verifier (43-128 characters)\ncharacters = string.ascii_letters + string.digits + '-._~'\ncode_verifier = ''.join(secrets.choice(characters) for _ in range(128))\n\n# Create the code challenge by hashing and encoding the verifier\ncode_challenge = base64.urlsafe_b64encode(\n    hashlib.sha256(code_verifier.encode()).digest()\n).decode().rstrip('=')\n</code></pre>"},{"location":"api/getting_started_with_oauth/#query-parameters","title":"Query Parameters","text":"Parameter Required Description <code>response_type</code> Yes Must be <code>code</code> <code>client_id</code> Yes Your client ID <code>redirect_uri</code> Yes The URL where you want to receive the authorization code. Must match a registered URI for your application <code>code_challenge</code> Yes The PKCE code challenge (base64url-encoded SHA256 hash of your code_verifier) <code>code_challenge_method</code> Yes Must be <code>S256</code> (SHA256) <code>state</code> Recommended Random string to prevent CSRF attacks. Store this and validate the response <code>scope</code> No Space-separated list of scopes. See available scopes in the API docs. If omitted, defaults to all scopes <code>team</code> No Specific team to scope the token to"},{"location":"api/getting_started_with_oauth/#example-request","title":"Example Request","text":"<pre><code>https://www.openchatstudio.com/o/authorize/?response_type=code&amp;client_id=${CLIENT_ID}&amp;redirect_uri=https://your-server/callback/&amp;code_challenge=${CHALLENGE}&amp;code_challenge_method=S256&amp;scope=chatbot:read+session:read&amp;state=random_state_string\n</code></pre>"},{"location":"api/getting_started_with_oauth/#step-3-handle-the-authorization-response","title":"Step 3: Handle the authorization response","text":"<p>After the user grants permission, OpenChatStudio redirects them to your <code>redirect_uri</code> with the authorization code in the query string:</p> <pre><code>https://your-server/callback/?code=auth_code_here&amp;state=random_state_string\n</code></pre> <p>Important validations:</p> <ol> <li>Verify the <code>state</code> parameter matches the one you sent in Step 1 (protects against CSRF attacks)</li> <li>Extract the <code>code</code> parameter</li> <li>Handle errors if present (user denied, invalid client, etc.)</li> </ol>"},{"location":"api/getting_started_with_oauth/#error-responses","title":"Error Responses","text":"<p>If an error occurs, the redirect will include error parameters:</p> <pre><code>https://your-server/callback/?error=access_denied&amp;error_description=The+user+denied+the+request&amp;state=random_state_string\n</code></pre> <p>Common error codes:</p> <ul> <li><code>access_denied</code>: User rejected the authorization request</li> <li><code>invalid_request</code>: Missing or invalid parameters</li> <li><code>unauthorized_client</code>: Client not authorized to use this flow</li> <li><code>server_error</code>: Authorization server encountered an error</li> </ul>"},{"location":"api/getting_started_with_oauth/#step-4-exchange-the-authorization-code-for-an-access-token","title":"Step 4: Exchange the authorization code for an access token","text":"<p>Your server must send a POST request to OpenChatStudio's token endpoint.</p>"},{"location":"api/getting_started_with_oauth/#required-post-parameters","title":"Required POST Parameters","text":"Parameter Description <code>grant_type</code> Must be <code>authorization_code</code> <code>code</code> The authorization code <code>client_id</code> Your client ID <code>client_secret</code> Your client secret (keep this server-side!) <code>code_verifier</code> The PKCE code verifier you generated in Step 1 <code>redirect_uri</code> Must match the redirect_uri used in Step 1"},{"location":"api/getting_started_with_oauth/#example-request_1","title":"Example Request","text":"<pre><code>curl -X POST https://www.openchatstudio.com/o/token/ \\\n  -H \"Content-Type: application/x-www-form-urlencoded\" \\\n  -d \"grant_type=authorization_code\" \\\n  -d \"code=auth_code_here\" \\\n  -d \"client_id=${CLIENT_ID}\" \\\n  -d \"client_secret=${CLIENT_SECRET}\" \\\n  -d \"code_verifier=${PKCE_CODE}\" \\\n  -d \"redirect_uri=https://your-server/callback/\"\n</code></pre>"},{"location":"api/getting_started_with_oauth/#response","title":"Response","text":"<p>A successful response returns a JSON object with the access token:</p> <pre><code>{\n  \"access_token\": \"eyJ0eXAiOiJKV1QiLCJhbGc...\",\n  \"token_type\": \"Bearer\",\n  \"expires_in\": 3600,\n  \"scope\": \"chatbot:read session:read\",\n  \"refresh_token\": \"1p1mG5sD2k4PCdILM9qLYB...\"\n}\n</code></pre> <p>Key fields:</p> <ul> <li><code>access_token</code>: Use this in the <code>Authorization: Bearer</code> header for API requests</li> <li><code>token_type</code>: Always <code>Bearer</code> for this flow</li> <li><code>expires_in</code>: Seconds until token expiration</li> <li><code>scope</code>: The actual scopes granted</li> <li><code>refresh_token</code>: Use this to get a new access token when the current one expires (see Step 6)</li> </ul>"},{"location":"api/getting_started_with_oauth/#step-5-use-the-access-token","title":"Step 5: Use the access token","text":"<p>Include the access token in the Authorization header when making API requests:</p> <pre><code>curl -H \"Authorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGc...\" \\\n  https://www.openchatstudio.com/api/...\n</code></pre>"},{"location":"api/getting_started_with_oauth/#step-6-get-a-new-access-token-using-the-refresh-token","title":"Step 6: Get a new access token using the refresh token","text":"<p>When your access token expires, use the refresh token to get a new one without requiring the user to re-authenticate. Send a POST request to the token endpoint with the refresh token grant type.</p> <p>Token endpoint: <code>https://www.openchatstudio.com/o/token/</code></p>"},{"location":"api/getting_started_with_oauth/#required-post-parameters_1","title":"Required POST Parameters","text":"Parameter Description <code>grant_type</code> Must be <code>refresh_token</code> <code>refresh_token</code> The refresh token received in Step 4 <code>client_id</code> Your client ID <code>client_secret</code> Your client secret"},{"location":"api/getting_started_with_oauth/#example-request_2","title":"Example Request","text":"<pre><code>curl -X POST https://www.openchatstudio.com/o/token/ \\\n  -H \"Content-Type: application/x-www-form-urlencoded\" \\\n  -d \"grant_type=refresh_token\" \\\n  -d \"refresh_token=${REFRESH_TOKEN}\" \\\n  -d \"client_id=${CLIENT_ID}\" \\\n  -d \"client_secret={$CLIENT_SECRET}\"\n</code></pre>"},{"location":"api/getting_started_with_oauth/#response_1","title":"Response","text":"<p>A successful response returns a new access token:</p> <pre><code>{\n  \"access_token\": \"eyJ0eXAiOiJKV1QiLCJhbGc...\",\n  \"token_type\": \"Bearer\",\n  \"expires_in\": 3600,\n  \"scope\": \"chatbot:read session:read\",\n  \"refresh_token\": \"new_refresh_token_here...\"\n}\n</code></pre> <p>Important: Save the new <code>refresh_token</code> returned in the response, as it replaces your previous refresh token. Use this new token for future refresh requests.</p>"},{"location":"api/getting_started_with_oauth/#openid-connect-oidc","title":"OpenID Connect (OIDC)","text":"<p>OpenChatStudio supports OpenID Connect, which extends OAuth 2.0 to provide identity information about authenticated users. This is particularly useful for applications that need to identify which team the authenticated user has access to.</p>"},{"location":"api/getting_started_with_oauth/#the-openid-scope","title":"The <code>openid</code> Scope","text":"<p>To use OpenID Connect features, include the <code>openid</code> scope in your authorization request (Step 2). You can combine it with other scopes as needed:</p> <pre><code>https://www.openchatstudio.com/o/authorize/?response_type=code&amp;client_id=${CLIENT_ID}&amp;redirect_uri=https://your-server/callback/&amp;code_challenge=${CHALLENGE}&amp;code_challenge_method=S256&amp;scope=openid+chatbot:read+session:read&amp;state=random_state_string\n</code></pre>"},{"location":"api/getting_started_with_oauth/#id-token-in-token-response","title":"ID Token in Token Response","text":"<p>When you request the <code>openid</code> scope, the token endpoint response (Step 4) will include an additional <code>id_token</code> field:</p> <pre><code>{\n  \"access_token\": \"eyJ0eXAiOiJKV1QiLCJhbGc...\",\n  \"token_type\": \"Bearer\",\n  \"expires_in\": 3600,\n  \"scope\": \"openid chatbot:read session:read\",\n  \"refresh_token\": \"1p1mG5sD2k4PCdILM9qLYB...\",\n  \"id_token\": \"eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9...\"\n}\n</code></pre> <p>The <code>id_token</code> is a JSON Web Token (JWT) that contains identity claims about the authenticated user, including:</p> <ul> <li><code>sub</code>: The user's email address</li> <li><code>name</code>: The user's full name</li> <li><code>is_active</code>: Whether the user account is active</li> <li><code>team</code>: The team slug for the team associated with this token</li> </ul> <p>You can decode this JWT to extract user identity information without making additional API calls.</p> <p>Security Note: When using the <code>id_token</code>, always verify its signature using a JWT library before trusting its contents. This ensures the token hasn't been tampered with and was actually issued by OpenChatStudio. Most JWT libraries can handle signature verification automatically using OpenChatStudio's public keys from the OIDC discovery endpoint.</p>"},{"location":"api/getting_started_with_oauth/#userinfo-endpoint","title":"UserInfo Endpoint","text":"<p>Alternatively, you can retrieve user information by calling the UserInfo endpoint with your access token:</p> <p>Endpoint: <code>https://www.openchatstudio.com/o/userinfo/</code></p> <p>Method: GET or POST</p> <p>Authentication: Include the access token in the Authorization header</p> <p>Note: The access token must have been issued with the <code>openid</code> scope to access this endpoint.</p> <pre><code>curl -H \"Authorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGc...\" \\\n  https://www.openchatstudio.com/o/userinfo/\n</code></pre> <p>Response:</p> <pre><code>{\n  \"sub\": \"user@example.com\",\n  \"name\": \"John Doe\",\n  \"is_active\": true,\n  \"team\": \"team-slug\"\n}\n</code></pre> <p>The UserInfo endpoint returns the same claims as the <code>id_token</code>, providing a standard way to retrieve user identity information when needed.</p>"},{"location":"chat_widget/","title":"Open Chat Studio Widget","text":"<p>The Open Chat Studio Widget is a customizable chat component that allows you to easily embed conversational AI bots into any website. Create engaging user experiences with minimal setup and extensive customization options.</p>"},{"location":"chat_widget/#features","title":"Features","text":"<ul> <li>Easy Integration: Add to any website with just a few lines of code</li> <li>Flexible Embedding: Choose between widget component or iframe methods</li> <li>Custom Styling: Match your brand with CSS variables and custom themes</li> <li>Welcome Messages: Greet users with personalized messages</li> <li>Starter Questions: Guide users with pre-defined clickable questions</li> <li>File Uploads: Allow users to attach files to their messages</li> <li>Responsive Design: Works seamlessly across desktop and mobile devices</li> </ul> <ul> <li> View the reference docs</li> <li> Changelog and upgrade info</li> </ul>"},{"location":"chat_widget/#getting-started","title":"Getting Started","text":"<p>Before embedding, you must create a bot in Open Chat Studio.</p> <ol> <li> <p>Add the widget script to your site's <code>&lt;head&gt;</code> section:</p> <pre><code>&lt;script type='module' src='https://unpkg.com/open-chat-studio-widget@0.6.0/dist/open-chat-studio-widget/open-chat-studio-widget.esm.js' async&gt;&lt;/script&gt;\n</code></pre> </li> <li> <p>Getting the Embed Code</p> <ol> <li>Log in to Open Chat Studio.</li> <li>Navigate to the Chatbot you wish to embed.</li> <li>Click on the  (plus) icon and select Embedded Widget from the dropdown.</li> <li>Complete the form (provide a name and any configuration options) and click Create.</li> </ol> <p>A new dialog will open with all the necessary details to embed your bot. You can either copy the chatbot ID and embed token for manual setup or use the Copy Widget Embed Code to copy a fully configured HTML snippet.</p> </li> <li> <p>Insert the widget where you want the chat button.</p> <p>The embed code snippet should look something like this:</p> <pre><code>&lt;open-chat-studio-widget\n  visible=\"false\"\n  chatbot-id=\"{CHATBOT_ID}\"\n  embed-key=\"{EMBED_KEY}\"\n  button-text=\"Let's Chat\"\n  position=\"right\"\n  expanded=\"false\"&gt;\n&lt;/open-chat-studio-widget&gt;\n</code></pre> </li> </ol> <p>Embed Key Required</p> <p>The <code>embed-key</code> parameter is required for all embedded widget channels. If you are upgrading from a previous version, you must create a new Embedded Widget channel (following the steps above) to obtain your embed key.</p>"},{"location":"chat_widget/#test-the-chatbot","title":"Test the Chatbot","text":"<ol> <li>Open your website in a web browser.</li> <li>Ensure the chatbot appears and functions as expected.</li> <li>Try sending a message to confirm it responds correctly.</li> </ol>"},{"location":"chat_widget/#troubleshooting","title":"Troubleshooting","text":"<p>If the chatbot does not appear:</p> <ul> <li>Ensure you copied and pasted the embed code correctly.</li> <li>Clear your browser cache and refresh the page.</li> <li>Check that your website allows embedding external scripts.</li> </ul>"},{"location":"chat_widget/changelog/","title":"Changelog","text":""},{"location":"chat_widget/changelog/#general-upgrade-guide","title":"General upgrade guide","text":"<p>This guide will help you upgrade from previous versions of the Open Chat Studio Widget to the latest version. Please follow these steps carefully to ensure a smooth transition and also review any changes and upgrade steps between your current version and the latest version.</p>"},{"location":"chat_widget/changelog/#quick-upgrade-steps","title":"Quick Upgrade Steps","text":""},{"location":"chat_widget/changelog/#1-update-the-script-tags","title":"1. Update the script tags","text":"<p>Update your script tags to use the latest version: <pre><code>&lt;script \n  src=\"https://unpkg.com/open-chat-studio-widget{LATEST_VERSION_NUMBER}/dist/open-chat-studio-widget/open-chat-studio-widget.js\"\n  type=\"module\"\n  async\n  &gt;&lt;/script&gt;\n</code></pre></p>"},{"location":"chat_widget/changelog/#2-review-your-implementation","title":"2. Review Your Implementation","text":"<p>Check your current HTML implementation and compare it with the latest properties reference.</p>"},{"location":"chat_widget/changelog/#changelog_1","title":"Changelog","text":""},{"location":"chat_widget/changelog/#v060-2026-01-27","title":"v0.6.0 (2026-01-27)","text":"<ul> <li> <p>NEW: Page Context Support - Pass page-specific context to the bot with each message</p> <ul> <li>New <code>pageContext</code> property to send dynamic context data (e.g., current user role, page location)</li> <li>Must be set using JavaScript as a plain object (not an HTML attribute)</li> <li>Context is automatically included with every message sent by the user</li> <li>Useful for personalizing bot responses based on the current page state</li> </ul> </li> <li> <p>Enhanced File Upload Support - Now accepts all text-based file types  </p> <ul> <li>Added support for all text file types via <code>text/*</code> MIME type</li> <li>Users can now upload any text-based files, not just predefined formats</li> <li>Includes .txt, .csv, and all other text/* types</li> </ul> </li> </ul>"},{"location":"chat_widget/changelog/#v052-2025-11-03","title":"v0.5.2 (2025-11-03)","text":"<ul> <li>Update default button logo to use the Open Chat Studio avatar</li> <li>Fix welcome messages and starter questions not being displayed when provided in translation files</li> <li>Improvements to session handling</li> </ul>"},{"location":"chat_widget/changelog/#v051","title":"v0.5.1","text":"<ul> <li>Change language codes for Italian and Portuguese to use standard codes: <code>ita</code> -&gt; <code>it</code>, <code>por</code> -&gt; <code>pt</code></li> </ul>"},{"location":"chat_widget/changelog/#v050","title":"v0.5.0","text":"<ul> <li>Allow users to drag and reposition the chat-widget launch button when it\u2019s fixed, to avoid obscuring page content.</li> <li>NEW: Internationalization Support<ul> <li>Added built-in translations for 9 languages: English, Spanish, French, Arabic, Hindi, Italian, Portuguese, Swahili, and Ukrainian</li> <li>New <code>language</code> property to set widget UI language (e.g., <code>language=\"es\"</code>)</li> <li>New <code>translations-url</code> property to load custom translations from a JSON file</li> <li>All UI strings can now be customized through translations</li> <li>Content properties (<code>button-text</code>, <code>header-text</code>, <code>welcome-messages</code>, <code>starter-questions</code>, <code>typing-indicator-text</code>) can be overridden in translation files</li> </ul> </li> </ul>"},{"location":"chat_widget/changelog/#deprecation-warnings","title":"Deprecation Warnings","text":"<ul> <li>The following HTML text attributes are now deprecated and will be removed in a future major release. You should migrate these to use the new translation system:<ul> <li><code>header-text</code> -&gt; <code>branding.headerText</code> translation key</li> <li><code>typing-indicator-text</code> -&gt; <code>status.typing</code> translation key</li> <li><code>new-chat-confirmation-message</code> -&gt; <code>modal.newChatBody</code> translation key</li> </ul> </li> </ul>"},{"location":"chat_widget/changelog/#upgrade-guide","title":"Upgrade Guide","text":"<ol> <li>No immediate action required - existing implementations continue to work unchanged</li> <li>To enable internationalization:<ul> <li>Add <code>language=\"xx\"</code> attribute for built-in language support</li> <li>Or add <code>translations-url=\"https://yoursite.com/translations.json\"</code> for custom translations</li> </ul> </li> <li>To migrate content to translations (recommended):<ul> <li>Create a custom translations JSON file with your content</li> <li>Remove corresponding deprecated HTML attributes and use the translation file instead</li> <li>See the internationalization documentation for details</li> </ul> </li> </ol>"},{"location":"chat_widget/changelog/#v048","title":"v0.4.8","text":"<ul> <li>Fix horizontal scrollbar styling.</li> <li>Improve scrolling behavior for new messages.</li> <li>Scroll to bottom when loading the window.</li> </ul>"},{"location":"chat_widget/changelog/#v047","title":"v0.4.7","text":"<ul> <li>Fix regression in font size consistency.</li> </ul>"},{"location":"chat_widget/changelog/#v046","title":"v0.4.6","text":"<ul> <li>Add support for sending messages with attachments.<ul> <li>Enabled by setting <code>allow-attachments=\"true\"</code></li> <li>See the file attachments section in the style guide for available CSS properties.</li> </ul> </li> <li>Update the 'Start a new session' icon.</li> <li>Add a confirmation dialog when starting a new chat.<ul> <li>Customize the text using the <code>new-chat-confirmation-message</code> attribute.</li> <li>See the confirmation dialog section in the style guide for available CSS properties.</li> </ul> </li> <li>Customize the typing indicator text using the <code>typing-indicator-text</code> attribute.</li> <li>Update message background and text colors.<ul> <li>See the messages section of the style guide.</li> </ul> </li> <li>Update link CSS styling.<ul> <li>New properties <code>--message-user-link-color</code>, <code>--message-assistant-link-color</code>, <code>--message-system-link-color</code>.</li> </ul> </li> <li>Removed unnecessary CSS properties for padding (<code>--*-padding*</code>).</li> <li>Added <code>--success-text-color</code> CSS property.</li> <li>Error handling improvements.</li> <li>Fix full screen mode layout.</li> </ul>"},{"location":"chat_widget/changelog/#v045","title":"v0.4.5","text":"<ul> <li>Internal API changes</li> </ul>"},{"location":"chat_widget/changelog/#v044","title":"v0.4.4","text":"<ul> <li>Merge width &amp; height vars:<ul> <li><code>--button-icon-width</code>, <code>--button-icon-height</code> -&gt; <code>--button-icon-size</code> </li> </ul> </li> <li>Fix launch button styling.<ul> <li>Correctly apply font size and borders.</li> </ul> </li> <li>Add variables to control header font and icon size:<ul> <li><code>--header-font-size</code> </li> <li><code>--header-button-icon-size</code> </li> </ul> </li> <li>Support for placing text in the window header using the <code>header-text</code>.<ul> <li>Use <code>--header-text-font-size</code> and <code>--header-text-color</code> to style it independently of the other header elements. </li> </ul> </li> </ul>"},{"location":"chat_widget/changelog/#v043","title":"v0.4.3","text":"<ul> <li>Fix markdown styling</li> <li>Allow customizing the chat window width and height using the following CSS vars:<ul> <li><code>--chat-window-width</code> </li> <li><code>--chat-window-height</code> </li> <li><code>--chat-window-fullscreen-width</code></li> <li>See the styling guide for details.</li> </ul> </li> <li>Change size units from <code>rem</code> to <code>em</code>.</li> </ul>"},{"location":"chat_widget/changelog/#v042","title":"v0.4.2","text":"<ul> <li>Fully configurable styling via CSS properties.</li> </ul>"},{"location":"chat_widget/changelog/#v041","title":"v0.4.1","text":"<ul> <li>Improved styling.</li> <li>Replaced 'expand' with 'fullscreen' mode.</li> </ul>"},{"location":"chat_widget/changelog/#attribute-changes","title":"Attribute changes","text":"<p>Added</p> <ul> <li><code>allow-full-screen</code>: Allow the user to make the chat window full screen. </li> </ul> <p>Removed</p> <ul> <li><code>expanded</code></li> </ul>"},{"location":"chat_widget/changelog/#v040","title":"v0.4.0","text":"<p>Warning</p> <p>This is a full rebuild of the widget and includes breaking changes. See the upgrade guide for details.</p> <ul> <li>Enhanced Mobile Experience<ul> <li>Improved responsive design for mobile devices</li> <li>Better touch interactions and scrolling</li> </ul> </li> <li>Draggable Chat Window<ul> <li>Desktop users can now drag the chat window to reposition it</li> </ul> </li> <li>Button Customization<ul> <li>You can now set a button icon and change the button shape</li> </ul> </li> <li>Welcome Messages &amp; Starter Questions<ul> <li>Display welcome messages when the chat opens</li> <li>Provide clickable starter questions to help users get started</li> <li>Both support rich markdown formatting</li> </ul> </li> <li>Session persistence across page loads<ul> <li>Store session data in browser local storage to allow resuming sessions across page loads. </li> </ul> </li> </ul>"},{"location":"chat_widget/changelog/#upgrading-from-03x","title":"Upgrading from 0.3.x","text":"<p>The minimal steps required to upgrade are to replace the <code>bot-url</code> attribute with the <code>chatbot-id</code> attribute:</p> <pre><code>  &lt;open-chat-studio-widget\n-     bot-url=\".../experiments/e/{CHATBOT_ID}/embed/start/\"\n+     chatbot-id=\"{CHATBOT_ID}\"\n  &lt;/open-chat-studio-widget&gt;\n</code></pre> <p>The <code>chatbot_id</code> can be extracted from the <code>bot-url</code> by copying the UUID from the URL as shown above.</p>"},{"location":"chat_widget/changelog/#upgrade-checklist","title":"Upgrade Checklist","text":"<p>\u2705 Check These Items</p> <p>Property Names: Ensure all property names use kebab-case (e.g., chatbot-id, not chatbotId) JSON Properties: Verify that welcome-messages and starter-questions are properly formatted JSON strings: <pre><code>&lt;!-- \u2705 Correct --&gt;\nwelcome-messages='[\"Message 1\", \"Message 2\"]'\n\n&lt;!-- \u274c Incorrect --&gt;\nwelcome-messages=\"Message 1, Message 2\"\n</code></pre></p> <p>API Base URL: If you were using a custom API URL, ensure the api-base-url property is set correctly</p> <p>Boolean Properties: Use string values for boolean properties: <pre><code>&lt;!-- \u2705 Correct --&gt;\nvisible=\"true\"\nexpanded=\"false\"\n\n&lt;!-- \u274c Incorrect --&gt;\nvisible={true}\nexpanded={false}\n</code></pre></p>"},{"location":"chat_widget/reference/","title":"Reference Docs","text":"<p>Learn how to customize the Open Chat Studio widget to match your brand and improve user experience.</p>"},{"location":"chat_widget/reference/#button-customization","title":"Button Customization","text":"<p>The widget button can be customized using the following properties:</p> <pre><code>&lt;open-chat-studio-widget\n  button-text=\"Chat with us\"\n  button-shape=\"round\"\n  icon-url=\"https://your-domain.com/custom-chat-icon.svg\"&gt;\n&lt;/open-chat-studio-widget&gt;\n</code></pre> <p>Button Text</p> <ul> <li>When button-text is provided, the button displays both icon and text</li> <li>When button-text is empty or not provided, only an icon is shown</li> </ul> <p>Button Shape</p> <ul> <li>round - Circular button</li> <li>square - Rectangular button with rounded corners</li> </ul> <p>Icon URL</p> <p>If no icon-url is provided, the default Open Chat Studio avatar is used.</p>"},{"location":"chat_widget/reference/#button-position","title":"Button position","text":"<p>Customize the button position using CSS variables or a CSS class attached to the widget element:</p> <pre><code>open-chat-studio-widget {\n    position: fixed;\n    right: 20px;\n    bottom: 20px;\n}\n</code></pre> <p>Drag the button</p> <p>The button can be dragged by the user to anywhere on the screen. This allows the user to move the button if it is obstructing other page content. The button will return to its original position on the next page load.</p> <ul> <li> See CSS Styling for more customization options.</li> </ul>"},{"location":"chat_widget/reference/#embed-authentication","title":"Embed Authentication","text":"<p>Secure your embedded widgets with authentication keys for controlled access to specific channels.</p>"},{"location":"chat_widget/reference/#overview","title":"Overview","text":"<p>The embed authentication feature allows you to:</p> <ul> <li>Restrict widget access to authorized embeddings only</li> <li>Authenticate specific embedded channel instances</li> <li>Provide secure access control for sensitive or premium content</li> <li>Track and manage different embedded deployments</li> </ul>"},{"location":"chat_widget/reference/#implementation","title":"Implementation","text":"<pre><code>&lt;open-chat-studio-widget\n  chatbot-id=\"your-chatbot-id\"\n  embed-key=\"your-secure-embed-key\"&gt;\n&lt;/open-chat-studio-widget&gt;\n</code></pre> <p>When an embed key is provided, it's automatically sent as an <code>X-Embed-Key</code> header with all API requests to authenticate the widget instance.</p>"},{"location":"chat_widget/reference/#user-identification","title":"User Identification","text":"<p>Control how users are identified across chat sessions to enable personalized experiences and session continuity.</p>"},{"location":"chat_widget/reference/#overview_1","title":"Overview","text":"<p>The chat widget uses user identification to:</p> <ul> <li>Maintain chat history across page reloads and different visits</li> <li>Separate conversations for different users on shared devices</li> <li>Personalize interactions with user names and context</li> <li>Enable analytics and user tracking in your chat system</li> </ul>"},{"location":"chat_widget/reference/#basic-implementation","title":"Basic Implementation","text":"<p>Anonymous Users (Default) <pre><code>&lt;open-chat-studio-widget\n  chatbot-id=\"your-chatbot-id\"&gt;\n&lt;/open-chat-studio-widget&gt;\n</code></pre> Identified Users <pre><code>&lt;open-chat-studio-widget\n  chatbot-id=\"your-chatbot-id\"\n  user-id=\"user_12345\"\n  user-name=\"Sarah Johnson\"&gt;\n&lt;/open-chat-studio-widget&gt;\n</code></pre></p>"},{"location":"chat_widget/reference/#auto-generated-user-ids","title":"Auto-Generated User IDs","text":"<p>When no user-id is provided, the widget automatically creates a unique identifier:</p> <ul> <li>Example: ocs:1703123456789_a7x9k2m8f</li> </ul> <p>Persistence Behavior:</p> <ul> <li>Same browser/device: ID persists across sessions</li> <li>Different browser/device: Gets new auto-generated ID</li> <li>Incognito mode: New ID that's cleared when session ends</li> </ul>"},{"location":"chat_widget/reference/#dynamic-user-management","title":"Dynamic User Management","text":"<p>Update user identification when authentication state changes: <pre><code>function updateChatUser(user) {\n  const widget = document.querySelector('open-chat-studio-widget');\n\n  if (user) {\n    widget.userId = user.id;\n    widget.userName = user.name;\n  }\n}\n</code></pre></p>"},{"location":"chat_widget/reference/#welcome-messages","title":"Welcome Messages","text":"<p>Enhance user experience by displaying personalized greeting messages when the chat opens. These messages appear as bot messages at the beginning of the conversation. Welcome messages are perfect for:</p> <ul> <li>Greeting users and introducing your bot's capabilities</li> <li>Providing context about what kind of help is available</li> <li>Creating a warm, engaging first impression</li> </ul> <p>Pass welcome messages as a JSON array string. Each message appears as a separate message bubble.</p> <pre><code>&lt;open-chat-studio-widget\n welcome-messages=\"['Hi! Welcome to our support chat.', 'How can I assist you today?']\"\n&gt;\n&lt;/open-chat-studio-widget&gt;\n</code></pre>"},{"location":"chat_widget/reference/#starter-questions","title":"Starter Questions","text":"<p>Accelerate user engagement with pre-defined clickable questions that address common queries. These starter questions help users quickly find what they're looking for without having to type, which improves the user experience. Starter questions are ideal for:</p> <ul> <li>Highlighting your most frequently asked questions</li> <li>Guiding users toward key features or information</li> <li>Improving accessibility for users who prefer clicking to typing</li> </ul> <p>These questions appear as blue-outlined buttons aligned to the right (similar to user messages), making it clear that they are user actions. When clicked, they automatically send that question as a user message, initiating the conversation flow. The starter questions disappear after the user clicks one or starts typing their own message.</p> <pre><code>&lt;open-chat-studio-widget\n starter-questions=\"[\n   'I need technical support',\n   'Tell me about pricing',\n   'Schedule a demo',\n   'Contact sales team'\n ]\"&gt;\n&lt;/open-chat-studio-widget&gt;\n</code></pre>"},{"location":"chat_widget/reference/#file-attachments","title":"File Attachments","text":"<p>Enable users to send files along with their messages. This feature is perfect for support scenarios where users need to share screenshots, documents, or other files.</p> <pre><code>&lt;open-chat-studio-widget\n allow-attachments=\"true\"&gt;\n&lt;/open-chat-studio-widget&gt;\n</code></pre>"},{"location":"chat_widget/reference/#supported-file-types","title":"Supported File Types","text":""},{"location":"chat_widget/reference/#documents","title":"Documents:","text":"<ul> <li>Text files: All text/* types (including .txt, .csv, .log, .md, and more)</li> <li>PDF documents: .pdf</li> <li>Microsoft Word: .doc, .docx</li> <li>Microsoft Excel: .xls, .xlsx</li> </ul>"},{"location":"chat_widget/reference/#images","title":"Images:","text":"<ul> <li>Common formats: .jpg, .jpeg, .png, .gif, .bmp, .webp</li> <li>Vector graphics: .svg</li> </ul>"},{"location":"chat_widget/reference/#media","title":"Media:","text":"<ul> <li>Video files: .mp4, .mov, .avi</li> <li>Audio files: .mp3, .wav</li> </ul>"},{"location":"chat_widget/reference/#file-size-limits","title":"File Size Limits","text":"<ul> <li>Maximum file size: 50MB per individual file</li> <li>Maximum total size: 50MB for all files combined in a single message</li> <li>Multiple files: Users can attach multiple files as long as the total doesn't exceed 50MB</li> </ul>"},{"location":"chat_widget/reference/#user-experience","title":"User Experience","text":"<ol> <li>Users click the paperclip icon next to the send button to select files</li> <li>Selected files appear in a preview area above the input field</li> <li>Files show name, size, and upload status</li> <li>Users can remove files before sending by clicking the X button</li> <li>Error messages appear for unsupported file types or files exceeding size limits</li> <li>Files are uploaded when the message is sent</li> </ol> <p>See CSS Styling for customization options</p>"},{"location":"chat_widget/reference/#internationalization","title":"Internationalization","text":"<p>The chat widget supports multiple languages and custom translations for all UI text elements.</p>"},{"location":"chat_widget/reference/#built-in-language-support","title":"Built-in Language Support","text":"<p>The widget includes built-in translations for the following languages:</p> <ul> <li>English (<code>en</code>) - Default</li> <li>Spanish (<code>es</code>)</li> <li>French (<code>fr</code>)</li> <li>Arabic (<code>ar</code>)</li> <li>Hindi (<code>hi</code>)</li> <li>Italian (<code>it</code>)</li> <li>Portuguese (<code>pt</code>)</li> <li>Swahili (<code>sw</code>)</li> <li>Ukrainian (<code>uk</code>)</li> </ul> <pre><code>&lt;open-chat-studio-widget\n  chatbot-id=\"your-chatbot-id\"\n  language=\"es\"&gt;\n&lt;/open-chat-studio-widget&gt;\n</code></pre>"},{"location":"chat_widget/reference/#custom-translations","title":"Custom Translations","text":"<p>You can provide custom translations using a JSON file hosted on your server:</p> <pre><code>&lt;open-chat-studio-widget\n  chatbot-id=\"your-chatbot-id\"\n  translations-url=\"https://yoursite.com/custom-translations.json\"&gt;\n&lt;/open-chat-studio-widget&gt;\n</code></pre>"},{"location":"chat_widget/reference/#translation-file-format","title":"Translation File Format","text":"<p>Provide translations as a flat JSON object that uses dot-notation keys. These keys loosely group strings by the part of the widget they affect:</p> <pre><code>{\n  \"launcher.open\": \"Open chat\",\n  \"window.close\": \"Close\",\n  \"window.newChat\": \"Start new chat\",\n  \"window.fullscreen\": \"Enter fullscreen\",\n  \"window.exitFullscreen\": \"Exit fullscreen\",\n  \"attach.add\": \"Attach files\",\n  \"attach.remove\": \"Remove file\",\n  \"attach.success\": \"File attached\",\n  \"status.starting\": \"Starting chat...\",\n  \"status.typing\": \"Preparing response\",\n  \"status.uploading\": \"Uploading\",\n  \"modal.newChatTitle\": \"Start New Chat\",\n  \"modal.newChatBody\": \"Starting a new chat will clear your current conversation. Continue?\",\n  \"modal.cancel\": \"Cancel\",\n  \"modal.confirm\": \"Confirm\",\n  \"composer.placeholder\": \"Type a message...\",\n  \"composer.send\": \"Send message\",\n  \"error.fileTooLarge\": \"File too large\",\n  \"error.totalTooLarge\": \"Total file size too large\",\n  \"error.unsupportedType\": \"Unsupported file type\",\n  \"error.connection\": \"Connection error. Please try again.\",\n  \"error.sessionExpired\": \"Session expired. Please start a new chat.\",\n  \"branding.poweredBy\": \"Powered by\",\n  \"branding.buttonText\": \"\",\n  \"branding.headerText\": \"\",\n  \"content.welcomeMessages\": [],\n  \"content.starterQuestions\": []\n}\n</code></pre>"},{"location":"chat_widget/reference/#translation-key-reference","title":"Translation Key Reference","text":"<p>Use the following reference when creating or updating translation bundles (mirrors the widget's <code>en.json</code>):</p> <ul> <li>launcher</li> <li><code>launcher.open</code> \u2014 Launcher button label, aria-label, and tooltip.</li> <li>window</li> <li><code>window.close</code> \u2014 Closes the chat window.</li> <li><code>window.newChat</code> \u2014 Menu item to start a new chat.</li> <li><code>window.fullscreen</code> \u2014 Enters fullscreen mode.</li> <li><code>window.exitFullscreen</code> \u2014 Leaves fullscreen mode.</li> <li>attach</li> <li><code>attach.add</code> \u2014 Adds file attachments.</li> <li><code>attach.remove</code> \u2014 Removes a pending attachment.</li> <li><code>attach.success</code> \u2014 Upload queued confirmation.</li> <li>status</li> <li><code>status.starting</code> \u2014 Shown while the session initializes.</li> <li><code>status.typing</code> \u2014 Typing indicator text (previously <code>typingIndicatorText</code>).</li> <li><code>status.uploading</code> \u2014 Attachment upload in progress.</li> <li>modal</li> <li><code>modal.newChatTitle</code> \u2014 \"Start new chat\" dialog title.</li> <li><code>modal.newChatBody</code> \u2014 Dialog body text.</li> <li><code>modal.cancel</code> \u2014 Dialog cancel button.</li> <li><code>modal.confirm</code> \u2014 Dialog confirmation button.</li> <li>composer</li> <li><code>composer.placeholder</code> \u2014 Message composer placeholder text.</li> <li><code>composer.send</code> \u2014 Send button text.</li> <li>error</li> <li><code>error.fileTooLarge</code> \u2014 Single file size violation.</li> <li><code>error.totalTooLarge</code> \u2014 Combined size violation.</li> <li><code>error.unsupportedType</code> \u2014 Unsupported file format.</li> <li><code>error.connection</code> \u2014 Generic connection failure.</li> <li><code>error.sessionExpired</code> \u2014 Session expiration prompt.</li> <li>branding</li> <li><code>branding.poweredBy</code> \u2014 Footer \"Powered by\" label.</li> <li><code>branding.buttonText</code> \u2014 Optional launcher text override; leave blank to use widget props.</li> <li><code>branding.headerText</code> \u2014 Optional header title override; leave blank to use widget props.</li> <li>content</li> <li><code>content.welcomeMessages</code> \u2014 Array of initial bot messages (empty array falls back to props).</li> <li><code>content.starterQuestions</code> \u2014 Array of starter questions (empty array falls back to props).</li> </ul>"},{"location":"chat_widget/reference/#customizable-content","title":"Customizable Content","text":"<p>You can override specific widget content through translations.</p> <p>If you only want to override some text, include just those keys in your custom translation file. The widget will use the values from the default language file for provided languages or fall back to English. Arrays must remain arrays, and null values in <code>branding.buttonText</code> defer to the runtime HTML attribute or prop.</p>"},{"location":"chat_widget/reference/#translation-priority","title":"Translation Priority","text":"<p>The widget uses the following priority order for text content:</p> <ol> <li>Custom translations from <code>translations-url</code> (highest priority)</li> <li>Built-in language translations (if <code>language</code> is specified)</li> <li>Widget props / HTML attributes (used when translation keys are null or missing - \u26a0\ufe0f DEPRECATED for long-term use)</li> <li>English defaults (lowest priority)</li> </ol> <p>Deprecation Notice</p> <p>HTML attributes for text content (<code>header-text</code>, <code>typing-indicator-text</code>, <code>new-chat-confirmation-message</code>) are deprecated and will be removed in a future major release. Please migrate to using the translations system for better internationalization support. Leave translation values blank when you want the widget props to supply the text instead of duplicating it.</p>"},{"location":"chat_widget/reference/#persistent-sessions","title":"Persistent Sessions","text":"<p>By default, the widget will save the chat messages in the browser local storage. This allows users to continue sessions after reloading the page or navigating to a new page. In addition to automatic session expiration, the user can also use the 'new chat' button to start a new session.</p> <p>To disable this feature, set the <code>persistent-session=\"false\"</code> attribute on the widget element.</p> <p>Note</p> <p>The session persistence is associated with the <code>chatbot-id</code>. If the <code>chatbot-id</code> changes, any previous session data will be ignored.</p> <p>The session data is set to expire after 24 hours. This is also configurable by using the <code>persistent-session-expire</code> attribute. The value is interpreted as \"the number of minutes since the last message before the session expires\". Setting this attribute to <code>0</code> will disable the expiration entirely.</p> <p>Note</p> <p>Session persistence works in conjunction with User Identification. Different users will have separate persistent sessions.</p>"},{"location":"chat_widget/reference/#page-context","title":"Page Context","text":"<p>Pass page-specific context to the bot with each message to enable more personalized and relevant responses. The context is automatically included with every user message and helps the bot understand the current page state and user environment.</p>"},{"location":"chat_widget/reference/#use-cases","title":"Use Cases","text":"<p>Page context is ideal for:</p> <ul> <li>Providing current user role or permissions to the bot</li> <li>Sharing page URL, document name, or current section information</li> <li>Passing feature flags or configuration state</li> <li>Including product/service information relevant to the page</li> </ul>"},{"location":"chat_widget/reference/#basic-implementation_1","title":"Basic Implementation","text":"<p>Context must be set using JavaScript with a plain object:</p> <pre><code>const widget = document.querySelector('open-chat-studio-widget');\n\nwidget.pageContext = {\n  user_role: 'admin',\n  page_location: 'dashboard'\n};\n</code></pre>"},{"location":"chat_widget/reference/#dynamic-context-management","title":"Dynamic Context Management","text":"<p>Update context dynamically using JavaScript when page state changes:</p> <pre><code>const widget = document.querySelector('open-chat-studio-widget');\n\n// Set initial context\nlet pageContext = {\n  user_role: 'customer',\n  page_location: 'product-listing',\n  product_category: 'electronics'\n};\n\nwidget.pageContext = pageContext;\n\n// Update context when user navigates\ndocument.addEventListener('navigate', (event) =&gt; {\n  let newContext = {\n    ...pageContext,\n    page_location: event.detail.location\n  };\n  widget.pageContext = newContext;\n});\n</code></pre>"},{"location":"chat_widget/reference/#context-object-structure","title":"Context Object Structure","text":"<p>The <code>pageContext</code> property accepts a plain JavaScript object (not a JSON string). Each property is included in the context sent to the bot:</p> <pre><code>widget.pageContext = {\n  // User information\n  user_role: 'support_agent',\n  user_department: 'technical_support',\n\n  // Page information\n  page_location: 'help-center',\n  page_title: 'Troubleshooting Guide',\n\n  // Application state\n  session_type: 'trial',\n  account_status: 'active',\n\n  // Custom data\n  feature_flags: ['new_ui', 'beta_features'],\n  request_context: 'urgent'\n};\n</code></pre> <p>Note</p> <p>The page context is persisted in the session state on the server side and is accessible via <code>session_state.remote_context</code>. See accessing remote context for more details.</p>"},{"location":"chat_widget/reference/#properties-reference","title":"Properties Reference","text":""},{"location":"chat_widget/reference/#core-configuration","title":"Core Configuration","text":"Property Type Required Default Description Example <code>chatbot-id</code> <code>string</code> REQUIRED - Your chatbot ID from Open Chat Studio <code>\"183312ac-cbe5-4c91-9e7b-d9df96b088e4\"</code> <code>api-base-url</code> <code>string</code> Optional <code>\"https://openchatstudio.com\"</code> API base URL for your Open Chat Studio instance <code>\"https://your-domain.com\"</code> <code>embed-key</code> <code>string</code> Optional <code>undefined</code> Authentication key for embedded channels <code>\"your-embed-auth-key\"</code>"},{"location":"chat_widget/reference/#button-ui-customization","title":"Button &amp; UI Customization","text":"Property Type Required Default Validation Description Example <code>button-text</code> <code>string</code> Optional <code>undefined</code> Max 50 chars Button display text. If empty, shows icon only <code>\"Need Help?\"</code> <code>button-shape</code> <code>string</code> Optional <code>\"square\"</code> <code>\"round\"</code> | <code>\"square\"</code> Button shape style <code>\"round\"</code> for circular button <code>icon-url</code> <code>string</code> Optional OCS default logo Valid URL Custom icon for the chat button <code>\"https://yoursite.com/chat-icon.svg\"</code> <code>visible</code> <code>boolean</code> Optional <code>false</code> <code>true</code> | <code>false</code> Show widget immediately on page load <code>\"true\"</code> to auto-open <code>position</code> <code>string</code> Optional <code>\"right\"</code> <code>\"left\"</code> | <code>\"center\"</code> | <code>\"right\"</code> Initial widget position on screen <code>\"left\"</code> for left side placement <code>header-text</code> <code>string</code> Optional <code>undefined</code> Max 100 chars \u26a0\ufe0f DEPRECATED: Text displayed in chat window header. Use <code>branding.headerText</code> in translations instead <code>\"Customer Support\"</code>"},{"location":"chat_widget/reference/#user-management","title":"User Management","text":"Property Type Required Default Validation Description Example <code>user-id</code> <code>string</code> Optional Auto-generated Alphanumeric + underscore/dash Unique user identifier for session continuityAuto-format: <code>ocs:1703123456789_a7x9k2m8f</code> <code>\"user_12345\"</code> or <code>\"customer@email.com\"</code> <code>user-name</code> <code>string</code> Optional <code>undefined</code> Max 200 chars Display name sent to chat API for personalization <code>\"John Smith\"</code> or <code>\"Customer #12345\"</code>"},{"location":"chat_widget/reference/#chat-behavior-sessions","title":"Chat Behavior &amp; Sessions","text":"Property Type Required Default Validation Description Example <code>pageContext</code> <code>object</code> Optional <code>undefined</code> Plain JS object Optional context data to send with each message for personalizationNote: Context is cleared after each message <code>{\"user_role\": \"admin\", \"page_location\": \"dashboard\"}</code> <code>persistent-session</code> <code>boolean</code> Optional <code>true</code> <code>true</code> | <code>false</code> Save chat history in browser localStorage <code>\"false\"</code> to disable session saving <code>persistent-session-expire</code> <code>number</code> Optional <code>1440</code> (24 hours) 0-43200 (30 days) Minutes before session expires <code>720</code> for 12 hours, <code>0</code> for never expire <code>allow-full-screen</code> <code>boolean</code> Optional <code>true</code> <code>true</code> | <code>false</code> Enable fullscreen mode button <code>\"false\"</code> to hide fullscreen option <code>allow-attachments</code> <code>boolean</code> Optional <code>false</code> <code>true</code> | <code>false</code> Enable file upload functionalityLimits: 50MB per file, 50MB total per message <code>\"true\"</code> to enable file uploads"},{"location":"chat_widget/reference/#messages-content","title":"Messages &amp; Content","text":"Property Type Required Default Validation Description Example <code>welcome-messages</code> <code>string</code> Optional <code>undefined</code> Valid JSON arrayMax: 5 messages, 500 chars each Welcome messages shown when chat opensFormat: <code>'[\"Message 1\", \"Message 2\"]'</code> <code>'[\"Welcome!\", \"How can I help?\"]'</code> <code>starter-questions</code> <code>string</code> Optional <code>undefined</code> Valid JSON arrayMax: 6 questions, 100 chars each Clickable question buttons to start conversationFormat: <code>'[\"Question 1\", \"Question 2\"]'</code> <code>'[\"Check my order\", \"Technical support\"]'</code> <code>typing-indicator-text</code> <code>string</code> Optional <code>\"Preparing response\"</code> Max 50 chars \u26a0\ufe0f DEPRECATED: Text shown while bot is typing. Use <code>status.typing</code> in translations instead <code>\"AI is thinking...\"</code> <code>new-chat-confirmation-message</code> <code>string</code> Optional <code>\"Starting a new chat will clear your current conversation. Continue?\"</code> Max 200 chars \u26a0\ufe0f DEPRECATED: Confirmation dialog text for new chat button. Use <code>modal.newChatBody</code> in translations instead <code>\"Start over? Your current chat will be lost.\"</code>"},{"location":"chat_widget/reference/#internationalization-translations","title":"Internationalization &amp; Translations","text":"Property Type Required Default Validation Description Example <code>language</code> <code>string</code> Optional <code>\"en\"</code> Valid language code Language code for widget UI (en, es, fr, ar, hi, it, pt, sw, uk) <code>\"es\"</code> for Spanish <code>translations-url</code> <code>string</code> Optional <code>undefined</code> Valid URL URL to custom JSON translations file for widget strings <code>\"https://yoursite.com/translations.json\"</code>"},{"location":"chat_widget/styling/","title":"CSS Styling","text":"<p>Customization of the widget appearance can be done using CSS variables as follows:</p> <pre><code>open-chat-studio-widget {\n    --button-background-color: #d1d5db;\n}\n</code></pre> <p>The following tables contain the full list of CSS variables that are available.</p>"},{"location":"chat_widget/styling/#launch-button","title":"Launch button","text":"<p>Customize the button position using CSS variables or a CSS class attached to the widget element:</p> <pre><code>open-chat-studio-widget {\n    position: fixed;\n    right: 20px;\n    bottom: 20px;\n}\n</code></pre> <p>Use the following CSS variables to style the button appearance.</p> Name Description <code>--button-background-color</code> Button background color  (#ffffff) <code>--button-background-color-hover</code> Button background color on hover  (#f3f4f6) <code>--button-border-color</code> Button border color  (#6b7280) <code>--button-border-color-hover</code> Button border color on hover  (#374151) <code>--button-font-size</code> Button text font size (0.875em) <code>--button-icon-size</code> Button icon size (1.5em) <code>--button-text-color</code> Button text color  (#111827) <code>--button-text-color-hover</code> Button text color on hover  (#1d4ed8)"},{"location":"chat_widget/styling/#chat-window","title":"Chat Window","text":"<p>Tip</p> <p>All font sizes and some margins / padding are relative to the widget element's font size which can be set using the <code>--chat-window-font-size</code> and <code>--button-font-size</code> variables.</p> Name Description <code>--chat-window-width</code> Chat window height in pixels or percent (25%) <code>--chat-window-height</code> Chat window width in pixels or percent (60%) <code>--chat-window-fullscreen-width</code> Chat window fullscreen width in pixels or percent (80%) <code>--chat-window-bg-color</code> Chat window background color  (#ffffff) <code>--chat-window-border-color</code> Chat window border color  (#d1d5db) <code>--chat-window-font-size</code> Default font size for text in the chat window (0.875em) <code>--chat-window-font-size-sm</code> Font size for small text in the chat window (0.75em) <code>--chat-window-shadow-color</code> Chat window shadow color  (rgba(0, 0, 0, 0.1))"},{"location":"chat_widget/styling/#header","title":"Header","text":"Name Description <code>--header-bg-color</code> Header background color (transparent) <code>--header-bg-hover-color</code> Header background color on hover  (#f9fafb) <code>--header-border-color</code> Header border color  (#f3f4f6) <code>--header-button-bg-hover-color</code> Header button background on hover  (#f3f4f6) <code>--header-button-text-color</code> Header button text color  (#6b7280) <code>--header-button-icon-size</code> Icon size for buttons in the header (1.5em) <code>--header-font-size</code> Header font size (1em) <code>--header-text-color</code> Color for the text in the header  (#525762) <code>--header-text-font-size</code> Font size for the text in the header (1em)"},{"location":"chat_widget/styling/#messages","title":"Messages","text":"Name Description <code>--message-user-bg-color</code> User message background color  (#e4edfb) <code>--message-user-text-color</code> User message text color  (#1f2937) <code>--message-user-link-color</code> User message link color  (#155dfc) <code>--message-assistant-bg-color</code> Assistant message background color  (#eae7e8) <code>--message-assistant-text-color</code> Assistant message text color (--message-user-text-color) <code>--message-assistant-link-color</code> Assistant message text color (--message-user-link-color) <code>--message-system-bg-color</code> System message background color  (#fbe4f8) <code>--message-system-text-color</code> System message text color (--message-user-text-color) <code>--message-system-link-color</code> System message text color (--message-user-link-color) <code>--message-timestamp-assistant-color</code> Assistant message timestamp color  (rgba(75, 85, 99, 0.7)) <code>--message-timestamp-color</code> User message timestamp color  (rgba(255, 255, 255, 0.7))"},{"location":"chat_widget/styling/#starter-question","title":"Starter question","text":"Name Description <code>--starter-question-bg-color</code> Starter question background color (transparent) <code>--starter-question-bg-hover-color</code> Starter question background on hover  (#eff6ff) <code>--starter-question-border-color</code> Starter question border color  (#3b82f6) <code>--starter-question-border-hover-color</code> Starter question border on hover  (#2563eb) <code>--starter-question-text-color</code> Starter question text color  (#3b82f6)"},{"location":"chat_widget/styling/#confirmation-dialog","title":"Confirmation dialog","text":"Name Description <code>--confirmation-overlay-bg-color</code> Confirmation dialog overlay background color  (rgba(0, 0, 0, 0.5)) <code>--confirmation-dialog-bg-color</code> Confirmation dialog background color (uses --chat-window-bg-color) <code>--confirmation-dialog-border-color</code> Confirmation dialog border color (uses --chat-window-border-color) <code>--confirmation-dialog-shadow-color</code> Confirmation dialog shadow color (uses --chat-window-shadow-color) <code>--confirmation-title-color</code> Confirmation dialog title text color  (#111827) <code>--confirmation-title-font-size</code> Confirmation dialog title font size (1.125em) <code>--confirmation-message-color</code> Confirmation dialog message text color (uses --loading-text-color) <code>--confirmation-message-font-size</code> Confirmation dialog message font size (1em) <code>--confirmation-button-cancel-bg-color</code> Cancel button background color (uses --button-background-color-hover) <code>--confirmation-button-cancel-bg-hover-color</code> Cancel button background on hover  (#e5e7eb) <code>--confirmation-button-cancel-text-color</code> Cancel button text color (uses --header-button-text-color) <code>--confirmation-button-confirm-bg-color</code> Confirm button background color (uses --error-text-color) <code>--confirmation-button-confirm-bg-hover-color</code> Confirm button background on hover (uses --error-text-color) <code>--confirmation-button-confirm-text-color</code> Confirm button text color (uses --send-button-text-color)"},{"location":"chat_widget/styling/#input-bar","title":"Input bar","text":"Name Description <code>--input-bg-color</code> Input area background color (transparent) <code>--input-border-color</code> Input field border color  (#d1d5db) <code>--input-outline-focus-color</code> Input field focus ring color  (#3b82f6) <code>--input-placeholder-color</code> Input placeholder text color  (#6b7280) <code>--input-text-color</code> Input text color  (#111827)"},{"location":"chat_widget/styling/#file-attachments","title":"File Attachments","text":"Name Description <code>--file-attachment-button-bg-color</code> Attach file button background color (transparent) <code>--file-attachment-button-bg-hover-color</code> Attach file button background hover color (--header-button-bg-hover-color) <code>--file-attachment-button-text-color</code> Attach file button text color (--header-button-text-color) <code>--file-attachment-button-text-disabled-color</code> Attach file button disabled text color (--send-button-text-disabled-color) <code>--selected-file-bg-color</code> Selected file item background color (--message-system-bg-color) <code>--selected-file-font-size</code> Selected file item font size (--chat-window-font-size-sm) <code>--selected-file-icon-size</code> Selected file item icon size (1.25em) <code>--selected-file-name-color</code> Selected file name color (--message-assistant-text-color) <code>--selected-file-remove-icon-color</code> Selected file remove icon color (--error-text-color) <code>--selected-file-remove-icon-hover-color</code> Selected file remove icon hover  (#dc2626) <code>--selected-file-size-color</code> Selected file size display color (--input-placeholder-color) <code>--selected-files-bg-color</code> Selected files container background color (--chat-window-bg-color) <code>--selected-files-border-color</code> Selected files container border color (--header-border-color) <code>--message-attachment-icon-size</code> Message attachment icon size (1em)"},{"location":"chat_widget/styling/#send-button","title":"Send button","text":"Name Description <code>--send-button-bg-color</code> Send button background color  (#3b82f6) <code>--send-button-bg-disabled-color</code> Send button background when disabled  (#d1d5db) <code>--send-button-bg-hover-color</code> Send button background on hover  (#2563eb) <code>--send-button-text-color</code> Send button text color  (#ffffff) <code>--send-button-text-disabled-color</code> Send button text when disabled  (#6b7280)"},{"location":"chat_widget/styling/#loading-indicators","title":"Loading indicators","text":"Name Description <code>--loading-spinner-fill-color</code> Loading spinner fill color  (#3b82f6) <code>--loading-spinner-size</code> Loading spinner size (1.25em) <code>--loading-spinner-track-color</code> Loading spinner track color  (#e5e7eb) <code>--loading-text-color</code> Loading text color  (#6b7280) <code>--typing-progress-bg-color</code> Typing progress bar background color  (#ade3ff)"},{"location":"chat_widget/styling/#markdown-code","title":"Markdown code","text":"<p>By default, the Markdown code colours are relative to the respective message text and background colours, but they can be overridden. </p> Name Description <code>--code-bg-assistant-color</code> Code background in assistant messages (--message-assistant-bg-color + 50% white) <code>--code-border-assistant-color</code> Code border in assistant messages (--message-assistant-bg-color + 10% black) <code>--code-text-assistant-color</code> Code text color in assistant messages (--message-assistant-text-color) <code>--code-bg-user-color</code> Code background in user messages (--message-user-bg-color + 20% white) <code>--code-border-user-color</code> Code border in user messages (--message-user-bg-color + 20% black) <code>--code-text-user-color</code> Code text color in user messages (--message-user-text-color)"},{"location":"chat_widget/styling/#error-message","title":"Error message","text":"Name Description <code>--error-text-color</code> Error text color  (#ef4444) <code>--success-text-color</code> Success text color  (#10b981)"},{"location":"chat_widget/styling/#z-index","title":"Z-Index","text":"<p>If the chatbot appears below other elements on the page you can increase the <code>z-index</code> of the chatbot by setting the <code>--chat-z-index</code> CSS variable. The default value is <code>50</code>.</p> <pre><code>open-chat-studio-widget {\n    --chat-z-index: 100;\n}\n</code></pre> <p>In some cases, it may also be necessary to reduce the z-index of other elements on the page.</p>"},{"location":"concepts/","title":"Conceptual Guide","text":"<p>This guide provides explanations of the key concepts behind the Open Chat Studio platform and AI applications more broadly.</p> <p>The conceptual guide does not cover step-by-step instructions or specific examples \u2014 those are found in the How-to guides.</p>"},{"location":"concepts/#terms","title":"Terms","text":"Assistant A chatbot that uses OpenAI`s Assistant API. Assistants can write and execute code and search and reference   information in uploaded files. Authentication Provider Authentication providers allow you to authenticate with external systems to access data or services. Channel The platform through which a chat occurs (e.g., WhatsApp, Telegram, Web, Slack). Consent Forms Forms that provide context to chatbot users on how their data will be used and who to contact regarding any concerns. Custom Actions Custom actions are a way to extend the functionality of the bot by integrating with external systems via HTTP APIs. Evaluations Evaluations is a testing system for measuring chatbot performance against different metrics. Events Events are a way to trigger actions in the bot based on specific conditions. Experiment The current name used in Open Chat Studio to refer to a chatbot. An experiment links all the configuration and data for a chatbot, including user sessions, data, actions, etc. Large Language Models (LLMs) Large language models are a type of AI model that can generate human-like text, images and audio. Messaging Provider Messaging providers hold the configuration required to send messages to users on a specific channel. Participant Data Data that persists across sessions and is tied to the same <code>User, Channel, Chatbot</code> scope. It helps retain long-term user preferences and contextual information beyond a single session. Pipelines A pipeline is a way to build a bot by combining one or more steps together. Prompt A prompt is the instructions that are given to the LLM to generate a response. Prompts can include text, source material, and other variables. Session The scope of conversations between a user and a chatbot within a specific channel. Sessions are isolated, ensuring data privacy and contextual continuity for the duration of an interaction. Source Material Additional information that can be included in the bot prompt using the <code>{source_material}</code> prompt variable. Tracing Provider Tracing providers hold the configuration required to send traces to the tracing provider. Versions The ability to create and manage different versions of a chatbot."},{"location":"concepts/assistants/","title":"OpenAI Assistants","text":"<p>Deprecation Warning</p> <p>OpenAI has deprecated Assistants and will completely remove support on 2026-08-26.</p> <p>Open Chat Studio supports all the features of Assistants in other ways. To find out about migrating away from Assistants see the migration guide.</p>"},{"location":"concepts/assistants/#syncing-with-openai","title":"Syncing with OpenAI","text":"<p>The in-sync status with OpenAI is automatically checked each time a user visits the edit screen of an assistant. If the assistant in OCS has the identical configurations and files with the assistant in OpenAI, then an in-sync status will appear under the assistant id: </p> <p>Otherwise, a warning will be displayed explaining what is out of sync. For example, in the image below, there are files uploaded in OpenAI that are not uploaded in OCS. This may result in unexpected behavior from the assistant. To resolve, upload the listed files in the edit screen. </p> <p>Assistants in versioned experiments</p> <p>Although an assistant cannot be modified in OCS once an experiment is released that references that assistant, it can still be modified in OpenAI. A new assistant in OpenAI will be created at the time of the experiment's release, and it is recommended not to modify that assistant to maintain the expected functionality of the released experiment.</p>"},{"location":"concepts/assistants/#archiving","title":"Archiving","text":"<ul> <li>Goal: Archiving an assistant in OCS deletes the associated assistant in OpenAI. This is an easy way to stop incurring costs and ensure that the assistant is closed for all experiments and pipelines that reference it within a project.</li> <li>OCS first checks if any published or unreleased experiments and pipelines reference the assistant. If so, the archival process is blocked, and a modal appears listing those experiments and pipelines preventing the archival. If only released versions reference the assistant, they are archived automatically.</li> <li>If any published or unreleased experiments reference the assistant, they are listed under the \u201cExperiments\u201d section of the modal. For published experiments, navigate to those versions and archive them. For unreleased versions, you must navigate to the version and either (1) remove the assistant reference or (2) archive the experiment.</li> <li>If a pipeline that references an assistant is not referenced by an experiment, the pipeline must be archived. These pipelines are listed under the \u201cPipelines\u201d section of the modal. Navigate to the pipeline and either archive it or remove the assistant reference to unblock the assistant archival.</li> <li>If a pipeline references an assistant and that pipeline is used by a published experiment, you must archive the experiment. These experiments are listed under the \u201cExperiments Referencing Pipeline\u201d section of the modal. The links direct you to the experiments where the pipeline is used. Navigate to the version listed in the modal and archive it.</li> </ul> <p>Archiving an assistant with versions</p> <p>If the assistant you are trying to archive has versions, the same checks apply to all versions of the assistant and are displayed together in the modal. Once confirmed, all assistant versions will be archived.</p>"},{"location":"concepts/channels/","title":"Channels","text":"<p>To enable users to interact with your bot through external social media platforms and similar services, OCS integrates with various messaging providers. This integration allows you to deploy your bot to external platforms. Once a platform is linked to your bot, users can communicate with it through that platform. In OCS, the term \"channel\" is synonymous with \"platform.\"</p> <p>The currently supported channels are:</p> <ul> <li>Telegram</li> <li>WhatsApp</li> <li>Facebook Messenger</li> <li>Slack</li> <li>API</li> <li>SureAdhere</li> </ul>"},{"location":"concepts/channels/#see-also","title":"See also","text":"<ul> <li>Deploying your bot to different channels</li> </ul>"},{"location":"concepts/consent/","title":"Consent Forms","text":"<p>Consent forms allow chatbot makers to provide context to chatbot users on how their data will be used, and who to contact regarding any concerns. Consent forms are displayed to users before they start interacting with the chatbot.</p> <p></p> <p>Using consent forms on WhatsApp, Telegram and other channels</p> <p>If you are deploying your chatbot on WhatsApp, Telegram or any channels other than the Web channel, you can still include consent forms in your chatbot by enabling the 'Conversational Consent' option for the chatbot.</p> <p>A default consent form is created for each team. You can customize this default form or create new forms by navigating to the \"Consent Forms\" section of the platform.</p>"},{"location":"concepts/consent/#what-to-put-in-a-consent-form","title":"What to put in a consent form","text":"<p>Some common elements you may want to include in a consent form are:</p> <ul> <li>A disclaimer stating that the accuracy of chatbot responses is not guaranteed.</li> <li>How you might use data from chatbot interactions.</li> <li>An email address and phone number for the relevant team responsible for managing the chatbot.</li> </ul>"},{"location":"concepts/custom_actions/","title":"Custom Actions","text":"<p>Custom Actions enable bots to communicate with external services via HTTP API calls.  This feature allows you to extend the functionality of your bot by integrating it with other services.</p> <p>This feature is analogous to the OpenAI's GPT Actions feature.</p>"},{"location":"concepts/custom_actions/#custom-action-fields","title":"Custom Action Fields","text":""},{"location":"concepts/custom_actions/#authentication-provider","title":"Authentication Provider","text":"<p>Before you create a Custom Action will need to create an Authentication Provider for your action to use (unless the API you are using does not require authentication). You can do this by navigating to the Authentication Providers section in Team Settings and creating a new Authentication Provider.</p>"},{"location":"concepts/custom_actions/#base-url","title":"Base URL","text":"<p>This is the URL of the external service you want to communicate with. For example: <code>https://www.example.com</code>. Only HTTPS URLs are supported.</p>"},{"location":"concepts/custom_actions/#api-schema","title":"API Schema","text":"<p>This is a JSON or YAML OpenAPI Schema document.</p> <p>You should be able to get this from the service you want to connect to. For example, the default location for the schema for FastAPI services is <code>/openapi.json</code> (https://fastapi.tiangolo.com/tutorial/first-steps/#openapi-and-json-schema).</p>"},{"location":"concepts/custom_actions/#how-custom-actions-work","title":"How Custom Actions work","text":"<p>When you create a custom action, each API endpoint in the OpenAPI schema will be available as a separate action in the Experiment configuration. This gives you full control over which actions are available to your bot.</p> <p>When you add a Custom Action to your Experiment, the bot will be able to make HTTP requests to the external service using the API endpoints you have configured. The bot will send the request and receive the response from the external service, which it can then use to generate a response to the user.</p>"},{"location":"concepts/custom_actions/#health-status-monitoring","title":"Health Status Monitoring","text":"<p>Open Chat Studio automatically monitors the health of your custom actions to help you ensure your external services are available when your bot needs them. This feature provides visibility into the operational status of your integrations and helps you quickly identify connection issues.</p>"},{"location":"concepts/custom_actions/#how-health-checks-work","title":"How Health Checks Work","text":"<p>The system automatically queries the health check endpoint of each custom action every 5 minutes. This periodic check verifies that the target server is reachable and responding correctly. The health status is determined by whether the server responds successfully to these automated checks.</p> <p>Health Check Endpoint</p> <p>The health check queries the base URL of your custom action. Ensure your external service has a health check endpoint configured at the base URL or root path.</p>"},{"location":"concepts/custom_actions/#viewing-health-status","title":"Viewing Health Status","text":"<p>You can view the health status of your custom actions in two places:</p> <ol> <li> <p>Custom Actions Table: The health status is displayed as a column in the table showing all your custom actions. This gives you an at-a-glance view of which services are operational.</p> </li> <li> <p>Edit Custom Action Page: When editing a custom action, the current health status is displayed, allowing you to verify connectivity while configuring your integration.</p> </li> </ol>"},{"location":"concepts/custom_actions/#manual-health-checks","title":"Manual Health Checks","text":"<p>In addition to automatic monitoring, you can manually trigger a health check at any time. This is useful when:</p> <ul> <li>You've just configured a new custom action and want to verify connectivity immediately</li> <li>You've made changes to your external service and want to confirm it's still reachable</li> <li>The automatic check shows a service is down and you want to verify it's back online</li> </ul> <p>Manual health checks are performed synchronously, giving you immediate feedback on the service status.</p>"},{"location":"concepts/custom_actions/#health-status-values","title":"Health Status Values","text":"<p>The health status can be one of the following:</p> <ul> <li>Up: The external service is reachable and responding correctly to health checks</li> <li>Down: The external service is not responding or is returning errors</li> <li>Unknown: A health check has never been performed. This can occur when:<ul> <li>A health check endpoint has not been configured for the custom action</li> <li>The custom action was recently created and the automatic health check task has not yet run</li> </ul> </li> </ul> <p>Troubleshooting</p> <p>If a custom action shows as \"Down,\" verify that:</p> <ul> <li>The base URL is correct and accessible</li> <li>Your authentication provider credentials are valid</li> <li>The external service is running and accepting connections</li> <li>Any firewall or network rules allow connections from Open Chat Studio</li> </ul>"},{"location":"concepts/events/","title":"Events","text":"<p>Open Chat Studio provides an event system that allows you to define actions triggered by specific events within a chat session. This functionality enables you to automate responses, manage session states, and enhance user interactions effectively.</p>"},{"location":"concepts/events/#overview","title":"Overview","text":"<p>Events in Open Chat Studio are categorized into two types:</p> <ol> <li>Static Events: Triggered by specific actions or occurrences within the chat session.</li> <li>Timeout Events: Triggered after a specified duration of inactivity following the last interaction.</li> </ol> <p>Each event has one action associated with it that is executed when the event occurs.</p>"},{"location":"concepts/events/#static-events","title":"Static Events","text":"<p>Static events are predefined triggers that occur based on specific actions or conditions within the chat session. The available static events are:</p> <ul> <li> <p>Conversation End: A catch-all trigger that fires whenever any conversation ends, regardless of how it ended. This trigger is always fired alongside any of the specific conversation end sub-triggers listed below. Use this trigger when you want to perform an action for all conversation endings, or use the sub-triggers when you need to respond to specific end conditions.</p> <ul> <li>The Conversation is Ended by the Participant: Triggered when the participant explicitly ends the conversation.</li> <li>The Conversation is Ended by the Bot: Triggered when the bot ends the conversation.</li> <li>The Conversation is Ended via the API: Triggered when the conversation ends via an API call.</li> <li>The Conversation is Ended by an Event: Triggered when the conversation ends due to an event.</li> <li>The Conversation is manually ended by an Admin: Triggered when an admin manually ends the conversation.</li> </ul> <p>How Sub-triggers Work</p> <p>When a specific end condition occurs (e.g., participant ends conversation), both the specific sub-trigger AND the generic \"Conversation End\" trigger will fire. This allows you to create both targeted events (using sub-triggers) and catch-all events (using the generic trigger) that respond to any conversation ending.</p> </li> <li> <p>Last Timeout: Triggered when the last timeout of any configured timeout events occur.</p> </li> <li>Human Safety Layer Triggered: Triggered when the safety layer is activated by a message from the user.</li> <li>Bot Safety Layer Triggered: Triggered when the safety layer is activated by a response from the bot.</li> <li>Conversation Start: Triggered when a new conversation is started.</li> <li>New Human Message: Triggered when a new human message is received.</li> <li>New Bot Message: Triggered when a new bot message is received.</li> <li>Participant Joined Experiment: Triggered when a participant starts interacting with the bot for the very first time.</li> </ul>"},{"location":"concepts/events/#event-actions","title":"Event Actions","text":"<p>Each event is associated with one action. The available actions are:</p> <ul> <li>End the conversation: Ends the conversation with the user. See Resetting Sessions.</li> <li>Prompt the bot to message the user: Prompts the bot to message the user.</li> <li>Trigger a schedule: This will create a once off or recurring schedule. Each time the schedule is triggered, the bot will be prompted to message the user.</li> <li>Start a pipeline: This will run the given pipeline when the event triggers. The input to the pipeline can be configured.</li> </ul>"},{"location":"concepts/llm/","title":"Large Language Models (LLMs)","text":"<p>Definition</p> <p>A Large Language Model (or LLM) is a type of artificial intelligence software that is trained on a vast amount of text data. Its primary function is to understand, interpret, and generate human language. This training allows it to produce text-based responses, answer questions, translate between languages, and perform various other language-related tasks. </p> <p>The term \"large\" in its name refers to the extensive volume of data it has been trained on and the complexity of its design, enabling it to handle complex language tasks.</p> <p>The definition above was authored by the famous LLM that powers ChatGPT: GPT-4 developed by OpenAI.</p> <p>When building chatbots, an LLM powers the chatbot's ability to understand and respond to user inputs, effectively acting as the brain behind the chatbot.</p>"},{"location":"concepts/llm/#which-large-language-models-are-supported-by-open-chat-studio","title":"Which Large Language Models are supported by Open Chat Studio?","text":"<p>Open Chat Studio is developed to support a range of LLMs. The platform is designed to be flexible and can work with any LLM that has an API. The platform currently supports all the models provided by the following APIs:</p> <ul> <li>OpenAI</li> <li>Azure OpenAi</li> <li>Anthropic</li> <li>Groq</li> <li>Perplexity</li> <li>Deepseek</li> </ul>"},{"location":"concepts/llm/#model-configuration-parameters","title":"Model Configuration Parameters","text":""},{"location":"concepts/llm/#temperature","title":"Temperature","text":"<p>Temperature controls the creativity or randomness of the chatbot's responses.</p> <ul> <li>A low temperature (e.g., 0.1) makes the chatbot more deterministic, providing straightforward and predictable answers.</li> <li>A high temperature (e.g., 0.9) makes responses more creative, varied, or even surprising.</li> </ul>"},{"location":"concepts/llm/#example","title":"Example:","text":"<ul> <li>Low temperature: What's a dog? \u2192 A dog is a domesticated animal.</li> <li>High temperature: What's a dog? \u2192 A dog is a loyal companion, a furry friend who fills your life with wagging tails and boundless joy.</li> </ul> <p>The default temperature of 0.7 is a balanced choice designed to provide responses that are both varied and  interesting, while still being coherent.</p>"},{"location":"concepts/llm/#prompt","title":"Prompt","text":"<p>A prompt is the input or instructions given to the LLM to guide its response. It sets the context for the chatbot. Prompts can be as simple as a user question or as detailed as a conversation framework or role-play setup.</p>"},{"location":"concepts/llm/#example_1","title":"Example:","text":"<p>You are a helpful assistant. Answer questions clearly and concisely.</p>"},{"location":"concepts/llm/#tokens","title":"Tokens","text":"<p>Tokens are the building blocks of text that the LLM processes. A token might be a word, a part of a word, or even just punctuation.</p>"},{"location":"concepts/llm/#example_2","title":"Example:","text":"<p>The sentence \"Chatbots are cool.\" is broken into 4 tokens: <code>Chatbots | are | cool | .</code></p> <p>Tokens are important because they determine the cost and the processing complexity of an LLM's response.</p>"},{"location":"concepts/llm/#max-token-limit","title":"Max Token Limit","text":"<p>The max token limit is the maximum number of tokens the LLM can handle in a single interaction, including both the input (prompt) and output (response).</p>"},{"location":"concepts/llm/#example_3","title":"Example:","text":"<p>If the max token limit is 4096 tokens: - A long prompt with 2000 tokens leaves 2096 tokens available for the response.</p> <p>Understanding the token limit helps you create effective prompts without truncating responses.</p>"},{"location":"concepts/participant_data/","title":"Participant Data","text":"<p>Participant data is the information collected from participants during their interactions with the system.</p> <p>Participant data is unique to each combination of channel platform, channel identifier, and chatbot. This means that the data for each bot may be different. For example, if the same person interacts with two different bots over the same  channel, e.g. WhatsApp, the data for each bot will be different. Furthermore, if the same person interacts with the same bot over two different channels, e.g. WhatsApp and Telegram, the data for each channel will be different.</p> <p>The reason for this is to ensure that the data is only accessible to the bot that it is intended for. There is no way for the system to know whether the same person is interacting with the bot on different channels.</p> <p>Participant data can be viewed and edited on the \"Participant Details\" page. This page can be accessed by clicking on the participant's name in the list of participants on the \"Participants\" list page.</p> <p>You can also export and import participant data from the \"Participants\" list page.</p>"},{"location":"concepts/participant_data/#using-participant-data","title":"Using participant data","text":""},{"location":"concepts/participant_data/#prompt-variable","title":"Prompt Variable","text":"<p>You can access the participant data using the <code>{participant_data}</code> prompt variable. This variable is a JSON object that contains the data for the participant. You can use this data to personalize the responses from the bot. For example, you can use the participant's name to personalize the greeting. For more information on prompt variables see here.</p>"},{"location":"concepts/participant_data/#pipeline-nodes","title":"Pipeline Nodes","text":"<p>Other than using the prompt variable described above, there are also various pipeline nodes which allow you to access the participant data:</p> <ul> <li>Python node: This node allows you to access the participant data using Python code.</li> </ul> <p>For more information, see the node documentation.</p>"},{"location":"concepts/participant_data/#system-properties","title":"System properties","text":"<p>There is only one system property that is set automatically by the system and only if the user interacts with a bot via the web channel. This is the <code>timezone</code> property. It is set to the timezone of the participant's browser. This is useful for localizing datetime variables in prompts.</p>"},{"location":"concepts/participant_data/#updating-participant-data","title":"Updating participant data","text":"<p>You can manually update the participant data using the Web UI. Participant data can also be updated dynamically using the methods described below.</p>"},{"location":"concepts/participant_data/#tools","title":"Tools","text":"<p>Open Chat Studio provides some tools that allow bots to update the participant data. These tools are available in the \"Tools\" tab of the chatbot edit page.</p> <p>These tools allow the bot to update the data in real time as the user is interacting with the bot.</p>"},{"location":"concepts/participant_data/#pipelines-nodes","title":"Pipelines Nodes","text":"<p>Both the \"Update Participant Data Node\" and the \"Python Node\" can be used to make updates to participant data. The \"Update participant data\" node is primarily used in conjunction with events (see below). The \"Python Node\" can be used to update the data using Python code as part of any pipeline.</p> <p>For more information, see the node documentation.</p>"},{"location":"concepts/participant_data/#events","title":"Events","text":"<p>You can also update the participant data using events. This is useful if you want to update the data based on the context of the conversation. This method also allows you to specify the schema for the data that is being updated.</p> <p>An example of this is extracting tasks from the conversation history using a timeout event. The event could be configured to run 15 minutes after the last message was sent. The event would execute a pipeline which would extract the tasks from the conversation history and update the participant data using the appropriate pipeline node. In this example the following schema could be used:</p> <pre><code>{\n  \"tasks\": [\n    {\n      \"name\": \"name of the task\",\n      \"due_date\": \"due date of the task in the format YYYY-MM-DD\"\n    }\n  ]\n}\n</code></pre>"},{"location":"concepts/participant_data/#api","title":"API","text":"<p>You can also update the participant data using the API. This is useful if you want full control over the data or when you want to update the data based from an external system. You can use the following endpoint to update the participant data:</p> <p><code>POST /api/participants/</code></p> <pre><code>{\n  \"platform\": \"Name of the channel platform e.g. WhatsApp, Telegram etc.\",\n  \"identifier\": \"ID of the participant on the specified platform\",\n  \"name\": \"Optional name for the participant\",\n  \"data\": [\n    {\n      \"experiment\": \"ID of the experiment the data is for\",\n      \"data\": {\n        \"key\": \"value\"\n      }\n    }\n  ]\n}\n</code></pre> <p>See the API docs for more information on the API.</p>"},{"location":"concepts/prompt_variables/","title":"Prompt variables","text":"<p>Prompt variables are a great way to make your prompt dynamic or tailored to the participant by injecting data into specified placeholders. These variables are predefined and look like this:</p> <pre><code>{variable}\n</code></pre> <p>The following variables are currently supported:</p> <ul> <li><code>{source_material}</code> - The source material linked to your bot.</li> <li><code>{participant_data}</code> - Information specific to this participant, bot and channel. See here for more information. </li> <li><code>{current_datetime}</code> - This refers to the date and time at which the response is generated.</li> <li><code>{media}</code> - (pipelines only) This refers to the linked media collection.</li> <li><code>{temp_state}</code> - (pipelines only) Access to the pipeline temporary state. See Temporary State.</li> <li><code>{session_state}</code> - (pipelines only) Access to the session state. See Session State</li> </ul> <p>Localizing injected datetime</p> <p>The injected datetime will be localized to the participant's timezone if it exists in their participant data. When a participant uses the web UI, their browser's timezone will automatically be saved to their participant data.</p> <p>A note on prompt caching</p> <p>Some LLM providers, like OpenAI, use a technique called \"prompt caching\" to reduce latency and costs (See here). This happens automatically. However, caching is only effective for static data, i.e. data that does not change. To take full advantage of this caching mechanism, you should place prompt variables near the end of your prompt whenever possible</p>"},{"location":"concepts/prompt_variables/#nested-data-access","title":"Nested data access","text":"<p>The following prompt variables allow referencing specific parts of the data:</p> <ul> <li><code>{participant_data}</code></li> <li><code>{temp_state}</code></li> <li><code>{session_state}</code></li> </ul> <p>Subsets of the data can be accessed using dot notation. For example, if you have a participant data object that looks like this:</p> <pre><code>{\n  \"name\": \"John Doe\",\n  \"address\": {\n    \"street\": \"123 Main St\"\n  },\n  \"tasks\": [\n    {\n      \"name\": \"Fix the roof\"\n    }\n  ]\n}\n</code></pre> <p>You can access specific parts of the data using the following prompt variables:</p> <pre><code>{participant_data.name}\n{participant_data.address.street}\n{participant_data.tasks[0].name}  # lists are zero-indexed\n</code></pre>"},{"location":"concepts/prompt_variables/#accessing-remote-context","title":"Accessing Remote Context","text":"<p>When using API integrations that pass context with messages, you can access this contextual information in your prompts using the session state variable:</p> <pre><code>{session_state.remote_context}\n</code></pre> <p>For example, if an API client sends a message with context like this:</p> <pre><code>{\n  \"page_url\": \"https://example.com/products/widget-x\",\n  \"product_id\": \"widget-x\",\n  \"category\": \"gadgets\",\n  \"user_segment\": \"premium\"\n}\n</code></pre> <p>You can access specific values in your prompts:</p> <pre><code>Current page: {session_state.remote_context.page_url}\nProduct ID: {session_state.remote_context.product_id}\nUser segment: {session_state.remote_context.user_segment}\n</code></pre> <p>Remote Context Availability</p> <p>The <code>remote_context</code> key in session state is only populated when explicitly provided via the API when sending messages. If no context is passed, this key will not be present in the session state. See the Chat API documentation for details on passing context with messages.</p>"},{"location":"concepts/sessions/","title":"Chat Sessions","text":""},{"location":"concepts/sessions/#overview","title":"Overview","text":"<p>Chat sessions in Open Chat Studio define the scope of conversations between a user and a chatbot within a specific channel. Sessions are isolated, ensuring data privacy and contextual continuity for the duration of an interaction.</p>"},{"location":"concepts/sessions/#session-scope","title":"Session Scope","text":"<p>A session is uniquely defined by:</p> <ul> <li>User: The individual engaging with the chatbot.</li> <li>Channel: The platform through which the chat occurs (e.g., WhatsApp, Telegram, Web, Slack). See channels.</li> <li>Chatbot: The specific chatbot handling the conversation.</li> </ul> <p>Each session is independent, meaning:</p> <ul> <li>The session's data is bound to that session only and is not shared with other sessions.</li> <li>When a user interacts with a chatbot, the bot receives the session's history to maintain context.</li> <li>Multi-Session Channels: Channels such as Web, API, and Slack allow multiple active sessions per user, enabling parallel conversations.</li> <li>Single-Session Channels: Platforms like WhatsApp, Telegram, and SureAdhere support only one active session per user at a time.</li> </ul>"},{"location":"concepts/sessions/#history-management","title":"History Management","text":"<ul> <li>As conversations progress, all previous messages within a session are stored as <code>history</code>.</li> <li>If the session history exceeds a predefined maximum length, it is summarized, and the bot will only receive:</li> <li>A summary of older interactions.</li> <li>The most recent exchanges to maintain context.</li> </ul>"},{"location":"concepts/sessions/#participant-data","title":"Participant Data","text":"<p>Aside from session-specific data, Open Chat Studio maintains participant data, which:</p> <ul> <li>Persists across sessions.</li> <li>Is tied to the same <code>User, Channel, Chatbot</code> scope.</li> <li>Helps retain long-term user preferences and contextual information beyond a single session.</li> </ul>"},{"location":"concepts/sessions/#anonymous-sessions","title":"Anonymous Sessions","text":"<p>On the Web channel, users can have anonymous sessions, where:</p> <ul> <li>Participant data is only available for the duration of the session.</li> <li>Since user identity cannot be verified, data cannot persist beyond the session.</li> </ul>"},{"location":"concepts/sessions/#resetting-sessions","title":"Resetting Sessions","text":"<p>For Single-Session Channels like WhatsApp and Telegram, the current session continues indefinitely. However, sessions can be reset either manually by the user or automatically using Events or the API. When a session is reset:</p> <ul> <li>The current session is marked as completed.</li> <li>A new session is started with a fresh history.</li> </ul> <p>This means that, aside from participant data, the bot loses all information about the previous conversation \u2014 including the fact that it even took place.</p>"},{"location":"concepts/sessions/#manual-resets","title":"Manual resets","text":"<p>Users can manually reset a session (start a new session) in two ways:</p>"},{"location":"concepts/sessions/#using-the-reset-command","title":"Using the <code>/reset</code> command","text":"<p>The <code>/reset</code> command allows chat users to start a new session by sending this text command in the chat. This command is available on all channels except Web and Slack.</p>"},{"location":"concepts/sessions/#using-the-session-reset-button","title":"Using the session reset button","text":"<p>In the web interface, users have access to a button that allows them to end the current session and immediately start a new one. When using this button, users can:</p> <ul> <li>Choose whether to trigger end conversation events: Users can opt to run any configured end conversation events before starting the new session, or skip them.</li> <li>Provide an initial bot message: Users must specify a prompt that will be used as the bot's first message in the new session. If the chatbot has a seed message configured, this field will be pre-filled with that seed message, but users can modify it as needed.</li> </ul> <p>This button provides more control over the session reset process compared to the <code>/reset</code> command, allowing users to customize how the transition between sessions occurs.</p>"},{"location":"concepts/sessions/#automatic-resets","title":"Automatic resets","text":"<p>There are two ways to automatically reset a session:</p> <ul> <li>Events: You can configure an event to end the current session when the event is triggered. This will not automatically create a new session; however, if the user sends a message after the session is ended, a new session will be created. See Events.</li> <li>API: When using the Trigger Bot Message API, you can set <code>\"start_new_session\": true</code>, which will end the current session and start a new one before messaging the user.</li> </ul> <p>By structuring sessions in this way, Open Chat Studio ensures privacy-conscious, context-aware, and seamless interactions across different communication channels.</p>"},{"location":"concepts/source_material/","title":"Source material","text":"<p>Source Material is a feature that allows you to provide specific content, information, or data which the chatbot can use as a reference or knowledge base. This is particularly useful for LLM-based chatbots, as it helps tailor the chatbot\u2019s responses to be more aligned with your specific needs or the theme of the chatbot.</p>"},{"location":"concepts/source_material/#how-to-use-source-material","title":"How to use source material","text":"<ul> <li> <p>Content Integration: You can integrate specific documents or text into the chatbot. This could be information on a given health area in an FAQs format, program-specific data, or any other relevant content that you want your chatbot to reference. </p> </li> <li> <p>Contextual Relevance: By providing this material, you're essentially giving the chatbot a more focused and relevant context to operate within. This means your chatbot can provide more accurate and tailored responses based on the Source Materials you've provided.</p> </li> <li> <p>Updating Information: Keep your Source Materials up-to-date. As you update program materials (for example guidelines for counselling or home visits), updating your Source Materials will help ensure your chatbot remains relevant and effective. </p> </li> </ul>"},{"location":"concepts/source_material/#best-practices","title":"Best Practices","text":"<ul> <li> <p>Relevance and Accuracy: Ensure the materials are relevant to the conversations your chatbot will engage in. </p> </li> <li> <p>Organization and Structure: Well-organized Source Materials make it easier for the chatbot to retrieve and use the information. It's useful to structure your content in a clear, concise manner, with appropriate labels for different sections. </p> </li> </ul>"},{"location":"concepts/source_material/#see-also","title":"See also","text":"<ul> <li>Prompt variables</li> <li>Add a_knowledge base</li> </ul>"},{"location":"concepts/tags/","title":"Tags","text":"<p>Tags are labels applied to chats and messages to categorize and organize interactions. For instance, tags can be used to mark messages that require follow-up or to segment users based on their interactions.</p> <p>Tags can be created using the \u201cManage Tags\u201d section.</p> <p></p>"},{"location":"concepts/tags/#types-of-tags","title":"Types Of Tags","text":"<p>There are 3 types of tags.</p> <ul> <li> <p>System tags These are tags generated by the system, such as those used in multi-prompt architectures to differentiate between parent and child bots.</p> </li> <li> <p>Session tags These tags are manually added to sessions.</p> </li> <li> <p>Message tags These tags are manually added to specific messages within a user session.</p> </li> </ul> <p></p>"},{"location":"concepts/versioning/","title":"Versioning","text":"<p>Versioning is now enabled by default for all projects on Open Chat Studio. This comes with a few important changes that modify the default behavior of the platform.</p>"},{"location":"concepts/versioning/#terms","title":"Terms","text":"<p>OCS uses the following terms:</p> <ul> <li> <p>Unreleased Version. This is the version of the chatbot that exists when you click the edit button on the experiment. It can also be considered a \"draft\" or that it has \"unsaved changes\".</p> </li> <li> <p>Published Version. This is the version that users will interact with through the web, WhatsApp or any other configured channel--including the public link.</p> </li> </ul> <p>A note on version functionality</p> <p>Once a version is made, it cannot be edited or modified. This ensures that the users' experience remains stable even if the authors may be changing the unreleased version.</p> <p>Chatting to the unreleased version</p> <p>To chat to the unreleased version, navigate to the Experiment home page and click on the speech bubble icon at the top right corner of the page. There, a drop down will say either \"Unreleased Version\" or \"Published Version\". Select the Unreleased Version, and that will open a web chat. Only bot authors can chat with the unreleased version as it is not available through channels.This is a change in the default behavior of the platform as prior to versioning, all channels chatted to the unreleased version at all times.</p>"},{"location":"concepts/versioning/#changing-the-published-version","title":"Changing the Published Version","text":"<p>The published version can be selected from any released version of the experiment. To modify which version is the published version:</p> <ul> <li>Select \"View Details\" of the version</li> <li>Press the \"Set as Published Version\" button at the button of the dialog box.</li> </ul> <p>Alternatively, when a new version is being created, it can be set as the published version by marking the checkbox \"Set as Published Version\".</p> <p>Only one version can be the published version at a time.</p>"},{"location":"concepts/versioning/#workflow","title":"Workflow","text":"<p>When a new experiment is first created, there exists two versions, a published version and a unreleased version as shown in the version table: </p> <p>Then, when you would like to create another version after making changes to the unreleased version, you can either press the create version button on the table, or navigate to the edit experiment page and scroll to the bottom to locate the create version button. Note: this button will only be enabled if changes have been made to the version. </p> <p>That will take you the the create new version page which will show you the difference between the previous version (note not the published version) and the unreleased version. Here, you can also set this newly created version as the published version. Also, there is an option to add a description to the version that will be shown in the version table to quickly remember the changes between versions. </p> <p>Tada! There you have a new released version! You will be directed back the experiment verisons table where it may take a few minutes for the version to be fully available. Then you can chat with the version and view its details. When you select view details it shows the the detailed specifications of that version and if you navigate to the bottom, you are able to set as the published version and archive from that screen. </p> <p>If you click on the webchat button, for an unpublished version, there will be a banner indication that it's the unpublished version, and which version it is: </p> <p>For this demo, I released a few more versions for this experiment and also changed the published version. To easily see which is the published version for the experiment, look right of the experiment name at the top of the experiment home screen at the icon in green. For this example, you'll see \"v2\" which indicates that the version 2 is the published version. You will also be able to see in the table looking at the published version row for the checkmark. </p> <p>Versioning experiments that use OpenAI Assistants</p> <p>Can this be done? Yes! When an experiment is released that has an OpenAI Assistant, there is no additional configuration required. However, please note that a read-only copy of the OpenAI Assistant is made in Open Chat Studio (see in the Assistants tab) and also in OpenAI. This includes all reference files. The existing OpenAI Assistant prior to creating the the version will still be available and be able to be modified in the unreleased experiment version.</p> <p>Modifying Assistants in OpenAI referenced by released versions</p> <p>As mentioned above, the copied assistant will be read-only in OCS, however, in OpenAI changes can still be made to that copy of the assistant. We recommend advising your team to not modify this assistant if it references a released version. This can cause unexpected behavior to the version and to its end users. To ensure that the released version acts as expected this assistant should remain as-is.</p>"},{"location":"concepts/chatbots/","title":"Chatbots","text":"<p>Info</p>"},{"location":"concepts/chatbots/#announcement-chatbots-feature-rolling-out","title":"Announcement: Chatbots Feature Rolling Out","text":"<p>We are excited to announce that we are releasing a new way to build chatbots in Open Chat Studio. You may already be using 'Pipelines' to build 'Experiments'. We are now transitioning to the term 'Chatbots' and away from 'Experiments'. This change is part of our ongoing effort to improve the user experience and make it easier for you to create and manage your chatbots. The term 'Experiment' will be phased out as we fully adopt 'Chatbots' instead. Bot building will shift from the current 'form-based' method to primarily using the pipeline approach. All existing experiments will continue to work without disruption during this rollout.</p> <p>Some key improvements include:</p> <ul> <li> <p>Simplified Workflow: Chatbots introduces a cleaner, more intuitive interface for building chatbots</p> </li> <li> <p>Streamlined Bot Building: We're transitioning from the 'form-based' approach to make pipelines the primary (and eventually only) method for bot building</p> </li> <li> <p>Enhanced Features: The new system allows building more bots with greater flexibility and complexity.</p> </li> <li> <p>Enhanced Features: Chatbots includes features that are not available to legacy 'Experiments' including:</p> <ul> <li>LLM tools such as 'web search', 'code interpreter', and 'file search'</li> </ul> </li> </ul> <p>Self-Managed Rollout: Team administrators can now control feature flags for their teams, allowing for more granular control over when new features are enabled. This means teams can adopt the Chatbots feature at their own pace rather than following a global rollout schedule.</p> <p>For more information check out our FAQ page</p>"},{"location":"concepts/chatbots/#what-is-a-chatbot","title":"What is a Chatbot?","text":"<p>A chatbot is a program that simulates conversation with people. It can use simple rules or advanced AI to understand and respond to messages. </p> <p>Within the context of Open Chat Studio, a chatbot is a specific configuration of a language model (LLM) that is designed to interact with users conversationally. It can be customized to perform various tasks, such as answering questions, providing information, or assisting with specific workflows.</p> <p>The specific way that a chatbot is configured with Open Chat Studio is through the use of a Pipeline. You can think of a pipeline as a flowchart that starts with user input on one end and ends with the chatbot\u2019s response at the other. Each message sent to the bot follows a specific path through the pipeline to generate the final output. </p> <p>This approach can be useful if you want to build a complex bot that performs different tasks depending on the user\u2019s request. Generally, trying to make a single bot prompt do multiple functions doesn\u2019t work well, so it is better to create multiple prompts for each task and then combine them using a Pipeline. </p>"},{"location":"concepts/chatbots/#what-is-a-pipeline","title":"What is a Pipeline?","text":"<p>As described above, a pipeline is a flowchart-like structure that defines how a chatbot processes user input and generates responses. It consists of a series of connected nodes, each representing a specific step in the workflow. Each node can perform a specific function, such as processing user input, generating responses, or integrating with external systems.</p> <p>The simplest pipeline is a single node that takes user input and generates a response. More complex pipelines can include multiple nodes that work together to create a more sophisticated interaction.</p> <p>You can read more about pipelines in the Pipelines documentation.</p>"},{"location":"concepts/chatbots/rollout_faq/","title":"Frequently Asked Questions","text":""},{"location":"concepts/chatbots/rollout_faq/#timeline","title":"Timeline","text":"<p>Now - September 10th: Team admins can enable the Chatbot Feature Flag for their team via the feature flag management page.</p> <p>September 10th: Chatbot Feature Flag is enabled for all teams</p> <p>October 1st: All legacy experiments are migrated to chatbots. Pipeline experiments are filtered out from the pipeline table.</p>"},{"location":"concepts/chatbots/rollout_faq/#are-my-existing-experiments-going-to-be-lost-after-the-upgrade-from-experiments-to-chatbots","title":"Are my existing Experiments going to be lost after the upgrade from Experiments to Chatbots?","text":"<p>No! This update is solely in the bot building experience and not in the end product the user sees. All existing experiments will be seamlessly transferred over to Chatbots without change in any of the chatbot functionality.</p>"},{"location":"concepts/chatbots/rollout_faq/#what-do-i-have-to-do-during-the-experiments-to-chatbots-transition","title":"What do I have to do during the Experiments to Chatbots transition?","text":"<p>Nothing! All of your experiments will be transferred over to use chatbots automatically. You will receive updates on the progress of the transition via banners on the site.</p>"},{"location":"concepts/chatbots/rollout_faq/#how-do-i-adjust-the-global-settings-of-my-chatbot","title":"How do I adjust the global settings of my chatbot?","text":"<p>Different from an Experiment, the settings have been moved to a tab on the Chatbot homepage.</p> The settings tab <p>Here, you are able to modify name, description, voice, tracing, consent, surveys, participant allowlist, and seed message.</p>"},{"location":"concepts/chatbots/rollout_faq/#how-can-i-control-feature-rollouts-for-my-team","title":"How can I control feature rollouts for my team?","text":"<p>Team administrators can manage the chatbot feature for their team.</p> <p>To access feature flag management:</p> <ol> <li>Navigate to your team's settings page</li> <li>Click on the \"Manage Feature Flags\" button</li> </ol> Feature Flag Management Feature Flag Management Page"},{"location":"concepts/collections/","title":"Collections","text":"<p>Collections are only supported with pipeline bots</p> <p>A collection in OCS refers to a collection of files. There are two types of collections:</p> <ul> <li>Media collection</li> <li>Indexed Collection (for RAG applications)</li> </ul>"},{"location":"concepts/collections/#adding-a-collection-to-a-bot","title":"Adding a collection to a bot","text":"<p>Navigate to the Collections section in the sidebar and click \"Add new\". Once the collection is created, you will be able to upload files to it.</p> <p>After your collection has been created and populated with files, you can link it to any LLM node.</p>"},{"location":"concepts/collections/#how-are-attachments-sent","title":"How are attachments sent?","text":"<p>Whenever your bot references a particular file or document, it will be sent to the user as an attachment. Depending on the channel, attachments are delivered in different ways.</p>"},{"location":"concepts/collections/#web-channels","title":"Web channels","text":"<p>Attachments are directly downloadable by clicking on them.</p>"},{"location":"concepts/collections/#multimedia-unsupported-channels","title":"Multimedia-unsupported channels","text":"<p>By default, attachments are sent as download links appended to the bot's message. The user will see the file name and a corresponding download link at the end of the message. These channels do not yet support sending multimedia files:</p> <ul> <li>Telegram</li> <li>WhatsApp (Turn.io provider)</li> <li>SureAdhere</li> <li>Facebook Messenger</li> <li>Slack</li> </ul>"},{"location":"concepts/collections/#multimedia-supported-channels","title":"Multimedia-supported channels","text":"<p>Channels that support sending multimedia files will receive each attachment as a separate message, following the bot's initial response. If a file type is not supported by the channel, or the file size exceeds the allowed limit, a download link will be appended to the bot's message instead. These channels support sending multimedia files:</p> <ul> <li>API - See the API documentation for more information</li> <li>WhatsApp (Twilio Provider) - Consult the Twilio docs for supported file types.</li> </ul>"},{"location":"concepts/collections/indexed/","title":"Indexed Collection (for RAG applications)","text":""},{"location":"concepts/collections/indexed/#when-should-i-use-this","title":"When should I use this?","text":"<p>When you want your bot's responses to be grounded in your uploaded documents.</p> <p>Definition</p> <p>Retrieval-Augmented Generation (RAG) is a technique where a language model retrieves relevant information from a set of documents to ground its answers in real data. Instead of relying solely on its built-in knowledge, the model uses indexes\u2014specialized databases that store document content as vectors (numerical representations of meaning). This makes it easy for the model to find and use the most relevant parts of your uploaded files when answering questions.</p> <p>Indexed collections will replace OpenAI Assistants' file search functionality in the future</p> <p>Consult the migration guide if you have assistants that you want to replace with indexed collections.</p> <p>If you\u2019ve used the OpenAI Assistants\u2019 file search capability in OCS, you\u2019ve already interacted with an index behind the scenes.</p> <p>In OCS, there are two types of indexes:</p> <ul> <li>Remote Index</li> <li>Local Index</li> </ul>"},{"location":"concepts/collections/indexed/#remote-index","title":"Remote Index","text":"<p>Remote indexes are hosted and managed by an LLM provider. Files and index configuration are uploaded to the provider, which maintains and manages the index. The embedding model used to create file embeddings is selected by the provider.</p>"},{"location":"concepts/collections/indexed/#supported-providers","title":"Supported providers","text":"<ul> <li>OpenAI (using the responses API)</li> </ul> <p>OpenAI Remote Collection Limit</p> <p>When using OpenAI as your LLM provider with remote (OpenAI-hosted) indexed collections, you can select a maximum of 2 collections per LLM node. This is a limitation imposed by OpenAI's API, not Open Chat Studio.</p> <ul> <li>Local indexes are NOT affected by this limit</li> <li>Non-OpenAI providers are NOT affected by this limit</li> <li>If you attempt to select more than 2 remote collections with OpenAI, you will receive a validation error</li> </ul> <p>If you need to use more than 2 collections, consider using local indexes instead.</p>"},{"location":"concepts/collections/indexed/#supported-file-types","title":"Supported file types","text":"<p>Supported files are determined by the selected provider:</p> <ul> <li>OpenAI - See the OpenAI docs</li> </ul>"},{"location":"concepts/collections/indexed/#local-index","title":"Local Index","text":"<p>Local indexes are a new feature in OCS. We are actively working to support additional file types and embedding models, allowing you to better customize your index with models that suit your needs.</p> <p>Local indexes are hosted and managed by OCS. When you create a local index, you can select which embedding model to use for generating file embeddings. Embedding models are provided by LLM providers. Just as different LLM models have varying strengths and weaknesses, different embedding models also have their own strengths and weaknesses. Thus, choosing the right embedding model is important to ensure the best performance for your specific use case.</p>"},{"location":"concepts/collections/indexed/#supported-providers_1","title":"Supported providers","text":"<ul> <li>OpenAI</li> </ul>"},{"location":"concepts/collections/indexed/#supported-file-types_1","title":"Supported file types","text":"<ul> <li>pdf</li> <li>txt</li> <li>csv</li> <li>docx</li> </ul>"},{"location":"concepts/collections/indexed/#supported-embedding-models","title":"Supported embedding models","text":"<p>You can see the supported embedding models for each provider when creating or editing the provider in your team settings.</p>"},{"location":"concepts/collections/indexed/#chunking-and-optimization","title":"Chunking and Optimization","text":"<p>When you upload a document to an index, it\u2019s broken up into smaller parts called chunks. These chunks are then converted into vectors and stored in the index. Chunking is a key part of how RAG works, as it affects how accurately the model can retrieve relevant information.</p> <p>In most cases, it will not be necessary to change the default chunking strategy, but you\u2019ll have the option to customize the chunking strategy for each set of uploaded files:</p> <ul> <li>Chunk size \u2013 how large each chunk is (in tokens)</li> <li>Chunk overlap \u2013 how much each chunk overlaps with the next, to preserve context</li> </ul> <p>Choosing the right chunking strategy can improve retrieval accuracy, especially for technical documents.</p> <p>This flexibility helps tailor the index to your use case\u2014whether it\u2019s short notes or long, complex reports.</p>"},{"location":"concepts/collections/indexed/#document-sources","title":"Document Sources","text":"<p>In addition to manually uploading documents to a collection, you can also configure document sources from which Open Chat Studio will automatically load and index documents.</p> <p>The primary advantage of document sources over manual uploads is that Open Chat Studio can check for updates periodically, which eliminates the need for manual updates.</p> <p>The following document source types are currently supported:</p>"},{"location":"concepts/collections/indexed/#confluence","title":"Confluence","text":"<p>Load pages from a Confluence site. Pages can be filtered using the space key, label, CQL, or individual page IDs.</p> <p>Authentication</p> <p>Use a Basic Auth authentication provider with your Atlassian username and use your API Key as the password.</p> <p>Configuration</p> Field Description Site URL The URL of the Confluence site (e.g. https://yoursite.atlassian.net/wiki) Max Pages The maximum number of pages to load Space Key Load pages from this space Label Load pages with this label CQL CQL query to use to search for pages to load Page IDs Load only these specific pages <p>Note</p> <p>Only one of the <code>Space Key</code>, <code>Lable</code>, <code>CQL</code> and <code>Page IDs</code> fields can be used at a time.</p>"},{"location":"concepts/collections/indexed/#github","title":"GitHub","text":"<p>Load pages from a GitHub repository. Files can be filtered by path and by matching patterns against the filenames.</p> <p>Authentication</p> <p>Use a Bearer Token authentication provider.</p> <p>Configuration</p> Field Description Repository URL GitHub repository URL (e.g. https://github.com/user/repo) Branch Git branch to sync from File Pattern File patterns to include. Prefix with '!' to exclude matching files. Path Filter Optional path prefix to filter files (e.g., docs/)"},{"location":"concepts/collections/media/","title":"Media collections","text":""},{"location":"concepts/collections/media/#when-should-i-use-this","title":"When should I use this?","text":"<p>When you want your bot to be able to send multimedia files to users.</p>"},{"location":"concepts/collections/media/#currently-supported-file-types","title":"Currently supported file types","text":"<p>Documents .txt, .pdf, .doc, .docx, .xls, .xlsx, .csv</p> <p>Images .jpg, .jpeg, .png, .gif, .bmp, .webp, .svg</p> <p>Video .mp4, .mov, .avi</p> <p>Audio .mp3, .wav</p> <p>Once a collection is linked, your bot will be able to send one or more files from it to users\u2014either as a download link or directly\u2014depending on the specific channel\u2019s support for the file type and file size.</p>"},{"location":"concepts/collections/media/#how-does-it-work","title":"How does it work?","text":""},{"location":"concepts/collections/media/#how-does-the-bot-know-when-to-attach-a-file","title":"How does the bot know when to attach a file?","text":"<p>When you create a collection and upload files to it, you'll be prompted to add a summary for each file. These summaries are included in the system prompt when you link a collection to your bot. This allows the bot to accurately determine when a particular file is relevant to a conversation.</p> <p>Additionally, OCS automatically provides the bot with a tool that enables it to attach files to its responses.</p> <p>The location in the prompt where these summaries are included is defined by the {media} prompt variable.</p> <p>Here\u2019s an example of how file details appear in the system prompt: <pre><code>You are a friendly assistant. Here's some files that you can attach to your responses when you think the user will benefit from it:\n{media}\n</code></pre> becomes</p> <pre><code>You are a friendly assitant. Here's some files that you can attach to your responses when you think the user will benefit from it:\n* File (id=22, content_type=image/png): This is an image of a border collie\n* File (id=23, content_type=application/pdf): This file contains information about the behaviours of border collies\n</code></pre>"},{"location":"concepts/evaluations/","title":"Evaluations","text":"<p>Evaluations is a testing system for measuring chatbot performance against different metrics.</p> <p>Evaluations can be run against existing conversation messages in the system or uploaded custom test datasets. Metrics can be defined either as python code, or output from an LLM.</p>"},{"location":"concepts/evaluations/#overview","title":"Overview","text":"<p>Evaluations are made up of a dataset and one or more evaluators.</p>"},{"location":"concepts/evaluations/#dataset","title":"Dataset","text":"<p>Datasets are collections of messages that serve as the foundation for running evaluations.</p> <p>Datasets can either be created directly from existing sessions, manually created in the UI, or uploaded with a CSV.</p>"},{"location":"concepts/evaluations/#evaluator","title":"Evaluator","text":"<p>Evaluators define the logic for analyzing messages and generating evaluation metrics. Each evaluator takes individual messages from a dataset and optionally a generated response, then outputs structured results in a table. You can apply many evaluators to a dataset in parallel, and the outputs of each will be added as new columns to the table.</p>"},{"location":"concepts/evaluations/#chatbot-generation","title":"Chatbot Generation","text":"<p>Messages can also optionally be passed in to a chatbot, whose generation output will be available to the evaluators.</p>"},{"location":"concepts/evaluations/#evaluation-execution","title":"Evaluation Execution","text":"<p>When an evaluation is run, each message from the dataset is first passed in to the defined chatbot (if applicaple). The result, with the added generation output is then passed in to each evaluator in parallel. The evaluators output structured data. This data is compiled into a table, whose rows are each message and the columns are the evaluator output.</p> <pre><code>flowchart LR\n    dataset([Dataset Message]) --&gt; generation{Chatbot Generation}\n    generation --&gt; evaluator1([Evaluator])\n    generation --&gt; evaluator2([Evaluator])\n    evaluator1 --&gt; structured_output([Structured Output])\n    evaluator2 --&gt; structured_output</code></pre>"},{"location":"concepts/evaluations/#session-retention","title":"Session Retention","text":"<p>When evaluations are run with chatbot generation enabled, temporary sessions are created to store the generated responses and conversation context. These evaluation sessions are automatically deleted after 30 days.</p> <p>Data Retention</p> <p>Session deletion is permanent and cannot be undone. Ensure you export any evaluation results you need to retain before the 30-day retention period expires.</p> <p>Source Sessions Unaffected</p> <p>This automatic deletion only affects sessions created during evaluation runs. Source sessions (the original sessions that datasets may be cloned from) are not affected by this retention policy and remain in the system according to their own lifecycle.</p>"},{"location":"concepts/evaluations/dataset/","title":"Evaluation Datasets","text":"<p>Datasets are collections of messages that serve as the foundation for running evaluations.</p> <p>Each dataset contains messages with the following structure:</p> <ul> <li>Input: The human message or prompt (required)</li> <li>Output: The expected AI response (required)</li> <li>Context: Additional metadata and context variables that can later be accessed in the evaluators (optional)</li> <li>History: Previous conversation messages for context (optional)</li> <li>Participant Data: Information about the participant that can be accessed during evaluation (optional)</li> <li>Session State: Session-specific state data that can be accessed during evaluation (optional)</li> </ul>"},{"location":"concepts/evaluations/dataset/#managing-datasets","title":"Managing Datasets","text":"<p>Datasets can be created by cloning an existing session, manually created in the UI, or uploaded with a CSV.</p>"},{"location":"concepts/evaluations/dataset/#cloning-a-session","title":"Cloning a session","text":"<p>When cloning a session, dataset messages are created automatically from messages from past conversations, including chat history and metadata. Selecting multiple sessions from the list will clone all the message from those sessions. Selecting \"filtered messages\" will only clone the messages that match the filter parameters. Selecting \"All messages\" will clone all the messages in that session.</p> <p>Messages that are cloned from a session will be \"connected\" to their actual message in OCS, and you will be able to follow links back to their original conversation when viewing the output of an evaluator. However, modifying or updating a cloned message will break this link.</p>"},{"location":"concepts/evaluations/dataset/#manually-creating-a-dataset","title":"Manually creating a dataset","text":"<p>Messages can be manually added in the UI. You must enter a Human Message, an AI response, and optionally the history for this message and any context.</p> <p>History must be entered as a new-line separated list of messages prepended with either <code>user:</code> or <code>assistant:</code>. For example:</p> <pre><code>user: Hello, how are you?\nassistant: I am doing well, thank you for asking. How can I help you?\nuser: Please tell me the time.\nassistant: It is currently 12:05 PM in Ankara.\n</code></pre>"},{"location":"concepts/evaluations/dataset/#sharing-dataset-messages","title":"Sharing Dataset Messages","text":"<p>When viewing dataset messages in the table, each row includes a link button that allows you to share a direct link to that specific message. When someone follows this link, the page automatically scrolls to the message and highlights it for easy identification.</p> <p>This feature is useful for collaborating with team members, referencing specific test cases in discussions, or documenting particular dataset examples. Simply click the link icon next to any message, and the URL will be copied to your clipboard with the message ID parameter included.</p>"},{"location":"concepts/evaluations/dataset/#csv-upload","title":"CSV Upload","text":"<p>A CSV can also be uploaded to populate the dataset. There should be columns for human messages, ai responses, and any context data. You can also include <code>participant_data</code> and <code>session_state</code> fields.</p> <p>An example structure for this csv might be:</p> Human Message AI Response Datetime History participant_data.name session_state.count What's the weather like? I don't have access to weather data 2024-03-15T10:30:00Z user: Helloassistant: Hi there!user: How are you?assistant: I'm doing well! John 1 Tell me a joke Why don't scientists trust atoms? Because they make up everything! 2024-03-15T10:32:00Z user: What's the weather like?assistant: I don't have access to weather data John 2 What is 2+2? 2+2 equals 4 2024-03-15T10:35:00Z Jane 1"},{"location":"concepts/evaluations/dataset/#participant-data-and-session-state","title":"Participant Data and Session State","text":"<p>When including participant_data or session_state in your CSV, you can use dot notation to specify nested keys. For example:</p> <ul> <li><code>participant_data.name</code> - Sets the participant's name</li> <li><code>participant_data.age</code> - Sets the participant's age</li> <li><code>session_state.counter</code> - Sets a session state counter</li> <li><code>context.foo</code> - Sets the <code>foo</code> field in the context</li> </ul> <p>You can also include complex JSON structures directly in these fields. For example, if you have a column named <code>participant_data.tasks</code> you can include JSON data like <code>[\"Buy socks\", \"Feed the dog\", \"Clean the car\"]</code>.</p> <p>You can also specify participant data and session state as raw JSON objects by using the keys <code>participant_data</code> and <code>session_state</code> (without dot notation).</p>"},{"location":"concepts/evaluations/dataset/#csv-history","title":"CSV History","text":"<p>When uploading a CSV, you can populate the history automatically from previous messages, or add it as a separate column.</p> <p>Adding the history automatically assumes the CSV is the transcript of a single conversation, in chronological order.</p>"},{"location":"concepts/evaluations/evaluators/","title":"Evaluators","text":"<p>Evaluators define the logic for analyzing messages and generating evaluation metrics. Each evaluator takes individual messages from a dataset and optionally a generated response, then outputs structured results in a table.</p>"},{"location":"concepts/evaluations/evaluators/#evaluator-types","title":"Evaluator Types","text":""},{"location":"concepts/evaluations/evaluators/#llm-evaluator","title":"LLM Evaluator","text":"<p>The LLM Evaluator uses language models to evaluate responses based on a custom prompt. This can be used as an LLM-as-judge to evaluate the performance of a chatbot, or to gain insight properties of both the user and assistant messages.</p> <p>Example prompt: <pre><code>Rate the helpfulness and accuracy of this response on a scale of 1-5:\n\nUser question: {input.content}\nExpected answer: {output.content}\nGenerated answer: {generated_response}\n\nConsider the conversation context: {context.topic}\n</code></pre></p> <p>Template Variables: The following variables are available to be used in the LLM prompt.</p> <ul> <li><code>{input.content}</code>: The human message content</li> <li><code>{output.content}</code>: The expected AI response content</li> <li><code>{generated_response}</code>: The generated response from your chatbot (if generation enabled)</li> <li><code>{context.[parameter]}</code>: Access any context variables, e.g., <code>{context.topic}</code></li> <li><code>{full_history}</code>: Complete conversation history as formatted text</li> </ul>"},{"location":"concepts/evaluations/evaluators/#output-schema","title":"Output Schema","text":"<p>The output schema defines the metrics that the LLM should attempt to output. Each item in the schema will become a column in the output table. You can specify the data type for each field to ensure structured, validated output.</p> <p>Available Types:</p> <ul> <li>string: Text output (default behavior)</li> <li>integer: Whole numbers (e.g., counts, ratings)</li> <li>float: Decimal numbers (e.g., confidence scores, percentages)</li> <li>choices (enum): Predefined options from a list</li> </ul> <p>The system automatically validates the LLM's output against the specified types using a dynamically generated schema. If the output doesn't match the expected format, the system will retry up to 3 times before failing, ensuring reliable structured data.</p> <p>Example Output Schema:</p> Column Name Type Description expected_helpfulness integer The helpfulness, on a scale of 1-5 of the expected assistant message actual_helpfulness integer The helpfulness, on a scale of 1-5 of the actual assistant message user_sentiment choices The sentiment of the user message (options: positive, neutral, negative) confidence_score float Confidence in the evaluation, from 0.0 to 1.0"},{"location":"concepts/evaluations/evaluators/#python-evaluator","title":"Python Evaluator","text":"<p>The Python Evaluator allows custom code execution against each message.</p> <p>The code must define a <code>main</code> function which takes the <code>input</code>, <code>output</code>, <code>full_history</code>, and <code>generated_response</code>. It should return a <code>dict</code> whose keys will become columns in the output table.</p> <p>Function Arguments:</p> Argument Type Description Example <code>input</code> dict The human message data with <code>content</code> and <code>role</code> keys <code>{'content': 'What is 2+2?', 'role': 'human'}</code> <code>output</code> dict The expected AI response with <code>content</code> and <code>role</code> keys <code>{'content': '2+2 equals 4', 'role': 'ai'}</code> <code>context</code> dict Additional metadata and variables <code>{'topic': 'math', 'difficulty': 'easy', 'user_id': '123'}</code> <code>full_history</code> str Complete conversation history <code>\"user: Hello\\nassistant: Hi there!\\nuser: What is 2+2?\"</code> <code>generated_response</code> str AI-generated response being evaluated <code>\"The answer is 4. Is there anything else I can help with?\"</code> <p>Example:</p> <pre><code>def main(input: dict, output: dict, context: dict, full_history: str, generated_response: str, **kwargs) -&gt; dict:\n    \"\"\"Evaluates response quality based on accuracy, length, and politeness.\n    \"\"\"\n\n    expected_answer = output['content'].lower()\n    actual_answer = generated_response.lower()\n    has_correct_answer = expected_answer in actual_answer\n\n    response_length = len(generated_response.split())\n    is_polite = any(word in actual_answer for word in ['please', 'thank', 'help', 'happy'])\n\n    return {\n        'correct_answer': has_correct_answer,\n        'response_length': response_length,\n        'politeness_score': 1.0 if is_polite else 0.0,\n        'topic': context.get('topic', 'unknown')\n    }\n</code></pre>"},{"location":"concepts/experiment/","title":"Experiments","text":"<p>An 'Experiment' is the current name used in Open Chat Studio to refer to a 'chatbot'. This will soon be a legacy term as we transition fully to the term 'Chatbots'.</p> <p>An Experiment links all the configuration and data for a chatbot including user sessions, data, actions etc.</p> <p>Deprecation Warning</p> <p>The term will be phased out as we fully adopt 'Chatbots' instead. Bot building will shift from the current 'form-based' method to primarily using the pipeline approach. All existing experiments will be smoothly migrated with adequate notice, and users can contact the Dimagi team for assistance during this transition with any questions.</p>"},{"location":"concepts/experiment/#experiment-types","title":"Experiment Types","text":"<p>There are three different types of chatbots that you can build in Open Chat Studio:</p> <ul> <li>Base language model</li> <li>Assistant</li> <li>Pipeline</li> </ul>"},{"location":"concepts/experiment/#base-language-model","title":"Base language model","text":"<p>This kind of bot is the most commonly used and simple to configure. It is backed the standard language model APIs such as the OpenAI chat completions API, Anthropic messages API or Google Gemini API.</p> <p>Bots configured in this way have all the basic features (memory, source material etc.) and can also use some of the advanced features like Scheduling and Reminders.</p>"},{"location":"concepts/experiment/#assistant","title":"Assistant","text":"<p>Assistant bots make use of OpenAI Assistants. The main advantage of using Assistants is that your bot gets access to the OpenAI tools:</p>"},{"location":"concepts/experiment/#code-interpreter","title":"Code Interpreter","text":"<p>This allows the bot to write and execute code to accomplish tasks.</p> <p>For more information see the OpenAI docs.</p>"},{"location":"concepts/experiment/#file-search","title":"File Search","text":"<p>Warning</p> <p>The functionality described here is planned to be replaced by Indexed Collections in the future. It\u2019s recommended to start using Indexed Collections instead to ensure forward compatibility.</p> <p>This allows the bot to search and reference information provided in uploaded files. Unless your bot needs either of these capabilities, you should use a Base Language Model type bot.</p> <p>For more information see the OpenAI docs.</p>"},{"location":"concepts/experiment/#pipeline","title":"Pipeline","text":"<p>Pipelines allow you to create more complex bots by defining a \u2018graph\u2019 (in the computer science sense) of nodes. You can think of this graph as a workflow that flows from input to output. Each message to the bot is processed by the graph to produce a final output. A single response from the chatbot will be one successful path through the graph from the input node to the output node.</p> <p>This can be useful if you want to build a complex bot that performs different tasks depending on the user\u2019s request. Generally, trying to make a single bot prompt do multiple functions doesn\u2019t work well so it is better to create multiple prompts for each task and then combine them using a Pipeline. This is similar to the Multi-bot setup but allows more flexibility and complexity.</p>"},{"location":"concepts/pipelines/","title":"Pipelines","text":"<p>A pipeline is a way to build a bot by combining one or more steps together.</p> <p>Pipelines are the future</p> <p>Pipelines are currently becoming the default way to build bots in Open Chat Studio. They are a superset of existing functionality, enabling complex safety layers, routing and conditionals. The transition is now underway, and we're providing communication as we begin phasing out other bot building approaches.</p>"},{"location":"concepts/pipelines/#overview","title":"Overview","text":"<p>Here is an example of a very simple pipeline that uses an LLM to respond to the users input. This pipeline has a single step that uses the LLM to generate a response.</p> A simple pipeline <p>Analyzing this pipeline from left to right:</p> <ul> <li>the user sends a message to the bot (this is the <code>input</code>)</li> <li>the message is then passed to the LLM which generates a response</li> <li>the response is then sent back to the user (this is the <code>output</code>)</li> </ul> <pre><code>graph LR\n  A@{ shape: stadium, label: \"Input\" } --&gt; B(LLM);\n  B --&gt; C@{ shape: stadium, label: \"Output\" };</code></pre> <p>Each time a user sends a message to the bot, the pipeline is executed and the final output is sent back to the user.</p> <p>Each 'step' in a pipeline is called a 'node' and pipelines can have multiple nodes. To learn more about the different types of nodes that can be used in a pipeline, see the node types documentation.</p>"},{"location":"concepts/pipelines/#pipeline-execution","title":"Pipeline Execution","text":"<p>Open Chat Studio  runs your application in organized steps. Think of it like a well-coordinated team where different parts of your application (pipeline nodes) communicate through shared channels (pipeline edges / connections).</p> <p>Here's how each step works:</p> <p>Plan \u2192 Execute \u2192 Update \u2192 Repeat</p> <ol> <li> <p>Plan: Decide which nodes should run next. Initially, this includes nodes that need your input data. In later steps, it includes nodes that are ready to process new information.</p> </li> <li> <p>Execute: Run all selected nodes at the same time. Each node does its work independently and can't see changes from other nodes until the next step.</p> </li> <li> <p>Update: Share the results from all nodes so they're available for the next step.</p> </li> </ol> <p>This process repeats until either all work is complete or a step limit is reached. This approach ensures your application runs efficiently while maintaining predictable behavior.</p> <p>See Parallel Pipelines for information about running nodes in parallel.</p>"},{"location":"concepts/pipelines/history/","title":"History Modes","text":"<p>There are several supported history modes for LLM-based nodes in a pipeline. Each is designed to solve a unique problem. In complex pipelines it is expected that a variety of history modes will be used across different nodes.</p> <p>Only valid for LLM nodes</p> <p>Note that <code>history</code> is only applicable to nodes that have an LLM response as they affect the conversational history sent to that LLM during inference, or completion. </p>"},{"location":"concepts/pipelines/history/#no-history","title":"No History","text":"<p>Nodes will default to <code>No History</code> as their history mode. This means that when a completion is requested from the LLM, no conversational history will be supplied. One common use case might be a formatting or translation node where the previous history may not be applicable to generating the correct output.</p>"},{"location":"concepts/pipelines/history/#node","title":"Node","text":"<p><code>Node</code> history will maintain a specific history for this particular node. The input to the node will be saved, along with the output from the LLM. </p> <p>LLM output is not necessarily the same as node output</p> <p>In a LLM Router node, the <code>output</code> from the node will be the same as the <code>input</code> to that node. That is, once it has done its routing, it will be a passthrough for the <code>input</code>. The output of the LLM however, will be the classification label. This is an important distinction to keep in mind.</p> <p>A common use case will be in a LLM Router node where we want to maintain a history of the node outputs (e.g., for continuity of what 'part' of the chatbot the user is interacting with), and we want to ensure that the history is using LLM outputs so that we don't unintentionally supply the LLM with few-shot examples of the wrong type of output.</p>"},{"location":"concepts/pipelines/history/#global","title":"Global","text":"<p>Nodes with <code>Global</code> history will supply the conversational history that the user would see to the LLM. The simple example uses a global history as the user is interacting directly with a single LLM. </p>"},{"location":"concepts/pipelines/history/#named","title":"Named","text":"<p>The final history mode is called <code>Named</code> and allows you to specify a specific, named, history that can be shared between nodes. Each node using the same shared history will contribute their <code>input</code> and LLM output to the history.</p> <p>Named history is updated immediately</p> <p>If there are multiple nodes serially that use the same <code>Named</code> history, then each node will add to the history. In the case of serial nodes, this will result in multiple new history entries for every processed user message.</p> <p>The most common use case to this will be when we have multiple parallel nodes after an LLM Router. In the Advanced Pipelines Example, the general, quiz, and roleplay LLM nodes would all likely use the same shared history, giving each node visibility into the larger conversation.</p> <p>Note that for this particular example, each of the nodes could use a <code>Global</code> history to achieve the same thing. However, if there was a translation or formatting node at before the final <code>output</code>, then the <code>Named</code> history mode would enable the interim nodes to share a history in the original language / formatting.</p>"},{"location":"concepts/pipelines/history/#history-compression-management-options","title":"History Compression Management Options","text":"<p>In addition to the basic history modes, pipeline authors have access to advanced history management options. These controls appear after selecting a basic history mode (No History, Node, Global, or Named) and help optimize token usage and conversation length.</p> <p>The UI presents a \"History Mode\" dropdown with three options:</p>"},{"location":"concepts/pipelines/history/#summarize","title":"Summarize","text":"<p>The Summarize option compresses history when it exceeds a token limit by summarizing older messages while preserving more recent ones. If the token count exceeds the limit, older messages will be summarized while keeping the last few messages intact.</p> <p>Input field Token Limit: Sets the maximum number of tokens before summarization occurs. When this threshold is reached, the system will summarize older messages to reduce token count.</p>"},{"location":"concepts/pipelines/history/#truncate-tokens","title":"Truncate Tokens","text":"<p>The Truncate Tokens option removes older messages when a token limit is reached, ensuring the total token count stays below a specified threshold. If the token count exceeds the limit, older messages will be removed until the token count is below the limit.</p> <p>Input field Token Limit: Sets the maximum number of tokens before truncation occurs. When this threshold is reached, the system will remove older messages to reduce token count.</p>"},{"location":"concepts/pipelines/history/#max-history-length","title":"Max History Length","text":"<p>the last N (where N is the specified number) messages.</p> <p>Input field Max History Length: Specifies how many of the most recent messages to keep in the history. Only this number of messages will be sent to the LLM.</p> <p>These history management options help pipeline authors balance context preservation with performance and cost considerations, particularly for long-running conversations or complex applications.</p>"},{"location":"concepts/pipelines/http_client/","title":"HTTP Client","text":"<p>The Python node provides an <code>http</code> global that enables secure HTTP requests to external APIs. This client includes built-in security features to protect against common vulnerabilities and supports automatic credential injection through Authentication Providers.</p>"},{"location":"concepts/pipelines/http_client/#overview","title":"Overview","text":"<p>The <code>http</code> is available as a global variable in your Python node code and can be used to make HTTP requests without importing external libraries. It provides a safe, team-aware way to interact with external APIs.</p> <pre><code>def main(input, **kwargs) -&gt; str:\n    # Make a GET request\n    response = http.get(\"https://api.example.com/data\")\n\n    # Process the response\n    return f\"Retrieved {len(response['json'])} items\"\n</code></pre>"},{"location":"concepts/pipelines/http_client/#available-methods","title":"Available Methods","text":"<p>The <code>http</code> supports the following HTTP methods:</p> <ul> <li><code>get(url, **kwargs)</code> - Send a GET request</li> <li><code>post(url, **kwargs)</code> - Send a POST request</li> <li><code>put(url, **kwargs)</code> - Send a PUT request</li> <li><code>patch(url, **kwargs)</code> - Send a PATCH request</li> <li><code>delete(url, **kwargs)</code> - Send a DELETE request</li> </ul> <p>All methods accept the following keyword-only parameters:</p> <ul> <li><code>headers</code> - Dictionary of HTTP headers</li> <li><code>params</code> - URL query parameters</li> <li><code>json</code> - JSON data to send in the request body (mutually exclusive with <code>data</code> and <code>files</code>)</li> <li><code>data</code> - Form data (dict) or raw body content (str/bytes). Can be combined with <code>files</code> for multipart form uploads</li> <li><code>timeout</code> - Request timeout in seconds (automatically clamped between 1s and the system maximum)</li> <li><code>files</code> - File upload data. See File Uploads for details</li> <li><code>auth</code> - Name of an Authentication Provider to inject credentials. This is a string, not a credentials tuple</li> </ul>"},{"location":"concepts/pipelines/http_client/#response-structure","title":"Response Structure","text":"<p>All HTTP methods return a dictionary containing the response data:</p> <pre><code>{\n    \"status_code\": 200,\n    \"headers\": {...},\n    \"text\": \"body as text\",  # this is always present\n    \"json\": {\"body\": \"as json\"},  # this is `None` if the response was not JSON\n    \"response_bytes\": b\"raw bytes\",  # raw response content as bytes\n    \"is_success\": 200 &lt;= status_code &lt; 300,\n    \"is_error\": status_code &gt;= 400,\n}\n</code></pre> <p>Access response data using dictionary keys:</p> <pre><code>response = http.get(\"https://api.example.com/data\")\nstatus = response[\"status_code\"]\ndata = response[\"json\"]\ntext_content = response[\"text\"]\nraw_bytes = response[\"response_bytes\"]\n</code></pre>"},{"location":"concepts/pipelines/http_client/#binary-content-handling","title":"Binary Content Handling","text":"<p>The <code>response_bytes</code> field contains the raw binary data from the HTTP response. This is particularly useful when working with binary content types like images, PDFs, or other non-text files.</p> <p>For binary content types (images, PDFs, etc.), text decoding is skipped in the <code>text</code> field to avoid doubling memory usage. The <code>json</code> field is parsed directly from the bytes when applicable.</p> <pre><code># Downloading a binary file\nresponse = http.get(\"https://example.com/document.pdf\")\npdf_bytes = response[\"response_bytes\"]\n\n# Check if the response is binary content\ncontent_type = response[\"headers\"].get(\"Content-Type\", \"\")\nif \"application/pdf\" in content_type:\n    # Use response_bytes for binary data\n    file_data = response[\"response_bytes\"]\n</code></pre>"},{"location":"concepts/pipelines/http_client/#security-features","title":"Security Features","text":"<p>The HTTP client includes several built-in security protections:</p> <p>SSRF Prevention: Blocks requests to private IP addresses, localhost, and other internal network resources to prevent Server-Side Request Forgery attacks.</p> <p>Request/Response Size Limits: Enforces maximum size limits on both requests and responses to prevent memory exhaustion.</p> <p>Timeout Clamping: Automatically limits request timeouts to prevent indefinite hanging.</p> <p>Blocked Headers: Certain sensitive headers are blocked to prevent security issues.</p> <p>Automatic Retries: Requests that receive a <code>429</code>, <code>502</code>, <code>503</code>, or <code>504</code> status code are automatically retried up to 3 times with exponential backoff. The client also respects the <code>Retry-After</code> header when present. Connection errors and timeouts are also retried.</p> <p>No Redirect Following: The HTTP client does not follow redirects automatically. If a request returns a <code>3xx</code> redirect status, you will receive the redirect response directly and must handle it in your code.</p> <p>Request Count Limit: There is a maximum number of HTTP requests that can be made per pipeline run. Exceeding this limit will raise an error.</p>"},{"location":"concepts/pipelines/http_client/#exceptions","title":"Exceptions","text":"<p>The HTTP client raises exceptions for infrastructure-level errors. You should handle these in your code if you need custom error handling:</p> Exception When it's raised <code>http.RequestLimitExceeded</code> Maximum number of requests per pipeline run exceeded <code>http.RequestTooLarge</code> Request body or file upload exceeds the size limit <code>http.ResponseTooLarge</code> Response body exceeds the size limit <code>http.ConnectionError</code> Connection failed after all retries exhausted <code>http.TimeoutError</code> Request timed out after all retries exhausted <code>http.InvalidURL</code> URL blocked by SSRF protection (private IPs, localhost, etc.) <code>http.AuthProviderError</code> Auth provider not found or not available <p>Note</p> <p>HTTP error status codes like <code>400</code>, <code>401</code>, <code>404</code>, or <code>500</code> do not raise exceptions. These are returned as normal response dicts. Always check <code>response[\"status_code\"]</code> or <code>response[\"is_error\"]</code> to detect HTTP-level errors.</p>"},{"location":"concepts/pipelines/http_client/#handling-exceptions","title":"Handling Exceptions","text":"<p>Use <code>try</code>/<code>except</code> blocks to catch infrastructure-level errors and provide user-friendly responses. Since the exception classes cannot be imported directly, reference them from the <code>http</code> global:</p> <pre><code>def main(input, **kwargs) -&gt; str:\n    try:\n        response = http.get(\"https://api.example.com/data\", timeout=10)\n    except http.TimeoutError:\n        return \"The request timed out. Please try again later.\"\n    except http.ConnectionError:\n        return \"Could not connect to the server. Please try again later.\"\n    except http.InvalidURL:\n        return \"The request was blocked for security reasons.\"\n    except http.RequestLimitExceeded:\n        return \"Too many requests have been made. Please try again later.\"\n\n    # HTTP error status codes (4xx, 5xx) don't raise exceptions,\n    # so check the response directly\n    if response[\"is_error\"]:\n        return f\"Request failed with status {response['status_code']}\"\n\n    return f\"Got data: {response['json']}\"\n</code></pre> <p>You can also catch multiple exceptions at once:</p> <pre><code>def main(input, **kwargs) -&gt; str:\n    try:\n        response = http.post(\n            \"https://api.example.com/submit\",\n            json={\"query\": input},\n            auth=\"my-api\",\n            timeout=15,\n        )\n    except (http.TimeoutError, http.ConnectionError) as e:\n        return f\"Network error: {e}\"\n    except (http.RequestTooLarge, http.ResponseTooLarge):\n        return \"The request or response was too large.\"\n    except http.AuthProviderError:\n        return \"Authentication is not configured correctly.\"\n\n    if response[\"is_success\"]:\n        return f\"Result: {response['json']}\"\n    else:\n        return f\"API error (status {response['status_code']}): {response['text']}\"\n</code></pre>"},{"location":"concepts/pipelines/http_client/#using-authentication-providers","title":"Using Authentication Providers","text":"<p>The <code>http</code> can automatically inject credentials from your team's Authentication Providers into HTTP requests. This provides a secure way to manage API credentials without hardcoding them in your code. To use a configured Authentication Provider, pass the name of the provider to the request method using the <code>auth</code> keyword:</p> <pre><code>http.get(\"https://example.com\", auth=\"my auth provider\")\n</code></pre> <p>Case-Insensitive Provider Names</p> <p>Authentication provider names are case-insensitive. <code>auth=\"My-Provider\"</code>, <code>auth=\"my-provider\"</code>, and <code>auth=\"MY-PROVIDER\"</code> will all match the same configured provider.</p>"},{"location":"concepts/pipelines/http_client/#complete-examples","title":"Complete Examples","text":""},{"location":"concepts/pipelines/http_client/#example-1-fetching-data-from-a-public-api","title":"Example 1: Fetching Data from a Public API","text":"<pre><code>def main(input, **kwargs) -&gt; str:\n    \"\"\"Fetch weather data from a public API\"\"\"\n    response = http.get(\n        \"https://api.weather.gov/gridpoints/TOP/31,80/forecast\",\n        timeout=10\n    )\n\n    if response[\"status_code\"] == 200:\n        data = response[\"json\"]\n        forecast = data[\"properties\"][\"periods\"][0]\n        return f\"Weather: {forecast['shortForecast']}, Temp: {forecast['temperature']}\u00b0F\"\n    else:\n        return f\"Error: Unable to fetch weather (status {response['status_code']})\"\n</code></pre>"},{"location":"concepts/pipelines/http_client/#example-2-posting-data-with-authentication","title":"Example 2: Posting Data with Authentication","text":"<pre><code>def main(input, **kwargs) -&gt; str:\n    \"\"\"Submit form data to an external API\"\"\"\n    # Parse user input\n    user_data = {\n        \"message\": input,\n        \"timestamp\": \"2024-01-01T12:00:00Z\"\n    }\n\n    # Post to API with authentication\n    response = http.post(\n        \"https://api.example.com/submissions\",\n        json=user_data,\n        auth=\"my-api-key\",\n        timeout=15\n    )\n\n    if response[\"status_code\"] == 201:\n        result = response[\"json\"]\n        return f\"Submission successful! ID: {result['id']}\"\n    else:\n        return f\"Submission failed with status {response['status_code']}\"\n</code></pre>"},{"location":"concepts/pipelines/http_client/#example-3-working-with-query-parameters","title":"Example 3: Working with Query Parameters","text":"<pre><code>def main(input, **kwargs) -&gt; str:\n    \"\"\"Search an API with query parameters\"\"\"\n    # Build query parameters from user input\n    params = {\n        \"q\": input,\n        \"limit\": 10,\n        \"format\": \"json\"\n    }\n\n    response = http.get(\n        \"https://api.example.com/search\",\n        params=params,\n        auth=\"search-api\"\n    )\n\n    if response[\"status_code\"] == 200:\n        results = response[\"json\"]\n        count = len(results[\"items\"])\n        return f\"Found {count} results for '{input}'\"\n    else:\n        return \"Search failed\"\n</code></pre>"},{"location":"concepts/pipelines/http_client/#example-4-handling-different-response-types","title":"Example 4: Handling Different Response Types","text":"<pre><code>def main(input, **kwargs) -&gt; str:\n    \"\"\"Handle both JSON and text responses\"\"\"\n    response = http.get(\n        \"https://api.example.com/data\",\n        auth=\"api-provider\"\n    )\n\n    # Check content type\n    content_type = response[\"headers\"].get(\"Content-Type\", \"\")\n\n    if \"application/json\" in content_type:\n        data = response[\"json\"]\n        return f\"JSON data: {data}\"\n    else:\n        text = response[\"text\"]\n        return f\"Text data: {text[:100]}...\"  # First 100 chars\n</code></pre>"},{"location":"concepts/pipelines/http_client/#file-uploads","title":"File Uploads","text":"<p>The <code>files</code> parameter allows you to upload files as part of a multipart form request. It accepts either a dictionary or a list of tuples mapping field names to file values.</p> <p>Each file value can be one of:</p> <ul> <li>An <code>Attachment</code> object from the temporary state <code>attachments</code> list</li> <li>A tuple of <code>(filename, data, content_type)</code> where <code>data</code> is bytes</li> <li>Raw <code>bytes</code> (the field name is used as the filename with <code>application/octet-stream</code> content type)</li> </ul> <p>Note</p> <p>The <code>json</code> parameter cannot be combined with <code>files</code>. Use <code>data</code> (dict) alongside <code>files</code> if you need to send additional form fields with your file upload.</p>"},{"location":"concepts/pipelines/http_client/#example-5-uploading-user-attachments-to-an-external-api","title":"Example 5: Uploading User Attachments to an External API","text":"<pre><code>def main(input, **kwargs) -&gt; str:\n    \"\"\"Forward all user attachments to an external API\"\"\"\n    attachments = get_temp_state_key(\"attachments\")\n\n    if not attachments:\n        return \"No files were uploaded\"\n\n    # Pass attachments directly \u2014 the HTTP client handles them\n    files = [(\"files\", attachment) for attachment in attachments]\n\n    response = http.post(\n        \"https://api.example.com/upload\",\n        files=files,\n        auth=\"upload-api\",\n        timeout=30\n    )\n\n    if response[\"is_success\"]:\n        return f\"Successfully uploaded {len(attachments)} file(s)\"\n    else:\n        return f\"Upload failed with status {response['status_code']}\"\n</code></pre>"},{"location":"concepts/pipelines/http_client/#example-6-uploading-a-single-attachment-with-form-data","title":"Example 6: Uploading a Single Attachment with Form Data","text":"<pre><code>def main(input, **kwargs) -&gt; str:\n    \"\"\"Upload the first attachment with additional metadata\"\"\"\n    attachments = get_temp_state_key(\"attachments\")\n\n    if not attachments:\n        return \"Please upload a file\"\n\n    attachment = attachments[0]\n\n    response = http.post(\n        \"https://api.example.com/documents\",\n        files={\"document\": attachment},\n        data={\"description\": input, \"source\": \"chatbot\"},\n        auth=\"doc-api\",\n        timeout=30\n    )\n\n    if response[\"is_success\"]:\n        result = response[\"json\"]\n        return f\"Uploaded '{attachment.name}' \u2014 Document ID: {result['id']}\"\n    else:\n        return f\"Upload failed: {response['text']}\"\n</code></pre>"},{"location":"concepts/pipelines/http_client/#example-7-uploading-raw-bytes","title":"Example 7: Uploading Raw Bytes","text":"<pre><code>def main(input, **kwargs) -&gt; str:\n    \"\"\"Generate a CSV and upload it\"\"\"\n    csv_content = \"name,email\\nAlice,alice@example.com\\nBob,bob@example.com\\n\"\n    csv_bytes = csv_content.encode(\"utf-8\")\n\n    response = http.post(\n        \"https://api.example.com/import\",\n        files={\"file\": (\"contacts.csv\", csv_bytes, \"text/csv\")},\n        auth=\"import-api\"\n    )\n\n    if response[\"is_success\"]:\n        return \"CSV uploaded successfully\"\n    else:\n        return f\"Upload failed: {response['status_code']}\"\n</code></pre>"},{"location":"concepts/pipelines/http_client/#example-8-filtering-attachments-by-type-before-uploading","title":"Example 8: Filtering Attachments by Type Before Uploading","text":"<pre><code>def main(input, **kwargs) -&gt; str:\n    \"\"\"Upload only image attachments\"\"\"\n    attachments = get_temp_state_key(\"attachments\")\n\n    images = [a for a in attachments if a.content_type.startswith(\"image/\")]\n\n    if not images:\n        return \"No images found in attachments\"\n\n    files = [(\"images\", img) for img in images]\n\n    response = http.post(\n        \"https://api.example.com/gallery\",\n        files=files,\n        auth=\"gallery-api\",\n        timeout=30\n    )\n\n    if response[\"is_success\"]:\n        return f\"Uploaded {len(images)} image(s)\"\n    else:\n        return f\"Upload failed: {response['text']}\"\n</code></pre>"},{"location":"concepts/pipelines/http_client/#downloading-and-attaching-files","title":"Downloading and Attaching Files","text":"<p>The HTTP client can be used in combination with the <code>attach_file_from_response()</code> helper function to download files from external APIs and attach them to the chat session. This is useful for generating reports, downloading documents, or retrieving images to share with the user.</p>"},{"location":"concepts/pipelines/http_client/#example-9-downloading-and-attaching-a-file","title":"Example 9: Downloading and Attaching a File","text":"<pre><code>def main(input, **kwargs) -&gt; str:\n    \"\"\"Download a PDF report from an external API and attach it to the chat\"\"\"\n    # Fetch the file from the API\n    response = http.get(\n        \"https://api.example.com/reports/monthly.pdf\",\n        auth=\"reports-api\",\n        timeout=30\n    )\n\n    if not response[\"is_success\"]:\n        return f\"Failed to download report: {response['status_code']}\"\n\n    # Attach the file to the chat session\n    attach_file_from_response(\n        response_bytes=response[\"response_bytes\"],\n        filename=\"monthly_report.pdf\"\n    )\n\n    return \"I've attached the monthly report for you to review.\"\n</code></pre>"},{"location":"concepts/pipelines/http_client/#example-10-downloading-multiple-files-based-on-user-input","title":"Example 10: Downloading Multiple Files Based on User Input","text":"<pre><code>def main(input, **kwargs) -&gt; str:\n    \"\"\"Download and attach multiple charts based on user request\"\"\"\n    # Parse which charts the user wants\n    chart_types = [\"sales\", \"revenue\", \"customers\"]\n\n    for chart_type in chart_types:\n        response = http.get(\n            f\"https://api.example.com/charts/{chart_type}.png\",\n            auth=\"charts-api\",\n            timeout=20\n        )\n\n        if response[\"is_success\"]:\n            attach_file_from_response(\n                response_bytes=response[\"response_bytes\"],\n                filename=f\"{chart_type}_chart.png\"\n            )\n\n    return f\"I've attached {len(chart_types)} charts for your review.\"\n</code></pre> <p>See the Python Node utility functions documentation for more details on the <code>attach_file_from_response()</code> function.</p>"},{"location":"concepts/pipelines/http_client/#common-status-codes","title":"Common Status Codes","text":"<p>When checking response status codes:</p> <ul> <li><code>200</code> - OK (successful GET)</li> <li><code>201</code> - Created (successful POST)</li> <li><code>204</code> - No Content (successful DELETE)</li> <li><code>400</code> - Bad Request (client error)</li> <li><code>401</code> - Unauthorized (authentication required)</li> <li><code>403</code> - Forbidden (insufficient permissions)</li> <li><code>404</code> - Not Found</li> <li><code>429</code> - Too Many Requests (rate limited)</li> <li><code>500</code> - Internal Server Error</li> <li><code>503</code> - Service Unavailable</li> </ul>"},{"location":"concepts/pipelines/http_client/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Check status codes: HTTP error status codes (4xx, 5xx) do not raise exceptions \u2014 they are returned as normal response dicts. Always check <code>response[\"status_code\"]</code> or use <code>response[\"is_success\"]</code> and <code>response[\"is_error\"]</code> to detect HTTP-level errors. Infrastructure errors like connection failures, timeouts, and size limit violations do raise exceptions (see Exceptions).</p> </li> <li> <p>Set reasonable timeouts: Specify timeout values to prevent requests from hanging indefinitely.</p> </li> <li> <p>Use Authentication Providers: Never hardcode API keys or credentials in your code. Use Authentication Providers instead.</p> </li> <li> <p>Validate user input: If using user input in URLs or parameters, validate and sanitize it first.</p> </li> <li> <p>Limit data size: Be mindful of the size of data you're requesting or sending.</p> </li> <li> <p>Cache when appropriate: If making repeated requests for the same data, consider caching results in session state.</p> </li> </ol>"},{"location":"concepts/pipelines/http_client/#troubleshooting","title":"Troubleshooting","text":"<p>\"Network access is disabled\": Network access must be enabled in the pipeline configuration. Contact your team administrator.</p> <p>\"SSRF protection blocked request\": The URL you're trying to access is blocked for security reasons (e.g., localhost, private IPs).</p> <p>\"Authentication provider not found\": Verify the provider name matches what's configured in your team settings (matching is case-insensitive).</p> <p>\"Request timeout\": The external API took too long to respond. Try increasing the timeout or check if the API is available.</p> <p>\"Connection refused\": The target server is not accepting connections. Verify the URL and check if the service is running.</p> <p>\"Request limit exceeded\": You have made too many HTTP requests in a single pipeline run. Reduce the number of requests or restructure your pipeline.</p> <p>\"Request body exceeds ... bytes\" / \"File upload exceeds ... bytes\": The data you are sending is too large. Reduce the payload size or split it into multiple requests.</p> <p>\"Response body exceeds ... bytes\": The external API returned a response that is too large. Consider requesting a smaller payload (e.g., use pagination or limit fields).</p>"},{"location":"concepts/pipelines/nodes/","title":"Node Types","text":"<p>Note</p> <p>See cookbook for example usage. </p>"},{"location":"concepts/pipelines/nodes/#llm","title":"LLM","text":"<p>Use an LLM to respond to the node input. This node can be configured with a prompt to give the LLM instructions on how to respond. It can also be configured to use tools which enable it to perform additional actions.</p>"},{"location":"concepts/pipelines/nodes/#routers","title":"Routers","text":"<p>Router nodes allow you to route the input to one of the linked nodes. This is useful if you want your bot to behave differently depending on the input or some persistent context. For example, you might want to route the input to a different node if the user is asking for help with a specific topic.</p> <p>Router nodes share some common configuration such as the list of route options. Router nodes can also be configured to tag the output message with the selected route. This is useful for debugging and for tracking the flow of messages through the pipeline. The format of the tag is <code>&lt;node_name&gt;:&lt;route_name&gt;</code> where <code>&lt;route_name&gt;</code> is the name of the route selected by the router node.</p> <p>Router Keywords Are Uppercase</p> <p>All router keywords are automatically converted to uppercase. When configuring router outputs, use uppercase keywords to match this behavior. Keyword matching is case-insensitive, so \"HELP\", \"Help\", and \"help\" will all match the same route.</p>"},{"location":"concepts/pipelines/nodes/#llm-router","title":"LLM Router","text":"<p>Routes the input to one of the linked nodes using an LLM. In this case, the LLM acts as a classifier using the prompt provided to classify an incoming message into a set of discrete categories that allow messages to be routed.</p> <p>The <code>outputs</code> listed by the node are the available classification labels. These should match the classification categories specified in your prompt. They can be adjusted through the <code>Advanced</code> settings for the node. The top output, which is prepended by a blue <code>*</code> is the default label. In the event that the LLM generates a response outside of the specified <code>outputs</code>, the route with the default label will be taken.</p> <p>Best practices for configuring a LLM Router</p> <p>It is advisable to use the Node history mode for an LLM Router to avoid unintentionally supplying few-shot examples to the node with an incorrect output format.</p>"},{"location":"concepts/pipelines/nodes/#static-router","title":"Static Router","text":"<p>The Static Router node allows you to route the input to one of the linked nodes based on the value of a specific key in the data source. This is useful if you want your bot to behave differently depending on the value of a specific key in the data source.</p> <p>The data source can be any of the following:</p> <ul> <li>Participant Data</li> <li>Session State</li> <li>Temporary State</li> </ul> <p>The key should be a name of a field in the data source and supports selecting nested fields via the <code>&lt;field&gt;.&lt;subfield&gt;</code> syntax. For example, if the data source is a JSON object with the following structure:</p> <pre><code>{\n    \"user\": {\n        \"name\": \"John\",\n        \"age\": 30\n    }\n}\n</code></pre> <p>You can select the <code>name</code> field using the key <code>user.name</code> and the <code>age</code> field using the key <code>user.age</code>.</p> <p>If the field is not present in the data source, the router will not route the input to the first linked node.</p>"},{"location":"concepts/pipelines/nodes/#assistant","title":"Assistant","text":"<p>Use an OpenAI assistant to respond to the input.</p>"},{"location":"concepts/pipelines/nodes/#python-node","title":"Python Node","text":"<p>Execute custom Python code for logic, data processing, and external API calls. See the Python Node page for full documentation including utility functions, the HTTP client, state management, and attachments.</p>"},{"location":"concepts/pipelines/nodes/#template","title":"Template","text":"<p>Renders a Jinja template.</p>"},{"location":"concepts/pipelines/nodes/#available-template-variables","title":"Available Template Variables","text":"<p>The following variables are available in the template context:</p> Key Description Type <code>input</code> The input to the node String <code>node_inputs</code> The list of all inputs to the node in the case of parallel workflows List of strings <code>temp_state</code> Pipeline temporary state Dict <code>session_state</code> Session state Dict <code>participant_details</code> Participant details (<code>identifier</code>, <code>platform</code>) Dict <code>participant_data</code> Participant data Dict <code>participant_schedules</code> Participant schedule data List"},{"location":"concepts/pipelines/nodes/#sample-template","title":"Sample Template","text":"<pre><code>Input: {{ input }}\nNode Inputs: {{ node_inputs }}\nTemp State Key: {{ temp_state.my_key }}\nParticipant ID: {{ participant_details.identifier }}\nParticipant Platform: {{ participant_details.platform }}\nParticipant Data: {{ participant_data.custom_key }}\nSchedules: {{ participant_schedules }}\n</code></pre>"},{"location":"concepts/pipelines/nodes/#email","title":"Email","text":"<p>Send the input to the specified list of email addresses. This node acts as a passthrough, meaning the output will be identical to the input, allowing it to be used in a pipeline without affecting the conversation.</p>"},{"location":"concepts/pipelines/nodes/#extract-structured-data","title":"Extract Structured Data","text":"<p>Extract structured data from the input. This node acts as a passthrough, meaning the output will be identical to the input, allowing it to be used in a pipeline without affecting the conversation.</p>"},{"location":"concepts/pipelines/nodes/#update-participant-data","title":"Update Participant Data","text":"<p>Extract structured data and save it as participant data.</p>"},{"location":"concepts/pipelines/parallel/","title":"Parallel Pipelines","text":"<p>Nodes in a pipeline can run in parallel, allowing multiple operations to proceed simultaneously.</p> <pre><code>flowchart LR\n    start([Input]) --&gt; LLM1 &amp; LLM2\n    LLM1 --&gt; out([Output])\n    LLM2 --&gt; out</code></pre> <p>Limitations</p> <p>Cycles</p> <p>Configurations that result in cycles (recursive loops) are not supported.</p> <p>Multiple Exectuion</p> <p>In cases where the branches of a workflow do not have the same number of nodes and then merge, nodes after the merge will be executed more than once without special handling. See the section below on Uneven Banches</p>"},{"location":"concepts/pipelines/parallel/#dangling-nodes","title":"Dangling nodes","text":"<p>Nodes without connected outputs (dangling nodes) are supported and will execute in turn. The outputs of these nodes will still be recorded in the pipeline state.</p> <pre><code>flowchart LR\n    start([Input]) --&gt; LLM1 &amp; PythonNode\n    LLM1 --&gt; out([Output])\n    PythonNode</code></pre>"},{"location":"concepts/pipelines/parallel/#multiple-outputs","title":"Multiple outputs","text":"<p>Connecting multiple outputs from one node (e.g. a router node) to the output of another node is allowed. If more than one of the outputs from the node have a value, the first one will be passed to the next node as input.</p> <pre><code>flowchart LR\n    start([Input]) --&gt; Router\n    Router -.outputA.-&gt; PythonNode\n    Router -.outputB.-&gt; PythonNode\n    PythonNode --&gt; out([Output])</code></pre> <p>Outputs can also be connected to multiple other nodes:</p> <pre><code>flowchart LR\n    start([Input]) --&gt; Router\n    Router -.outputA.-&gt; PythonNode\n    Router -.outputB.-&gt; PythonNode\n    Router -.outputB.-&gt; LLM\n    LLM --&gt; out([Output])</code></pre>"},{"location":"concepts/pipelines/parallel/#uneven-branches","title":"Uneven branches","text":"<p>Consider the following graph:</p> <pre><code>flowchart LR\n    start([Input]) --&gt; NodeA\n    start --&gt; NodeB\n    NodeA --&gt; NodeC\n    NodeC --&gt; NodeD\n    NodeB --&gt; NodeD\n    NodeD --&gt; out([Output])</code></pre> <p>The execution steps are as follows:</p> <ol> <li><code>NodeA</code> and <code>NodeB</code> in parallel</li> <li><code>NodeC</code> and <code>NodeD</code> in parallel</li> <li><code>NodeD</code></li> </ol> <p>Notice how <code>NodeD</code> gets executed twice. The first time <code>NodeD</code> runs it will have the output from <code>NodeC</code> as it's input. The 2nd time it runs it will have both the outputs from <code>NodeB</code> and <code>NodeC</code> as its inputs.</p> <p>To understand why this happens you need to understand the execution model.</p> <p>You can manage this challenge by using a <code>PythonNode</code> with some utility functions:</p> <ul> <li><code>require_node_outputs</code>: This function will abort any node run if all the requested data is not available.</li> <li><code>wait_for_next_input</code>: This is a lower level function that can be used when <code>require_node_outputs</code> isn't suitable.</li> </ul> <p>In the example above, we could use the following code in <code>NodeD</code> to merge the outputs:</p> <pre><code>def main(input, **kwargs):\n    # this will abort the first run since only `NodeB` has outputs\n    require_node_outputs(\"NodeB\", \"NodeC\")\n    b = get_node_output(\"NodeB\")\n    c = get_node_output(\"NodeC\")\n    return f\"{b}\\n{c}\"\n</code></pre> <p>Using the lower level <code>wait_for_next_input</code> function we can do the same thing:</p> <pre><code>def main(input, **kwargs):\n    b = get_node_output(\"NodeB\")\n    c = get_node_output(\"NodeC\")\n    if b is None and c is None:\n        # abort until both are available\n        wait_for_next_input()\n    return f\"{b}\\n{c}\"\n</code></pre>"},{"location":"concepts/pipelines/parallel/#optional-parallel-branches","title":"Optional Parallel Branches","text":"<p>This shows a use case for the <code>wait_for_next_input</code> function. We have a pipeline which has parallel branches and a merge node but not all the branches will execute.</p> <pre><code>flowchart LR\n    start([Input]) --&gt; Router\n    start --&gt; NodeA\n    Router -.-&gt; NodeB\n    Router -.-&gt; NodeC\n    NodeA --&gt; Merge\n    NodeB --&gt; Merge\n    NodeC --&gt; Merge\n    Merge --&gt; out([Output])</code></pre> <p>The <code>Merge</code> node will get outputs from <code>NodeA</code> and either <code>NodeB</code> or <code>NodeC</code>. We can't use <code>require_node_outputs</code> because not all outputs will be generated. Instead we need to use the <code>wait_for_next_input</code> function:</p> Option 1Option 2 <pre><code>def main(input, **kwargs):\n    b = get_node_output(\"NodeB\")\n    c = get_node_output(\"NodeC\")\n    b_or_c = b or c\n    if not b_or_c:\n        # wait until we have either b or c \n        wait_for_next_input()\n    a = get_node_output(\"NodeA\")\n    return f\"{a}\\n{b_or_c}\"\n</code></pre> <p>Note that we don't need to check if we have output from <code>NodeA</code> since it will be guaranteed to be available by the time <code>NodeB</code> or <code>NodeC</code> execute due to the execution order.</p> <p>This option makes use of the <code>node_inputs</code> keyword argument which contains a list of all the inputs available to the current node execution. Since we want to wait until we have inputs from <code>NodeA and (NodeB or NodeC)</code> we can check that the inputs list has at least two values. </p> <pre><code>def main(input, **kwargs):\n    all_inputs = kwargs.get(\"node_inputs\", [])\n    if len(all_inputs) &lt; 2:\n        # wait until we have at least two inputs \n        wait_for_next_input()\n    return \"\\n\".join(all_inputs)\n</code></pre> <ul> <li> <p> More Example Workflows</p> <p> Workflow Cookbook</p> </li> </ul>"},{"location":"concepts/pipelines/python_node/","title":"Python Node","text":"<p>The Python node allows the bot builder to execute custom Python code to perform logic, data processing, or other tasks.</p> <p>The code must define a <code>main</code> function which takes the node input as a string and returns a string to pass to the next node. The <code>main</code> function must also accept arbitrary keyword arguments to support future features. Here is an example of what the code might look like:</p> <pre><code>def main(input, **kwargs) -&gt; str:\n    # Put your code here\n    return input\n</code></pre> <p>The <code>input</code> parameter is a string that contains the input to the node. The return value of the function is a string that will be passed to the next node in the pipeline.</p>"},{"location":"concepts/pipelines/python_node/#additional-keyword-arguments","title":"Additional Keyword Arguments","text":"<p>The following additional arguments are provided:</p> <ul> <li><code>node_inputs: list[str]</code> - A list of all the inputs to the node at the time of execution. This will be the same as <code>[input]</code> except when the node is part of a workflow with parallel branches.</li> </ul> <p>Warning</p> <p>All the code must be encapsulated in a <code>main</code> function. You can write other functions but they must be within the scope of the <code>main</code> function. For example:</p> <pre><code>def main(input, **kwargs):\n    def important(arg):\n        return arg + \"!\"\n\n    return important(input)\n</code></pre>"},{"location":"concepts/pipelines/python_node/#utility-functions","title":"Utility Functions","text":"<p>The Python node provides a set of utility functions that can be used to interact with the user's data and the pipeline state.</p>"},{"location":"concepts/pipelines/python_node/#python_node.get_participant_data","title":"<code>get_participant_data() -&gt; dict</code>","text":"<p>Returns the current participant's data as a dictionary.</p>"},{"location":"concepts/pipelines/python_node/#python_node.set_participant_data","title":"<code>set_participant_data(data: dict) -&gt; None</code>","text":"<p>Updates the current participant's data with the provided dictionary. This will overwrite any existing data.</p>"},{"location":"concepts/pipelines/python_node/#python_node.set_participant_data_key","title":"<code>set_participant_data_key(key_name: str, data: any) -&gt; None</code>","text":"<p>Updates the current participant's data with the provided value at the specified key.</p>"},{"location":"concepts/pipelines/python_node/#python_node.append_to_participant_data_key","title":"<code>append_to_participant_data_key(key_name: str, data: any) -&gt; None</code>","text":"<p>Appends the provided value to the participant's data at the specified key. If the value at the key is not a list, it will be converted to a list containing the provided value.</p>"},{"location":"concepts/pipelines/python_node/#python_node.increment_participant_data_key","title":"<code>increment_participant_data_key(key_name: str, data: any) -&gt; None</code>","text":"<p>Increments the value at the participant's data key with the specified value</p>"},{"location":"concepts/pipelines/python_node/#python_node.get_participant_schedules","title":"<code>get_participant_schedules() -&gt; list</code>","text":"<p>Returns all active scheduled messages for the participant in the current experiment session.</p>"},{"location":"concepts/pipelines/python_node/#python_node.get_temp_state_key","title":"<code>get_temp_state_key(key_name: str) -&gt; str | None</code>","text":"<p>Returns the value of the temporary state key with the given name. If the key does not exist, it returns <code>None</code>.</p> <p>See also: Temporary State</p>"},{"location":"concepts/pipelines/python_node/#python_node.set_temp_state_key","title":"<code>set_temp_state_key(key_name: str, data: Any) -&gt; None</code>","text":"<p>Sets the value of the temporary state key with the given name to the provided data. This will override any existing data for the key.</p> <p>See also: Temporary State</p>"},{"location":"concepts/pipelines/python_node/#python_node.get_session_state_key","title":"<code>get_session_state_key(key_name: str) -&gt; str | None</code>","text":"<p>Returns the value of the session state key with the given name. If the key does not exist, it returns <code>None</code>.</p> <p>See also: Session State</p>"},{"location":"concepts/pipelines/python_node/#python_node.set_session_state_key","title":"<code>set_session_state_key(key_name: str, data: Any) -&gt; None</code>","text":"<p>Sets the value of the session state key with the given name to the provided data. This will override any existing data for the key.</p> <p>See also: Session State</p>"},{"location":"concepts/pipelines/python_node/#python_node.get_selected_route","title":"<code>get_selected_route(router_node_name: str) -&gt; str | None</code>","text":"<p>Returns the route selected by a specific router node with the given name. If the node does not exist or has no route defined, it returns <code>None</code>.</p>"},{"location":"concepts/pipelines/python_node/#python_node.get_node_path","title":"<code>get_node_path(node_name: str) -&gt; list | None</code>","text":"<p>Returns a list containing the sequence of nodes leading to the target node. If the node is not found in the pipeline path, returns a list containing only the specified node name.</p>"},{"location":"concepts/pipelines/python_node/#python_node.get_all_routes","title":"<code>get_all_routes() -&gt; dict</code>","text":"<p>Returns a dictionary containing all routing decisions made in the pipeline up to the current node. The keys are the node names and the values are the route keywords chosen by each router node.</p> <p>Note that in parallel workflows only the most recent route for a particular node will be returned.</p>"},{"location":"concepts/pipelines/python_node/#python_node.add_message_tag","title":"<code>add_message_tag(tag_name: str)</code>","text":"<p>Adds a tag to the output message. To add multiple tags, call this function multiple times.</p>"},{"location":"concepts/pipelines/python_node/#python_node.add_session_tag","title":"<code>add_session_tag(tag_name: str)</code>","text":"<p>Adds a tag to the chat session. To add multiple tags, call this function multiple times.</p>"},{"location":"concepts/pipelines/python_node/#python_node.get_node_output","title":"<code>get_node_output(node_name: str) -&gt; Any</code>","text":"<p>Returns the output of the specified node if it has been executed. If the node has not been executed, it returns <code>None</code>.</p>"},{"location":"concepts/pipelines/python_node/#python_node.require_node_outputs","title":"<code>require_node_outputs(*node_names)</code>","text":"<p>This function is used to ensure that the specified nodes have been executed and their outputs are available in the pipeline's state. If any of the specified nodes have not been executed, the node will not execute and the pipeline will wait for the required nodes to complete.</p> <p>This should be called at the start of the main function.</p> <pre><code>def main(input, **kwargs):\n    require_node_outputs(\"nodeA\", \"nodeB\")\n    return get_node_output(\"nodeA\") + get_node_output(\"nodeB\")\n</code></pre>"},{"location":"concepts/pipelines/python_node/#python_node.wait_for_next_input","title":"<code>wait_for_next_input()</code>","text":"<p>Advanced utility that will abort the current execution when not all inputs have been received. This is only useful in cases where the workflow has parallel branches which might result in the node being executed more than once.</p> <p>This is similar to <code>require_node_outputs</code> but useful where some node outputs may be optional.</p> <pre><code>def main(input, **kwargs):\n    a = get_node_output(\"a\")\n    b = get_node_output(\"b\")\n    if not a and not b:\n        wait_for_next_input()\n    # do something with a or b\n</code></pre>"},{"location":"concepts/pipelines/python_node/#python_node.abort_with_message","title":"<code>abort_with_message(message, tag_name: str = None) -&gt; None</code>","text":"<p>Calling this will terminate the pipeline execution. No further nodes will get executed in any branch of the pipeline graph.</p> <p>The message provided will be used to notify the user about the reason for the termination. If a tag name is provided, it will be used to tag the output message.</p> <pre><code># Abort pipeline with custom message\nif safety_violation_detected:\n    abort_with_message(\"Content policy violation detected\")\n</code></pre>"},{"location":"concepts/pipelines/python_node/#python_node.attach_file_from_response","title":"<code>attach_file_from_response(response_bytes: bytes, filename: str) -&gt; None</code>","text":"<p>Attaches a file downloaded from an HTTP response to the chat session.</p> <p>This function is used in combination with the HTTP client to download files from external APIs and attach them to the assistant's response message. The file will be available for the user to download.</p> <p>Parameters:</p> Name Type Description Default <code>response_bytes</code> <code>bytes</code> <p>The raw bytes of the file, typically from the <code>response_bytes</code> field of an HTTP response</p> required <code>filename</code> <code>str</code> <p>The name to give the attached file, including the file extension</p> required Example <pre><code>def main(input, **kwargs):\n    # Download a file from an API\n    response = http.get(\"https://api.example.com/report.pdf\", auth=\"my-api\")\n\n    if response[\"is_success\"]:\n        # Attach the file to the chat\n        attach_file_from_response(\n            response_bytes=response[\"response_bytes\"],\n            filename=\"report.pdf\"\n        )\n        return \"I've attached the report for you.\"\n\n    return \"Failed to download the report.\"\n</code></pre> <p>See also: HTTP Client - Downloading and Attaching Files</p>"},{"location":"concepts/pipelines/python_node/#http-client","title":"HTTP Client","text":"<p>The Python node provides an <code>http</code> global that enables secure HTTP requests to external APIs. See the HTTP Client page for full documentation.</p>"},{"location":"concepts/pipelines/python_node/#temporary-state","title":"Temporary State","text":"<p>The Python node can also access and modify the temporary state of the pipeline. The temporary state is a dictionary that is unique to each run of the pipeline (each new message from the user) and is not stored between sessions.</p> <p>The temporary state can be accessed and modified using the get_temp_state_key and set_temp_state_key utility functions.</p> <p>Temporary state contains the following keys by default. These keys can not be modified or deleted:</p> Key Description <code>user_input</code> The message sent by the user <code>outputs</code> The outputs generated by the previous node <code>attachments</code> A list of attachments passed in by the user. See Attachments <p>In addition to these keys, the temporary state can also contain custom key-value pairs that can be set and accessed by the Python node and by the Static Router node.</p> <p>Here is an example of a temporary state dictionary:</p> <pre><code>{\n    \"user_input\": \"Please help me with my problem\",\n    \"outputs\": {\n        \"Assistant\": \"I'm here to help! What can I do for you?\"\n    },\n    \"attachments\": [\n        Attachment(...),\n    ],\n    \"my_custom_key\": \"my_custom_value\",\n}\n</code></pre>"},{"location":"concepts/pipelines/python_node/#session-state","title":"Session State","text":"<p>The Python node can also access and modify the state of the participant's session. This state is a dictionary that is scoped to each session that the user might have with the bot.</p> <p>The session state can be accessed and modified using the get_session_state_key and set_session_state_key utility functions.</p>"},{"location":"concepts/pipelines/python_node/#attachments","title":"Attachments","text":"<p>Part of the temporary state is a list of attachments. Attachments are files that the user has uploaded to the bot. Each attachment has the following fields:</p> Field Description <code>name</code> The name of the file <code>size</code> The size of the file in bytes <code>content_type</code> The MIME type of the file <code>upload_to_assistant</code> Whether the file should be uploaded to the assistant as an attachment <code>read_bytes()</code> Reads the attachment content as bytes. <code>read_text()</code> Reads the attachment content as text. <p>Here is an example of an attachment object:</p> <pre><code>attachment = Attachment(\n    name=\"proposal.pdf\",\n    size=1234,\n    content_type=\"application/pdf\",\n    upload_to_assistant=False,\n)\ncontent = attachment.read_text()\n</code></pre>"},{"location":"concepts/pipelines/python_node/#supported-file-types","title":"Supported File Types","text":"<p>The Python node currently only supports reading the contents of the following file types:</p> <ul> <li>Text-based formats (TXT, CSV, HTML, JSON, XML, etc.)</li> <li>PDF</li> <li>DOCX</li> <li>XLSX</li> <li>XLS</li> <li>Outlook</li> <li>PPTX</li> </ul> <p>Other file types can still be uploaded to assistants but the Python Node is not able to read the file contents using the <code>read_text()</code> method on the attachment.</p>"},{"location":"concepts/team/","title":"Teams","text":"<p>Open Chat Studio is a multitenant platform that can support multiple organizations using the same instance at the same time. Each 'tenant' is called a 'team'. Teams are created by an organization and can have multiple members. Each team has its own settings and experiments.</p> <p>A user can be a member of multiple teams and have a different set of permissions in each team.</p> <p>A team serves as the root container for all data in Open Chat Studio.</p>"},{"location":"concepts/team/#team-configuration","title":"Team configuration","text":"<p>There is a set of global configuration that can be set at the team level. This includes:</p> <ul> <li>LLM Service Providers</li> <li>Speech Service Providers</li> <li>Messaging Providers</li> <li>Authentication Providers</li> <li>Custom Actions</li> <li>Tracing providers</li> <li>User Management</li> </ul>"},{"location":"concepts/team/authentication_providers/","title":"Authentication Providers","text":"<p>Authentication Providers are used to authenticate with external services via HTTP API calls. Authentication Providers provide a centralized location to manage the credentials and tokens required to authenticate with external services.</p> <p>These credentials are used by features like Custom Actions and the HTTP Client in Python nodes.</p>"},{"location":"concepts/team/authentication_providers/#authentication-provider-types","title":"Authentication Provider Types","text":"<p>Open Chat Studio supports various different authentication types. You should select the type that matches the API service you will be using.</p>"},{"location":"concepts/team/authentication_providers/#basic-auth","title":"Basic Auth","text":"<p>Basic Auth is a simple authentication scheme built into the HTTP protocol.</p>"},{"location":"concepts/team/authentication_providers/#api-key","title":"API Key","text":"<p>API Key is a simple authentication scheme that involves sending a key with the request to authenticate the user. The key is sent in a header of the request. The name of the header can be customized when creating the Authentication Provider.</p>"},{"location":"concepts/team/authentication_providers/#bearer-token","title":"Bearer Token","text":"<p>Bearer Token is a type of access token that is sent with the request to authenticate the user. The token is sent in the Authorization header of the request.</p>"},{"location":"concepts/team/authentication_providers/#commcare","title":"CommCare","text":"<p>CommCare HQ uses a custom authentication scheme as described in the CommCare Documentation</p>"},{"location":"concepts/team/groups/","title":"User Groups on OCS","text":"<p>Users can be assigned to specific groups upon invitation to the OCS platform enabling tailored access to features and resources based on their role or requirements. Users can be put in one or multiple groups.</p>"},{"location":"concepts/team/groups/#permissions-table","title":"Permissions Table","text":"Permission Super Admin Team Admin Experiment Admin Chat Viewer Analysis Admin Analysis User Assistant Admin Event Admin Pipeline Admin Can See Experiments \u2705 \u274c \u2705 \u274c \u274c \u274c \u274c \u274c \u274c Can View Safety Layers, Source Material, Surveys, Consent Forms \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 Can See Tags \u2705 \u274c \u2705 \u274c \u274c \u274c \u274c \u274c \u274c Can Access Prompt Builder \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 Can View Graphs/Download Files \u2705 \u274c \u274c \u2705 \u274c \u274c \u274c \u274c \u274c Can Invite Participants \u2705 \u274c \u2705 \u274c \u274c \u274c \u274c \u274c \u274c Can Export Chat Transcripts \u2705 \u274c \u2705 \u274c \u274c \u274c \u274c \u274c \u274c Can Manage Assistants and Files \u2705 \u274c \u274c \u274c \u274c \u274c \u2705 \u274c \u274c Create and Manage Experiment Events \u2705 \u274c \u274c \u274c \u274c \u274c \u274c \u2705 \u274c Create and Manage Pipelines \u2705 \u274c \u274c \u274c \u274c \u274c \u274c \u274c \u2705 Additional Notes Full Access, Default Role - Cannot see sessions - - - - - -"},{"location":"concepts/team/llm_providers/","title":"LLM Service Providers","text":"<p>Building chatbots in Open Chat Studio requires access to LLMs. This requires providing Open Chat Studio with credentials needed to access the models provided by external services such as OpenAI, Anthropic, and Google.</p> <p>Open Chat Studio currently supports the following providers:</p> <ul> <li>OpenAI</li> <li>Anthropic</li> <li>Google Gemini</li> <li>Azure OpenAI</li> <li>Groq</li> <li>Perplexity</li> <li>DeepSeek</li> </ul>"},{"location":"concepts/team/llm_providers/#models","title":"Models","text":"<p>Each service provider is pre-configured with the most common models for both AI inference and text embedding. Should the service provider configuration not include a model which is available via the provider, it may be added directly via the user interface. This is done by editing the provider and using the  button in the 'Custom Models' section.</p>"},{"location":"concepts/team/messaging_providers/","title":"Messaging Providers","text":"<p>Messaging providers offer access to communication platforms such as WhatsApp, Facebook Messenger, Slack, and more. Connecting a chatbot to these services allows users to interact with the bot on the respective service.```</p>"},{"location":"concepts/team/messaging_providers/#supported-providers","title":"Supported providers","text":"<p>Below is a list of supported providers and their integrated platforms in OCS:</p> <ul> <li>Twilio<ul> <li>WhatsApp</li> <li>Facebook Messenger</li> </ul> </li> <li>Turn.io<ul> <li>WhatsApp</li> </ul> </li> <li>Slack</li> <li>SureAhdere</li> </ul>"},{"location":"concepts/team/messaging_providers/#see-also","title":"See also","text":"<ul> <li>Configure a messaging provider</li> </ul>"},{"location":"concepts/team/tracing_providers/","title":"Tracing Providers","text":"<p>LLM tracing involves capturing a model's output and decision-making process. Tracing providers offer platforms to visualize and analyze these traces, aiding bot developers in understanding their chatbot's behavior.</p> <p>Open Chat Studio (OCS) has built-in support for the following providers:</p> <ul> <li>LangFuse</li> <li>LangSmith</li> </ul> <p>Tracing will automatically begin with the next conversation after configuring a tracing provider.</p>"},{"location":"concepts/team/tracing_providers/#see-also","title":"See also","text":"<ul> <li>Configure a provider</li> </ul>"},{"location":"concepts/tools/","title":"Tools","text":"<p>Tools allow LLMs to affect change in the real world. An LLM on its own can only produce intentions, but it is not able to execute those intentions. Tools are a way of telling the LLM what requests it can make and of executing that request when it is made.</p> <p>Open Chat Studio provides a number of built-in tools as well as the ability to add your own tools in the form of Custom Actions.</p> <p>The current set of built-in tools are listed below. If you need to refer to the tool in a prompt, use the tool's name directly e.g. <code>update-user-data</code>.</p>"},{"location":"concepts/tools/#user-configurable-tools","title":"User configurable tools","text":""},{"location":"concepts/tools/#calculator","title":"Calculator","text":"<p>Allows to bot to do mathematical calculations reliably.</p> <ul> <li>Name: <code>calculator</code></li> <li>Arguments:</li> <li><code>expression</code>: The mathematical expression to evaluate.</li> </ul>"},{"location":"concepts/tools/#recurring-reminders","title":"Recurring reminders","text":"<p>Allows the bot to schedule recurring reminders for the participant.</p> <ul> <li>Name: <code>recurring-reminder</code></li> <li>Arguments:</li> <li><code>datetime_due</code>: The first (or only) reminder start date in ISO 8601 format.</li> <li><code>every</code>: Number of 'periods' to wait between reminders.</li> <li><code>period</code>: The time period used in conjunction with 'every'. One of <code>minutes</code>, <code>hours</code>, <code>days</code>, <code>weeks</code>, <code>months</code></li> <li><code>message</code>: The reminder message.</li> <li><code>schedule_name</code>: The name for this reminder.</li> <li><code>datetime_end</code>: The date of the last reminder in ISO 8601 format (optional).</li> <li><code>repetitions</code>: The number of messages to send before stopping (optional).</li> </ul>"},{"location":"concepts/tools/#one-off-reminder","title":"One-off Reminder","text":"<p>Allows the bot to schedule once-off reminders for the participant.</p> <ul> <li>Name: <code>one-off-reminder</code></li> <li>Arguments:</li> <li><code>datetime_due</code>: The datetime that the reminder is due in ISO 8601 format</li> <li><code>message</code>: The reminder message</li> <li><code>schedule_name</code>: The name for this reminder</li> </ul>"},{"location":"concepts/tools/#delete-reminder","title":"Delete Reminder","text":"<p>Allows the bot to delete existing reminders (either once-off or recurring)</p> <ul> <li>Name: <code>delete-reminder</code></li> <li>Arguments:</li> <li><code>message_id</code>: The ID of the scheduled message to delete.</li> </ul>"},{"location":"concepts/tools/#move-reminder-date","title":"Move Reminder Date","text":"<p>Allows the bot to update the reminder date</p> <ul> <li>Name: <code>move-scheduled-message-date</code></li> <li>Arguments:</li> <li><code>message_id</code>: The ID of the scheduled message to update.</li> <li><code>weekday</code>: The new day of the week (1-7 where 1 = Monday).</li> <li><code>hour</code>: The new hour of the day, in UTC.</li> <li><code>minute</code>: The new minute of the hour.</li> <li><code>specified_date</code>: A specific date to re-schedule the message for in ISO 8601 format</li> </ul>"},{"location":"concepts/tools/#update-participant-data","title":"Update Participant Data","text":"<p>Allows the bot to make changes to the participant data</p> <ul> <li>Name: <code>update-user-data</code></li> <li>Arguments:</li> <li><code>key</code>: The key in the user data to update.</li> <li><code>value</code>: The new value of the user data.</li> </ul>"},{"location":"concepts/tools/#append-to-participant-data","title":"Append to Participant Data","text":"<p>Append a value to participant data at a specific key. This will convert any existing value to a list and append the new value to the end of the list. Use this tool to track lists of items e.g. questions asked.</p> <ul> <li>Name: <code>append-to-participant-data</code></li> <li>Arguments:</li> <li><code>key</code>: The key in the user data to append to.</li> <li><code>value</code>: The value to append.</li> </ul>"},{"location":"concepts/tools/#increment-counter","title":"Increment Counter","text":"<p>Increment the value of a counter. The counter is stored in participant data with the key <code>_counter_{counter_name}</code>.</p> <ul> <li>Name: <code>increment-counter</code></li> <li>Arguments:</li> <li><code>counter</code>: The name of the counter to increment.</li> <li><code>value</code>: Integer value to increment the counter by (defaults to 1).</li> </ul>"},{"location":"concepts/tools/#end-session","title":"End Session","text":"<p>End the current chat session. This will mark the session as completed. New messages will result in a new session being created.</p> <ul> <li>Name: <code>end-session</code></li> <li>Arguments: (none)</li> </ul>"},{"location":"concepts/tools/#internal-tools","title":"Internal tools","text":"<p>The following tools are used internally by Open Chat Studio and enabled / disabled automatically depending on the chatbot configuration.</p>"},{"location":"concepts/tools/#attach-media","title":"Attach Media","text":"<p>Allows the bot to attach media when a media collection is configured. </p> <ul> <li>Name: <code>attach-media</code></li> <li>Arguments:</li> <li><code>file_id</code>: The ID of the media file to attach.</li> </ul>"},{"location":"concepts/tools/#file-search","title":"File Search","text":"<p>Allows the bot to search indexed collections when a collection is configured.</p> <ul> <li>Name: <code>file-search</code></li> <li>Arguments:</li> <li><code>query</code>: A natural language query to search for relevant information in the documents.</li> </ul>"},{"location":"concepts/tools/#llm-provider-tools","title":"LLM Provider Tools","text":"<p>In addition to the tools provided by Open Chat Studio, some LLM providers have their own set of tools which are executed interally by the provider.</p>"},{"location":"concepts/tools/#openai-tools","title":"OpenAI tools","text":""},{"location":"concepts/tools/#openai-web-search","title":"Web Search","text":"<ul> <li>Search the web and pass the results to the LLM.</li> <li>See https://platform.openai.com/docs/guides/tools-web-search</li> <li> Supported by OCS</li> </ul>"},{"location":"concepts/tools/#openai-code-interpreter","title":"Code Interpreter","text":"<ul> <li>Execute code to analyse data, generate graphs etc.</li> <li>See https://platform.openai.com/docs/guides/tools-code-interpreter</li> <li> Supported by OCS</li> </ul>"},{"location":"concepts/tools/#anthropic-tools","title":"Anthropic tools","text":""},{"location":"concepts/tools/#anthropic-web-search","title":"Web Search","text":"<ul> <li>Search the web and pass the results to the LLM.</li> <li>See https://docs.anthropic.com/en/docs/agents-and-tools/tool-use/web-search-tool</li> <li> Supported by OCS</li> </ul>"},{"location":"concepts/tools/#anthropic-code-execution","title":"Code Execution","text":"<ul> <li>Execute code to analyse data, generate graphs etc.</li> <li>See https://docs.anthropic.com/en/docs/agents-and-tools/tool-use/code-execution-tool</li> <li> Not supported by OCS</li> </ul>"},{"location":"concepts/tools/#gemini-tools","title":"Gemini tools","text":""},{"location":"concepts/tools/#gemini-web-search","title":"Grounding with search","text":"<ul> <li>Search the web and pass the results to the LLM.</li> <li>See https://ai.google.dev/gemini-api/docs/google-search</li> <li> Not supported by OCS</li> </ul>"},{"location":"concepts/tools/#gemini-code-execution","title":"Code Execution","text":"<ul> <li>Execute code to analyse data, generate graphs etc.</li> <li>See https://ai.google.dev/gemini-api/docs/code-execution</li> <li> Not supported by OCS</li> </ul>"},{"location":"how-to/","title":"How-to Guides","text":"<p>Here you\u2019ll find answers to \u201cHow do I...?\u201d types of questions. These guides are goal-oriented and concrete; they're meant to help you complete a specific task. </p> <p>For conceptual explanations, see the Conceptual guide.</p> <ul> <li> <p> Quickstart Guide</p> <p>If you're not sure where to start, start here!</p> <p> My first chatbot</p> </li> </ul>"},{"location":"how-to/add_a_knowledge_base/","title":"Add a knowledge base","text":"<p>Adding knowledge to your bot depends on the type of bot you are building.</p>"},{"location":"how-to/add_a_knowledge_base/#base-llm-and-pipeline","title":"Base LLM and Pipeline","text":""},{"location":"how-to/add_a_knowledge_base/#add-your-source-material","title":"Add your source material","text":"<p>Select the Source Material tab on the left-hand menu and click Add new</p>"},{"location":"how-to/add_a_knowledge_base/#select-the-source-material-for-your-bot","title":"Select the source material for your bot","text":"<p>Once you\u2019ve created your source material, it should appear in the list of source materials when editing your bot.</p>"},{"location":"how-to/add_a_knowledge_base/#reference-the-source-material-in-your-prompt","title":"Reference the source material in your prompt","text":"<p>To reference the source material, include the <code>{source_material}</code> prompt variable in your prompt. Be mindful of its placement\u2014it\u2019s best to include it in a separate section rather than within a sentence.</p> <p>Example prompt:</p> <pre><code>You are a friendly bot. Be sure to reference the source material before answering the user's query: \n\n### Source material\n{source_material}\n</code></pre>"},{"location":"how-to/add_a_knowledge_base/#assistant","title":"Assistant","text":"<p>To add knowledge to your assistant, you must upload files to serve as the source material. When creating or editing your assistant, select the file_search or code_interpreter checkboxes to allow the assistant to read files.</p> <ul> <li>File search: This allows the bot to search and reference information provided in uploaded files.</li> <li>Code Interpreter: This allows the bot to write and execute code to accomplish tasks.</li> </ul>"},{"location":"how-to/add_a_knowledge_base/#see-also","title":"See also","text":"<ul> <li>Source Material</li> </ul>"},{"location":"how-to/api_access/","title":"API access","text":"<p>Using the API allows you to interact with your bot programmatically. This comes in especially useful when you're using a thrid party system to evaluate your bot. API access is enabled by default for all bots. To get started, you will need an API key, which you can generate by going to your profile page.</p> <p>See the API documentation for more details.</p>"},{"location":"how-to/assistants_migration/","title":"Migrate Assistants","text":"<p>OpenAI has deprecated Assistants and will completely remove support on 2026-08-26.</p> <p>Open Chat Studio supports all the features of Assistants in alternative ways as shown in the table below:</p> Assistant Feature Replacement Feature Threads Open Chat Studio sessions Code Interpreter OpenAI Code Interpreter tool in LLM nodes File Search Indexed Collections"},{"location":"how-to/assistants_migration/#migrating-code-interpreter","title":"Migrating Code Interpreter","text":"<p>Info</p> <p>This guide assumes that you have enabled the chatbots feature</p> <p>To use OpenAI's code interpreter tool without using Assistants:</p> <ul> <li>Create a Chatbot with an LLM node.</li> <li>Select an OpenAI LLM Provider</li> <li>Check the \"Code Execution\" checkbox under the \"Builtin Tools\" section of the configuration.</li> </ul>"},{"location":"how-to/assistants_migration/#migrating-file-search","title":"Migrating File Search","text":"<p>Make sure you're familiar with the different types of Collections before continuing.</p>"},{"location":"how-to/assistants_migration/#general-steps","title":"General Steps","text":"<ol> <li>Create an indexed collection using the same files your assistant uses under its \"file search\" tool.</li> <li>Set up or update your chatbot to reference this collection.</li> </ol>"},{"location":"how-to/assistants_migration/#step-1-create-the-collection","title":"Step 1: Create the Collection","text":"<p>Click on the \"Collections\" tab in the sidebar and click the \"Create from Assistant\" button in the top right.</p> <ul> <li>Select the assistant you'd like to migrate.</li> <li>Give your new collection a name.</li> <li>Click \"Create Collection\".</li> </ul>"},{"location":"how-to/assistants_migration/#what-happens-behind-the-scenes","title":"What Happens Behind the Scenes?","text":"<ul> <li>A new indexed collection is created using the same LLM provider as your assistant.</li> <li>All files from the assistant\u2019s \"file search\" tool are copied to this new collection.</li> <li>A vector store is created at OpenAI for the collection.</li> <li>The assistant\u2019s original vector store and files remain unchanged.</li> </ul>"},{"location":"how-to/assistants_migration/#step-2-update-your-chatbot","title":"Step 2: Update your chatbot","text":"<p>Once your collection is created:</p> <ul> <li>Open your Chatbot's pipeline editor.</li> <li>Add an LLM node to the pipeline. If you have been using an assistant node, this LLM node should replace your assistant node.</li> <li>Within the node, select your newly created indexed collection.</li> </ul> <p>You're done!</p>"},{"location":"how-to/configure_providers/","title":"Configure Providers","text":"<p>Providers are configured in your team settings under \"LLM and Embedding Model Service Providers\". Before configuring a provider, ensure that you have an active account at the provider and access to the necessary integration credentials.</p>"},{"location":"how-to/configure_providers/#prerequisites","title":"Prerequisites","text":"<p>API Key Required: You cannot create a provider without a valid API key. The API key is used for authentication between Open Chat Studio and the provider. You must:</p> <ol> <li>Have an active account with the provider (OpenAI, Anthropic, Google, etc.)</li> <li>Generate an API key from your provider account</li> <li>Have access to the specific models you want to use</li> </ol>"},{"location":"how-to/configure_providers/#adding-a-new-provider","title":"Adding a New Provider","text":"<ol> <li>Go to your team settings</li> <li>Navigate to \"LLM and Embedding Model Service Providers\"</li> <li>Click \"Add Provider\"</li> <li>Select your provider from the dropdown</li> <li>Enter your API key</li> <li>Save the configuration</li> </ol>"},{"location":"how-to/configure_providers/#adding-custom-models","title":"Adding Custom Models","text":"<p>If your provider doesn't have a pre-configured model you want to use, you can add it under the \"Custom LLM Models\" section.</p>"},{"location":"how-to/configure_providers/#model-naming-conventions","title":"Model Naming Conventions","text":"<p>Important: Model names must match the exact format used by the provider's API. Use lowercase with hyphens as specified by the provider.</p>"},{"location":"how-to/configure_providers/#openai-models","title":"OpenAI Models","text":"<p>For OpenAI models, use the exact model names from their API documentation. Examples: - <code>gpt-4o</code> (not <code>GPT-4o</code> or <code>gpt4o</code>) - <code>gpt-5-nano-2025-08-07</code> (for specific snapshots)</p> <p>Find current model names at: https://platform.openai.com/docs/models (check the \"snapshots\" section for each model)</p>"},{"location":"how-to/configure_providers/#anthropic-models","title":"Anthropic Models","text":"<p>For Anthropic (Claude) models, use the names from the \"Claude API\" column in their documentation. Examples: - <code>claude-opus-4-6</code> - <code>claude-3-5-sonnet-20241022</code> - <code>claude-3-haiku-20240307</code></p> <p>Find current model names at: https://docs.anthropic.com/en/docs/about-claude/models</p>"},{"location":"how-to/configure_providers/#google-models","title":"Google Models","text":"<p>For Google (Gemini) models, use the names from the \"Model Variant\" column. Examples: - <code>gemini-2.5-flash-exp</code> - <code>gemini-1.5-pro</code></p> <p>Find current model names at: https://ai.google.dev/gemini-api/docs/models</p>"},{"location":"how-to/configure_providers/#testing-your-configuration","title":"Testing Your Configuration","text":"<p>After adding a provider and models, it's recommended to: 1. Create a test bot 2. Configure it to use your new provider/model 3. Send a test message to verify everything works correctly</p>"},{"location":"how-to/deploy_to_different_channels/","title":"Deploy your bot to different platforms","text":"<p>To link a channel to your bot:</p> <p>Note</p> <p>Not all channels require a provider.</p> <ol> <li>Navigate to the Chatbot you wish to embed.</li> <li>Click on the  (plus) icon and select the provider from the dropdown.</li> <li>Complete the form and click Create. Follow the guide below to get the required information for each channel.</li> </ol> <p>You may need to Configure a messaging provider before you will be able to select it from the dropdown. </p>"},{"location":"how-to/deploy_to_different_channels/#web-and-api","title":"Web and API","text":"<p>The web channel uses the web interface and is enabled by default for all bots. Likewise, all bots can be accessed via the APIs.</p>"},{"location":"how-to/deploy_to_different_channels/#telegram","title":"Telegram","text":"<ul> <li>Follow this guide to create a Telegram bot.</li> <li>Copy the bot token and paste it into the form on OCS. It will look something like this: <code>4839574812:AAFD39kkdpWt3ywyRZergyOLMaJhac60qc</code>.</li> </ul> <p>Note</p> <p>Depending on your usecase, you probably want to disable group joins for your bot on Telegram. Since your telegram bot is public, anyone can add it to a group, which could end up costing you a lot. To achieve this, use the setjoingroups setting in BotFather.</p>"},{"location":"how-to/deploy_to_different_channels/#whatsapp","title":"WhatsApp","text":""},{"location":"how-to/deploy_to_different_channels/#setting-up-your-whatsapp-channel","title":"Setting Up Your WhatsApp Channel","text":"<ol> <li> <p>Add your WhatsApp number to the form in the Open Chat Studio channels section.</p> </li> <li> <p>Configure the webhook URL in your provider:</p> </li> </ol> <p>The webhook URL is always: <code>https://openchatstudio.com/channels/whatsapp/incoming_message</code></p> <p>This URL is the same for all WhatsApp chatbots and channels on Open Chat Studio.</p>"},{"location":"how-to/deploy_to_different_channels/#provider-specific-configuration","title":"Provider-Specific Configuration","text":""},{"location":"how-to/deploy_to_different_channels/#for-new-whatsapp-numbers","title":"For New WhatsApp Numbers","text":"<p>If you're setting up a brand new WhatsApp number, you'll need: - Admin access to your Twilio/Turn.io account - To register the number with Meta/WhatsApp - To configure the webhook URL in your provider settings</p>"},{"location":"how-to/deploy_to_different_channels/#for-existing-whatsapp-numbers","title":"For Existing WhatsApp Numbers","text":"<p>If you're reusing an existing WhatsApp number that was previously configured for Open Chat Studio: - No additional webhook configuration needed - the number is already set up to forward messages to Open Chat Studio - Simply add the number to your bot's channels in Open Chat Studio</p>"},{"location":"how-to/deploy_to_different_channels/#provider-instructions","title":"Provider Instructions","text":"<ul> <li>For Twilio: See this page to configure the webhook URL in your messaging service</li> <li>For Turn.io: Go to Settings \u2192 API &amp; Webhooks \u2192 Add a webhook and paste the OCS webhook URL</li> </ul>"},{"location":"how-to/deploy_to_different_channels/#facebook-messenger","title":"Facebook Messenger","text":"<p>Note</p> <p>It is assumed that you already have a Facebook page and a Twilio account with the Facebook page linked. Follow this guide if this is not the case.</p> <ul> <li>Add the ID of your Facebook page.</li> <li>After you submit the form, you will be provided with a webhook URL. Copy this URL and navigate back to your provider's settings to configure it with this URL.<ul> <li>For Twilio, edit your Facebook page settings and paste the URL into the \"Webhook URL for incoming messages\" field.</li> </ul> </li> </ul>"},{"location":"how-to/deploy_to_different_channels/#slack","title":"Slack","text":""},{"location":"how-to/deploy_to_different_channels/#prerequisites","title":"Prerequisites","text":"<p>Before configuring a Slack channel for your bot, you need to set up a Slack messaging provider. This requires:</p> <ul> <li>Admin access to your Slack workspace</li> <li>Following the Slack OAuth and application installation flow</li> <li>Configuring the messaging provider in Open Chat Studio (see Configure a messaging provider)</li> </ul> <p>Once your Slack messaging provider is configured, you can link channels to your bot.</p>"},{"location":"how-to/deploy_to_different_channels/#configuration-options","title":"Configuration Options","text":"<p>There are three different ways to configure how your bot responds to messages in Slack. The bot will check each configuration in order of priority:</p>"},{"location":"how-to/deploy_to_different_channels/#1-respond-to-messages-from-a-single-channel-highest-priority","title":"1. Respond to messages from a single channel (Highest Priority)","text":"<ul> <li>Specify the channel name (with or without the '#' prefix) in the channel configuration form.</li> <li>This is the highest specificity - messages on this channel will only be responded to by this bot.</li> <li>Only one bot can be configured per Slack channel.</li> </ul>"},{"location":"how-to/deploy_to_different_channels/#2-respond-to-messages-from-any-channel-using-keyword-matching","title":"2. Respond to messages from any channel using keyword matching","text":"<ul> <li>Enter keywords separated by commas in the keyword field (e.g., <code>support, help, assistance</code>).</li> <li>Keywords are matched using exact word matching (case-insensitive) to the first word of the user's message.</li> <li>Multiple keywords can be used to trigger the bot.</li> <li>Keywords must be unique to this bot &amp; Slack workspace combination.</li> </ul>"},{"location":"how-to/deploy_to_different_channels/#3-respond-to-messages-from-all-channels-lowest-priority","title":"3. Respond to messages from all channels (Lowest Priority)","text":"<ul> <li>The bot will respond to any message on any channel if it hasn't matched one of the previous two configurations.</li> <li>This is the lowest priority matching option.</li> <li>Only one bot per Slack workspace can be configured with this option.</li> </ul>"},{"location":"how-to/deploy_to_different_channels/#bot-interaction","title":"Bot Interaction","text":"<p>Once the channel is linked, users interact with the bot by mentioning it in Slack messages. The bot mention name is determined by your Slack app configuration. For example, on the Dimagi hosted instance of Open Chat Studio, the bot is mentioned using <code>@Dimagi Bots</code>.</p> <p>Priority and Precedence</p> <p>If multiple bots could match a message, the bot with the most specific configuration wins:</p> <ol> <li>Single channel configuration (highest priority)</li> <li>Keyword matching</li> <li>Respond to all channels (lowest priority)</li> </ol>"},{"location":"how-to/deploy_to_different_channels/#sureadhere","title":"SureAdhere","text":"<ul> <li>Enter the Tenant ID that would have been provided to you when setting up your SureAdhere account.</li> <li>After you submit the form, you will be provided with a webhook URL. Copy this URL and navigate back to your provider's settings to configure it with this URL.</li> </ul>"},{"location":"how-to/first_chatbot/","title":"Create your first chatbot","text":"<p>In this video, I walk you through the basic steps to set up your first chatbot in Open Chat Studio. We start by creating an LLM provider, which allows us to connect with services like OpenAI or Anthropic; I chose Anthropic and entered my API key. I then created a chatbot and test it out. After testing the chatbot to ensure it functions correctly, I explain the importance of versioning to maintain user experience while making updates. I encourage you to follow along and create your own chatbot using these steps.</p>"},{"location":"how-to/global_search/","title":"Using Global Search","text":"<p>Open Chat Studio has a global search feature that allows you to find objects using their public UUIDs. This is useful when you want to quickly find an object without navigating through the UI, especially if you have copied the UUID from a trace or another source.</p> <p>To use the global search feature, navigate to <code>https://openchatstudio.com/search?q=UUID</code> where <code>UUID</code> is the public UUID of the object you want to find.</p> <p>If the object is found, and you have permissions to access it, you will be redirected to the page for that object.</p>"},{"location":"how-to/global_search/#objects-that-are-currently-supported","title":"Objects that are currently supported","text":"<ul> <li>Experiments</li> <li>ExperimentSessions</li> <li>Participants</li> </ul>"},{"location":"how-to/remote_api/","title":"Connecting to a remote API","text":"<p>Open Chat Studio allows you to connect to external services via HTTP API calls. This feature is useful for extending the functionality of your bot by integrating it with other services. This feature is analogous to OpenAI's GPT Actions feature.</p> <p>To do this you will need to create an action by navigating to the \"Custom Actions\" section in Team Settings. See the Custom Actions guide for more information on creating a Custom Action.</p>"},{"location":"how-to/remote_api/#using-the-custom-action-in-your-bot","title":"Using the custom action in your bot","text":"<p>Once you have created a custom action you can add it to your bot by following these steps:</p> <ol> <li>Open your Experiment's edit page.</li> <li>Navigate to the Tools tab.</li> <li>Select the action you want to add from the Custom Actions checkbox list.</li> </ol>"},{"location":"how-to/remote_api/#testing-the-custom-action","title":"Testing the custom action","text":"<p>To test the custom action, you can open a chat with your Experiment and type a message that triggers the action. The bot will make an HTTP request to the external service and return the response to you.</p> <p>To see more detail about the request and response, you can enable tracing in the Advanced tab of your Experiment.</p>"},{"location":"how-to/setting_up_a_survey/","title":"Setting up a survey","text":"<p>This page provides an overview of how to utilize surveys in your OCS chatbot.</p>"},{"location":"how-to/setting_up_a_survey/#what-are-surveys-on-open-chat-studio","title":"What are Surveys on Open Chat Studio?","text":"<p>The Surveys feature allows chatbot makers to give users a link to a Google form (or any other link to a survey), both at the start and end of an OCS chatbot web session.\u00a0</p> <p>External Channel Survey Limitations &amp; Workarounds</p> <p>Surveys will not be automatically presented to the user before or after the chat if you deploy your chatbot on external channels like WhatsApp or Telegram. If you would still like to capture pre- or post-survey questions using these channels, you can: </p> <ul> <li>Incorporate survey questions in your prompt and structuring the prompt such that the chatbot starts and ends with questions as you would like it to.\u00a0</li> <li>Send users links to a Google form or other kind of survey directly, before or after providing them with the link to the chatbot.\u00a0</li> </ul> <p>Example of a pre-survey when using an OCS bot on the web </p> <p>Example of a post-survey when using an OCS bot on the web </p>"},{"location":"how-to/setting_up_a_survey/#create-a-survey","title":"Create a Survey","text":"<p>The very first step is to create a survey and generate a web link for that survey. For example, you might use Google forms to create pre- and post-surveys. Once this step is complete, navigate to the \"Surveys\" option on the left-hand menu on Open Chat Studio and follow the steps given below to add your survey(s) to a chatbot.\u00a0</p>"},{"location":"how-to/setting_up_a_survey/#select-add-new","title":"Select \"Add New\".","text":"<ul> <li>Name: This is a name for you or your team members on OCS to identify different surveys.</li> <li>URL:\u00a0Add the URL of your survey.\u00a0</li> <li>Confirmation text:\u00a0This is the text a user sees when they see the link to the survey, before they begin to use the chatbot. You can edit this text as you'd like.\u00a0Here, it's also important to add <code>{survey_link}</code> where you would like to show the URL to your survey.</li> </ul> <p>Example</p> <p>Before starting the experiment, we ask that you complete a short survey. Please click on the survey link, fill it out, and, when you have finished, select the checkbox to confirm you have completed it. Survey link: {survey_link}.\u00a0</p> <p>If you would like to include both a pre-survey and a post-survey, repeat the above process for each survey.</p>"},{"location":"how-to/setting_up_a_survey/#final-step","title":"Final Step","text":"<p>Now edit your chatbot and choose which survey to use as the pre- or post survey.</p>"},{"location":"how-to/setting_up_a_survey/#using-google-forms","title":"Using Google Forms","text":"<p>Go to Google Forms and create your form. In order to link a particular participant, session and experiment with a specific form, you'll need to include questions with the titles \"Participant ID\", \"Session ID\" and \"Experiment ID\".</p> <p>For example: </p> <p>From here, click on the 3 dots in the top right corner and go to \"Get Prefilled Link\". Now fill in the fields that you want prefilled. In this case, \"Participant ID\", \"Session ID\" and \"Experiment ID\".</p> <p> When you click on \"Get Link\", you'll get a link that looks something like:</p> <p>https://docs.google.com/forms/some/uri/viewform?usp=pp_url&amp;entry.1118764343=participant&amp;entry.791635770=session&amp;entry.784126073=experiment</p> <p>Replace the sections in the URL as follows:</p> <ul> <li>participant -&gt; {participant_id}</li> <li>session -&gt; {session_id}</li> <li>experiment -&gt; {experiment_id}</li> </ul> <p>This will result in a link that looks like this:</p> <p>https://docs.google.com/forms/some/uri/viewform?usp=pp_url&amp;entry.1118764343={participant_id}&amp;entry.791635770={session_id}&amp;entry.784126073={experiment_id}</p> <p>This new link should be used for your survey link.</p>"},{"location":"how-to/workflow_cookbook/","title":"Chatbot Workflow Cookbook","text":""},{"location":"how-to/workflow_cookbook/#split-bot-into-multiple-smaller-bots","title":"Split bot into multiple smaller bots","text":"<p>For complex bots it may be the case that a single LLM node with a large prompt does not perform well. For example, a bot that is expected to perform multiple different functions such as Role Play, Quiz, Q&amp;A.</p> <p>In such cases, it can be better to create smaller, narrowly focused prompts and use a router to select which 'mode' the bot is currently in.</p> <p>Here is a more complex example that uses a LLM Router to route the input to one of three linked nodes.</p> <pre><code>graph TB\n  A@{ shape: stadium, label: \"Input\" } --&gt; Router(\"`**LLM Router**\n  Route to one of the linked nodes using an LLM`\");\n  Router --&gt;|GENERAL| Gen(LLM);\n  Router --&gt;|ROLEPLAY| Rp(LLM);\n  Router --&gt;|QUIZ| Qz(LLM);\n  Gen --&gt; C@{ shape: stadium, label: \"Output\" };\n  Qz --&gt; C\n  Rp --&gt; C;</code></pre>"},{"location":"how-to/workflow_cookbook/#safety-check-in-parallel","title":"Safety check in parallel","text":"<p>In this example, we are using a Router to determine if the user input complies with the usage policy of the bot. The router has two outputs, safe and unsafe. The safe output is not connected to any other nodes but the unsafe output is connected to a Python Node which will abort the pipeline with an error message.</p> <pre><code>flowchart TD\n    start[\"start\"] --&gt; Safety[\"SafetyRouter\"] &amp; LLM\n    Safety -. safe .-&gt; Dangle:::hidden\n    Safety -. unsafe .-&gt; PythonNode[\"PythonNode\n    *abort_with_message('...')*\"]\n    LLM --&gt; __end__([\"&lt;p&gt;end&lt;/p&gt;\"])\n\n     start:::first\n     __end__:::last</code></pre> <p>If the Safety Router routes to the Python Node, the user will not see the output generated by the LLM node but will instead see a message generated by an LLM based on the message passed to the <code>abort_with_message</code> function.</p>"},{"location":"how-to/workflow_cookbook/#router-for-classification","title":"Router for classification","text":"<p>Router nodes can have unconnected outputs as seen above, enabling more flexible routing patterns where not all paths need to be explicitly handled. It is also OK to connect multiple router outputs to the same input of another node. This can be useful if you want to the router node to categorize the input but not actually affect the execution flow.</p> <pre><code>flowchart TD\n    start[\"start\"] --&gt; Router[RouterA]\n    Router -. categoryA .-&gt; PythonNode\n    Router -. categoryB .-&gt; PythonNode\n    PythonNode --&gt; LLM\n    LLM --&gt; __end__([\"&lt;p&gt;end&lt;/p&gt;\"])\n\n     start:::first\n     __end__:::last</code></pre> <p>You might use this to perform some logic in the PythonNode:</p> <pre><code>def main(input, **kwargs):\n    route = get_selected_route(\"RouterA\")\n    if route == \"categoryA\":\n        set_temp_state_key(\"question\", \"A\")\n    elif route == \"categoryB\":\n        set_temp_state_key(\"question\", \"B\")\n    return input\n</code></pre> <p>Then in the LLM node prompt you could use the temp_state to inject the category:</p> <pre><code>The current category is {temp_state.category}\n</code></pre>"},{"location":"how-to/workflow_cookbook/#reading-user-uploaded-files","title":"Reading user uploaded files","text":"<p>This workflow allows users (participants) to upload files that your chatbot can process and analyze. Supported file types are listed here.</p>"},{"location":"how-to/workflow_cookbook/#setup-steps","title":"Setup Steps","text":"<ol> <li>Enable file uploads: In your chatbot settings, enable the \"File uploads enabled\" option</li> <li>Create a Python Node: Use a Python node to read and process the uploaded file contents from the temporary state - specifically from the attachments key.</li> <li>Pass to LLM: Either return the user input along with the file contents directly to the LLM node, or save the file contents to the temporary state and inject them into your LLM prompt</li> </ol>"},{"location":"how-to/workflow_cookbook/#workflow-structure","title":"Workflow Structure","text":"<pre><code>flowchart TD\n    start[\"start\"] --&gt; PythonNode\n    PythonNode --&gt; LLM\n    LLM --&gt; __end__([\"&lt;p&gt;end&lt;/p&gt;\"])\n\n     start:::first\n     __end__:::last</code></pre> <p>Python Node Implementation:</p> <p>Option 1: Single File Processing Process only the first uploaded file:</p> <pre><code>def main(input: str, **kwargs) -&gt; str: \n    # Get uploaded files from temp state\n    attachments = get_temp_state_key(\"attachments\")\n    if not attachments:\n        return input\n\n    # Read the first file's content\n    file_content = attachments[0].read_text()\n    set_temp_state_key(\"file_contents\", file_content)\n\n    return input\n</code></pre> <p>Option 2: Multiple Files Processing Process all uploaded files:</p> <pre><code>def main(input: str, **kwargs) -&gt; str: \n    # Get uploaded files from temp state\n    attachments = get_temp_state_key(\"attachments\")\n    if not attachments:\n        return input\n\n    # Read all files and combine their contents\n    all_file_contents = []\n    for i, attachment in enumerate(attachments):\n        file_content = attachment.read_text()\n        filename = attachment.name if hasattr(attachment, 'name') else f\"File {i+1}\"\n        all_file_contents.append(f\"## {filename}\\n{file_content}\")\n\n    # Save combined contents to temp state\n    combined_contents = \"\\n\\n\".join(all_file_contents)\n    set_temp_state_key(\"file_contents\", combined_contents)\n\n    return input\n</code></pre> <p>In these examples, the Python node reads the uploaded file(s) and saves their contents to the temp state under the key \"file_contents\". The user's original input is passed through unchanged to the LLM node. </p> <p>LLM Node Configuration:</p> <p>Configure your LLM node to utilize the uploaded file contents by injecting them into the prompt using temp state variables.</p> <p>Basic Prompt Template: <pre><code>You are a helpful assistant. Answer the user's query as best you can.\n\nHere are some file contents that you should consider when generating your answer:\n\n## File Contents\n{temp_state.file_contents}\n\nUser Query: {input}\n\nInstructions:\n- If the file contents are empty or not provided, inform the user that no files were uploaded\n- Base your response on both the file contents and the user's query\n- Be specific about what you found in the uploaded files\n- If you cannot find relevant information in the files, clearly state this\n</code></pre></p>"},{"location":"how-to/workflow_cookbook/#making-external-api-calls","title":"Making external API calls","text":"<p>This workflow demonstrates how to integrate external APIs into your chatbot using the HTTP Client. The HTTP client allows your bot to fetch data from external services, submit information, or interact with third-party APIs securely.</p>"},{"location":"how-to/workflow_cookbook/#prerequisites","title":"Prerequisites","text":"<ol> <li>Enable network access: Network access must be enabled for your pipeline (contact your team administrator if needed)</li> <li>Set up Authentication Provider (if required): If the API requires authentication, configure an Authentication Provider in your team settings</li> </ol>"},{"location":"how-to/workflow_cookbook/#workflow-structure_1","title":"Workflow Structure","text":"<pre><code>flowchart TD\n    start[\"start\"] --&gt; PythonNode[\"Python Node\n    *Make API call using http_client*\"]\n    PythonNode --&gt; LLM[\"LLM Node\n    *Process API response*\"]\n    LLM --&gt; __end__([\"&lt;p&gt;end&lt;/p&gt;\"])\n\n     start:::first\n     __end__:::last</code></pre>"},{"location":"how-to/workflow_cookbook/#example-1-fetch-external-data","title":"Example 1: Fetch External Data","text":"<p>This example fetches weather information from an external API and passes it to the LLM for processing:</p> <pre><code>def main(input, **kwargs) -&gt; str:\n    \"\"\"Fetch weather data and prepare it for the LLM\"\"\"\n    try:\n        # Make GET request to weather API\n        response = http.get(\n            \"https://api.weather.gov/gridpoints/TOP/31,80/forecast\",\n            timeout=10\n        )\n\n        if response[\"status_code\"] == 200:\n            data = response[\"json\"]\n            forecast = data[\"properties\"][\"periods\"][0]\n\n            # Format the data for the LLM\n            weather_info = f\"\"\"\nWeather Forecast:\n- Condition: {forecast['shortForecast']}\n- Temperature: {forecast['temperature']}\u00b0F\n- Wind: {forecast['windSpeed']} {forecast['windDirection']}\n- Detailed Forecast: {forecast['detailedForecast']}\n\"\"\"\n            # Store in temp state for LLM prompt access\n            set_temp_state_key(\"weather_data\", weather_info)\n            return input\n        else:\n            set_temp_state_key(\"weather_data\", \"Weather data unavailable\")\n            return input\n\n    except Exception as e:\n        set_temp_state_key(\"weather_data\", f\"Error: {str(e)}\")\n        return input\n</code></pre> <p>LLM Prompt Configuration:</p> <pre><code>You are a helpful weather assistant. Use the weather data provided to answer the user's question.\n\n{temp_state.weather_data}\n\nUser Question: {input}\n\nInstructions:\n- Provide a clear, conversational response based on the weather data\n- If weather data is unavailable, inform the user politely\n</code></pre>"},{"location":"how-to/workflow_cookbook/#example-2-submit-data-to-external-api","title":"Example 2: Submit Data to External API","text":"<p>This example takes user input, submits it to an external API, and returns the result:</p> <pre><code>def main(input, **kwargs) -&gt; str:\n    \"\"\"Submit user feedback to external service\"\"\"\n    try:\n        # Prepare data from user input\n        feedback_data = {\n            \"message\": input,\n            \"timestamp\": \"2024-01-01T12:00:00Z\",\n            \"user_id\": get_participant_data().get(\"identifier\", \"unknown\")\n        }\n\n        # POST to API with authentication\n        response = http.post(\n            \"https://api.example.com/feedback\",\n            json=feedback_data,\n            auth=\"feedback-api-key\",  # Reference to Auth Provider\n            timeout=15\n        )\n\n        if response[\"status_code\"] == 201:\n            result = response[\"json\"]\n            return f\"Thank you for your feedback! Reference ID: {result['id']}\"\n        else:\n            return \"We encountered an issue submitting your feedback. Please try again later.\"\n\n    except Exception as e:\n        return f\"Sorry, we couldn't process your feedback at this time: {str(e)}\"\n</code></pre>"},{"location":"how-to/workflow_cookbook/#example-3-enriching-user-data","title":"Example 3: Enriching User Data","text":"<p>This workflow fetches additional information based on user input and enriches the conversation context:</p> <pre><code>def main(input, **kwargs) -&gt; str:\n    \"\"\"Look up product information from external catalog\"\"\"\n    try:\n        # Extract product ID from user input (simplified example)\n        product_id = input.strip()\n\n        # Query external product API\n        response = http.get(\n            f\"https://api.example.com/products/{product_id}\",\n            auth=\"product-api\",\n            timeout=10\n        )\n\n        if response[\"status_code\"] == 200:\n            product = response[\"json\"]\n\n            # Store product details in temp state\n            set_temp_state_key(\"product_name\", product[\"name\"])\n            set_temp_state_key(\"product_price\", product[\"price\"])\n            set_temp_state_key(\"product_description\", product[\"description\"])\n            set_temp_state_key(\"product_available\", product[\"in_stock\"])\n\n            return input\n        elif response[\"status_code\"] == 404:\n            set_temp_state_key(\"product_error\", \"Product not found\")\n            return input\n        else:\n            set_temp_state_key(\"product_error\", \"Unable to fetch product details\")\n            return input\n\n    except Exception as e:\n        set_temp_state_key(\"product_error\", str(e))\n        return input\n</code></pre> <p>LLM Prompt Configuration:</p> <pre><code>You are a product support assistant.\n\n{% if temp_state.product_error %}\nProduct Information: Not available ({{ temp_state.product_error }})\n{% else %}\nProduct Information:\n- Name: {{ temp_state.product_name }}\n- Price: ${{ temp_state.product_price }}\n- Description: {{ temp_state.product_description }}\n- Availability: {{ \"In Stock\" if temp_state.product_available else \"Out of Stock\" }}\n{% endif %}\n\nUser Query: {input}\n\nInstructions:\n- Help the user with their product inquiry\n- Use the product information provided above\n- If product information is not available, help the user find what they need\n</code></pre>"},{"location":"how-to/workflow_cookbook/#best-practices-for-api-integration","title":"Best Practices for API Integration","text":"<ol> <li>Always handle errors: Wrap API calls in try-except blocks to gracefully handle failures</li> <li>Set appropriate timeouts: Prevent requests from hanging indefinitely</li> <li>Use Authentication Providers: Never hardcode API keys in your Python code</li> <li>Validate user input: Sanitize any user input used in API calls</li> <li>Store results in temp state: Use <code>set_temp_state_key()</code> to make API data available to LLM prompts</li> <li>Check status codes: Always verify the response status before processing data</li> <li>Provide user feedback: Return meaningful messages when API calls fail</li> </ol> <p>See the HTTP Client documentation for more details on available methods, security features, and advanced usage.</p>"}]}